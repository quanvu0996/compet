{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!cp '../input/coded-file/quanvh8_funcs.py' .","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"'''ENSEMBLE NETS\nInspire by https://www.kaggle.com/c/otto-group-product-classification-challenge/discussion/14335'''\n\nimport numpy as np, pandas as pd, copy, tensorflow as tf, matplotlib.pyplot as plt, sklearn\n\nfrom tensorflow import feature_column as fc\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.layers import (Dense, DenseFeatures, Dropout, \n                                     BatchNormalization, Embedding, Input, Concatenate, Average,\n                                     InputLayer, Lambda)\nfrom tensorflow.keras.metrics import AUC, Precision, Recall\nfrom tensorflow.keras import backend as K, Sequential, Model\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.optimizers import Adam, RMSprop\n\nimport tensorflow_addons as tfa\nfrom tensorflow_addons.layers import WeightNormalization\n\nfrom sklearn.preprocessing import QuantileTransformer, StandardScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom sklearn.cluster import KMeans\n\nfrom quanvh8_funcs import (DerivedFeatures, kfolds_bagging_training, voting_predict,\n                           kolds_stacked_ensemble_training, stacked_ensemble_predict )\n\nimport sys\n\ndef log_loss_metric(y_true, y_pred):\n    bce = tf.keras.losses.BinaryCrossentropy()\n    return bce(y_true, y_pred).numpy()\n\nprint(pd.__version__)\nprint(tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading data and encoding\n\nfolder_path = '../input/lish-moa/'\nraw_test = pd.read_csv(folder_path + 'test_features.csv')\nraw_train = pd.read_csv(folder_path + 'train_features.csv')\nraw_targets = pd.read_csv(folder_path + 'train_targets_scored.csv')\n\n# Phân loại dữ liệu\ncols_id = ['sig_id']\ncols_to_remove = ['cp_type']\ncols_fts = [i for i in raw_train.columns if i not in cols_id +cols_to_remove]\ncols_gene = [col for col in raw_train.columns if col.startswith(\"g-\")]\ncols_cell = [col for col in raw_train.columns if col.startswith(\"c-\")]\ncols_experiment = [col for col in cols_fts if col not in cols_gene+cols_cell]\ncols_target = [i for i in raw_targets.columns if i not in cols_id]\nnum_fts, num_labels = len(cols_fts), len(cols_target)\n\n# xử lý categorical\ndef transform_data(input_data):\n    '''Clean data and encoding\n        * input_data: table '''\n    out = input_data.copy()\n    out['cp_dose'] = out['cp_dose'].map({'D1':0, 'D2':1})\n    out['cp_time'] = out['cp_time']/72\n    \n    return out\n\nto_train = transform_data(raw_train[raw_train['cp_type'] != 'ctl_vehicle'])\nto_train_targets = raw_targets.iloc[to_train.index]\nfull_pred  = transform_data(raw_test)\nto_pred = full_pred[full_pred['cp_type'] != 'ctl_vehicle']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# preprocessing pipeline\ndef pipe_line_builder(quantiles_num, pca_dims, kmean_clusters):\n    '''Dựng pipe line cho từng nhóm columns\n    :quantiles_num: int: số quantile khi normalise\n    :pca_dims: int: số chiều pca'''\n    norm = QuantileTransformer(n_quantiles=quantiles_num,random_state=0, output_distribution=\"normal\")\n    pca = PCA(n_components = pca_dims)\n    derived_ft = DerivedFeatures(n_clusters = kmean_clusters)\n    tsne = sklearn.manifold.TSNE(n_components = int(pca_dims/2))\n    isomap = sklearn.manifold.Isomap(n_neighbors = 128, n_components = int(pca_dims/2) )\n\n    p_derived_ft = Pipeline([\n        ('norm', norm), \n        ('derived', derived_ft)])\n    \n    p_isomap = Pipeline([\n        ('norm', norm), \n        ('isomap', isomap)])\n\n    p_norm_pca = Pipeline([ \n        ('norm', norm),\n        ('pca', pca) ])\n    return FeatureUnion([\n        ('norm', norm), \n        ('norm_pca', p_norm_pca),\n        ('derived', p_derived_ft),\n#         ('isomap', p_isomap)\n    ])\n\n# Dựng pipe transform data\n\npipe = Pipeline([\n    ('norm_pca', ColumnTransformer([\n                     ('gene', pipe_line_builder(quantiles_num = 200, pca_dims = 600, kmean_clusters = 5), cols_gene),\n                     ('cell', pipe_line_builder(quantiles_num = 200, pca_dims = 50, kmean_clusters = 5), cols_cell),\n                    ]) \n    ), \n    ('var', VarianceThreshold(0.01)) \n])\n\npipe = ColumnTransformer([\n    ('gene_cell', pipe, cols_gene+ cols_cell),\n    ('experiment', 'passthrough', cols_experiment)\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transform data\npipe.fit(to_train[cols_fts].append(to_pred[cols_fts]))\nX_train = pipe.transform(to_train[cols_fts])\nX_pred = pipe.transform(to_pred[cols_fts])\ny_train = to_train_targets[cols_target].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef get_list_contains_ohe(keywords, cols_list):\n    ouput = []\n    for keyword in keywords:\n        ouput.append( [1 if keyword in i else 0 for i in cols_list] )\n    return ouput\n\n# Tiếp cận theo hướng recommend - cell -> chemical | cell/gene: user, chemial: item\nn_components = 350\n\nu_fts_num = X_train.shape[1]#num_fts\ni_fts_num = num_labels\n\ninitializer = 'he_normal'\n\n#User embedding\ninput_u = Input(shape = (u_fts_num,) , name ='input_u1' )\nlayer_u = BatchNormalization( ) (input_u)\nlayer_u = Dropout(0.25 ) (layer_u)\nlayer_u = WeightNormalization(Dense(1024, activation=\"selu\", kernel_initializer= initializer, kernel_regularizer= tf.keras.regularizers.l2(0.0005) )) (layer_u)\nlayer_u = BatchNormalization( ) (layer_u)\nlayer_u = Dropout(0.25 ) (layer_u)\nlayer_u = WeightNormalization(Dense(512, activation=\"selu\", kernel_initializer=initializer, kernel_regularizer= tf.keras.regularizers.l2(0.0005) )) (layer_u)\nlayer_u = BatchNormalization( ) (layer_u)\nlayer_u = Dropout(0.25 ) (layer_u)\nlayer_u = WeightNormalization(Dense(n_components, activation = 'selu', kernel_initializer= initializer, kernel_regularizer= tf.keras.regularizers.l2(0.0005) )) (layer_u)\nlayer_u = BatchNormalization() (layer_u)\n\n#Item embedding\n  # Addition information for item_info\nlist_chem_gr = ['_inhibitor', '_agonist', '_agent', '_antagonist', '_blocker', '_activator']\nchemical_category = tf.transpose( tf.constant( get_list_contains_ohe( list_chem_gr, cols_target  ) ))\n  # Full item fts: addition + onehot\nitem_ft = tf.concat(\n    [chemical_category ,\n     tf.eye(i_fts_num, dtype = tf.int32) # Create tensor 0-1 coresponse with chemical labels\n    ], axis = 1\n)\nlayer_i = WeightNormalization(Dense(n_components, activation = 'selu', kernel_initializer= initializer, kernel_regularizer= tf.keras.regularizers.l2(0.0005) )) (item_ft)\nlayer_i = BatchNormalization() (layer_i)\n\n# Dot product user - item\ndef dot_2layer(x):\n    return K.dot( x[0], K.transpose(x[1]))\ndot_ui = Lambda( dot_2layer, name = 'lambda_dot' ) ([layer_u,layer_i])\ndot_ui= BatchNormalization() (dot_ui)\ndot_ui= WeightNormalization(Dense(512, activation=\"selu\" , kernel_initializer= initializer, kernel_regularizer= tf.keras.regularizers.l2(0.0005))) (dot_ui)\ndot_ui= BatchNormalization() (dot_ui)\ndot_ui= WeightNormalization(Dense(256, activation=\"selu\", kernel_initializer= initializer, kernel_regularizer= tf.keras.regularizers.l2(0.0005) )) (dot_ui)\ndot_ui= BatchNormalization() (dot_ui)\ndot_ui = Dense(i_fts_num, activation = 'sigmoid' )(dot_ui)\n\n# Compile model\nmodel = Model(inputs=[input_u, ], outputs= [dot_ui])\nopt = Adam(lr=0.0005)\n\nbce = tf.keras.losses.BinaryCrossentropy()\nmodel.compile(loss= BinaryCrossentropy(label_smoothing=0.001), optimizer=opt \n              , metrics= [bce])\n# print( model.summary() )\n\n# tf.keras.utils.plot_model(model,show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduce_lr = ReduceLROnPlateau(monitor='val_binary_crossentropy', factor=0.1, patience=5, mode='min', min_lr=1E-5, verbose= 0)\nearly_stopping = EarlyStopping(monitor='val_binary_crossentropy', min_delta=1E-5, patience=15, mode='min',restore_best_weights=True, verbose= 0)\n    \nmodel.fit(\n        X_train, y_train, validation_split = 0.25, \n        callbacks=[reduce_lr, early_stopping], epochs=150, verbose =1,\n        batch_size=32 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = model.predict(X_pred)\ndf_preds_non_ctl =  pd.DataFrame(prediction, columns= cols_target, index = to_pred.index)\n\n# concat with all to pred values\ndf_preds = pd.concat([ full_pred[cols_id], df_preds_non_ctl], axis = 1).fillna(0)\n\ndf_preds.iloc[:,[34,82]] = 0\n# to csv\ndf_preds.to_csv(\"submission.csv\", index = None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_preds","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}