{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "moa-stacked-ensemble-multimodels.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3opyzGTGHcN"
      },
      "source": [
        "COLAB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNZPYerEGF3Q"
      },
      "source": [
        "folder_path = '/content/drive/MyDrive/Data/colabs_data/MOA_kaggle/'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVvLybRlJSW3",
        "outputId": "ca6e2056-cb1f-4dd2-d4c1-eaccd766b89c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ux7iJfNKQub"
      },
      "source": [
        "!cp '/content/drive/MyDrive/Data/colabs_data/MOA_kaggle/quanvh8_funcs.py' ."
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dya6YyhbGMo-"
      },
      "source": [
        "KAGGLE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "672aJy7uGQ47"
      },
      "source": [
        "# folder_path = '../input/lish-moa/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "NVgqVGJhGDpU"
      },
      "source": [
        "# !cp '../input/coded-file/quanvh8_funcs.py' ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "SPXySQXZGDpW",
        "outputId": "3c0b83b6-f4ad-4965-ba8f-595c2809dd16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "'''ENSEMBLE NETS\n",
        "Inspire by https://www.kaggle.com/c/otto-group-product-classification-challenge/discussion/14335'''\n",
        "\n",
        "import numpy as np, pandas as pd, copy, tensorflow as tf, matplotlib.pyplot as plt, sklearn\n",
        "\n",
        "from tensorflow import feature_column as fc\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.layers import (Dense, DenseFeatures, Dropout, \n",
        "                                     BatchNormalization, Embedding, Input, Concatenate, Average,\n",
        "                                     InputLayer, Lambda)\n",
        "from tensorflow.keras.metrics import AUC, Precision, Recall\n",
        "from tensorflow.keras import backend as K, Sequential, Model\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow_addons.layers import WeightNormalization\n",
        "\n",
        "from sklearn.preprocessing import QuantileTransformer, StandardScaler\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from quanvh8_funcs import (DerivedFeatures, kfolds_bagging_training, voting_predict,\n",
        "                           kolds_stacked_ensemble_training, stacked_ensemble_predict )\n",
        "\n",
        "import sys\n",
        "\n",
        "def log_loss_metric(y_true, y_pred):\n",
        "    bce = tf.keras.losses.BinaryCrossentropy()\n",
        "    return bce(y_true, y_pred).numpy()\n",
        "\n",
        "print(pd.__version__)\n",
        "print(tf.__version__)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.1.4\n",
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Fl1NAQ3LGDpW"
      },
      "source": [
        "# Loading data and encoding\n",
        "\n",
        "\n",
        "raw_test = pd.read_csv(folder_path + 'test_features.csv')\n",
        "raw_train = pd.read_csv(folder_path + 'train_features.csv')\n",
        "raw_targets = pd.read_csv(folder_path + 'train_targets_scored.csv')\n",
        "\n",
        "# Phân loại dữ liệu\n",
        "cols_id = ['sig_id']\n",
        "cols_to_remove = ['cp_type']\n",
        "cols_fts = [i for i in raw_train.columns if i not in cols_id +cols_to_remove]\n",
        "cols_gene = [col for col in raw_train.columns if col.startswith(\"g-\")]\n",
        "cols_cell = [col for col in raw_train.columns if col.startswith(\"c-\")]\n",
        "cols_experiment = [col for col in cols_fts if col not in cols_gene+cols_cell]\n",
        "cols_target = [i for i in raw_targets.columns if i not in cols_id]\n",
        "num_fts, num_labels = len(cols_fts), len(cols_target)\n",
        "\n",
        "# xử lý categorical\n",
        "def transform_data(input_data):\n",
        "    '''Clean data and encoding\n",
        "        * input_data: table '''\n",
        "    out = input_data.copy()\n",
        "    out['cp_dose'] = out['cp_dose'].map({'D1':0, 'D2':1})\n",
        "    out['cp_time'] = out['cp_time']/72\n",
        "    \n",
        "    return out\n",
        "\n",
        "to_train = transform_data(raw_train[raw_train['cp_type'] != 'ctl_vehicle'])\n",
        "to_train_targets = raw_targets.iloc[to_train.index]\n",
        "full_pred  = transform_data(raw_test)\n",
        "to_pred = full_pred[full_pred['cp_type'] != 'ctl_vehicle']"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "7rIjLd5UGDpW"
      },
      "source": [
        "# preprocessing pipeline\n",
        "def pipe_line_builder(quantiles_num, pca_dims, kmean_clusters):\n",
        "    '''Dựng pipe line cho từng nhóm columns\n",
        "    :quantiles_num: int: số quantile khi normalise\n",
        "    :pca_dims: int: số chiều pca'''\n",
        "    norm = QuantileTransformer(n_quantiles=quantiles_num,random_state=0, output_distribution=\"normal\")\n",
        "    pca = PCA(n_components = pca_dims)\n",
        "    derived_ft = DerivedFeatures(n_clusters = kmean_clusters)\n",
        "    tsne = sklearn.manifold.TSNE(n_components = int(pca_dims/2))\n",
        "    isomap = sklearn.manifold.Isomap(n_neighbors = 128, n_components = int(pca_dims/2) )\n",
        "\n",
        "    p_derived_ft = Pipeline([\n",
        "        ('norm', norm), \n",
        "        ('derived', derived_ft)])\n",
        "    \n",
        "    p_isomap = Pipeline([\n",
        "        ('norm', norm), \n",
        "        ('isomap', isomap)])\n",
        "\n",
        "    p_norm_pca = Pipeline([ \n",
        "        ('norm', norm),\n",
        "        ('pca', pca) ])\n",
        "    return FeatureUnion([\n",
        "        ('norm', norm), \n",
        "        ('norm_pca', p_norm_pca),\n",
        "        ('derived', p_derived_ft),\n",
        "#         ('isomap', p_isomap)\n",
        "    ])\n",
        "\n",
        "# Dựng pipe transform data\n",
        "\n",
        "pipe = Pipeline([\n",
        "    ('norm_pca', ColumnTransformer([\n",
        "                     ('gene', pipe_line_builder(quantiles_num = 200, pca_dims = 600, kmean_clusters = 5), cols_gene),\n",
        "                     ('cell', pipe_line_builder(quantiles_num = 200, pca_dims = 50, kmean_clusters = 5), cols_cell),\n",
        "                    ]) \n",
        "    ), \n",
        "    ('var', VarianceThreshold(0.01)) \n",
        "])\n",
        "\n",
        "pipe = ColumnTransformer([\n",
        "    ('gene_cell', pipe, cols_gene+ cols_cell),\n",
        "    ('experiment', 'passthrough', cols_experiment)\n",
        "])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "nQyBLx8OGDpX"
      },
      "source": [
        "# Transform data\n",
        "pipe.fit(to_train[cols_fts].append(to_pred[cols_fts]))\n",
        "X_train = pipe.transform(to_train[cols_fts])\n",
        "X_pred = pipe.transform(to_pred[cols_fts])\n",
        "y_train = to_train_targets[cols_target].values"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "6brIdJSDGDpX"
      },
      "source": [
        "\n",
        "def get_list_contains_ohe(keywords, cols_list):\n",
        "    ouput = []\n",
        "    for keyword in keywords:\n",
        "        ouput.append( [1 if keyword in i else 0 for i in cols_list] )\n",
        "    return ouput\n",
        "\n",
        "# Tiếp cận theo hướng recommend - cell -> chemical | cell/gene: user, chemial: item\n",
        "n_components = 350\n",
        "\n",
        "u_fts_num = X_train.shape[1]#num_fts\n",
        "i_fts_num = num_labels\n",
        "\n",
        "initializer = 'he_normal'\n",
        "\n",
        "#User embedding\n",
        "input_u = Input(shape = (u_fts_num,) , name ='input_u1' )\n",
        "layer_u = BatchNormalization( ) (input_u)\n",
        "layer_u = Dropout(0.25 ) (layer_u)\n",
        "layer_u = WeightNormalization(Dense(1024, activation=\"selu\", kernel_initializer= initializer, kernel_regularizer= tf.keras.regularizers.l2(0.0001) )) (layer_u)\n",
        "layer_u = BatchNormalization( ) (layer_u)\n",
        "layer_u = Dropout(0.25 ) (layer_u)\n",
        "layer_u = WeightNormalization(Dense(1024, activation=\"selu\", kernel_initializer=initializer, kernel_regularizer= tf.keras.regularizers.l2(0.0001) )) (layer_u)\n",
        "layer_u = BatchNormalization( ) (layer_u)\n",
        "layer_u = Dropout(0.25 ) (layer_u)\n",
        "layer_u = WeightNormalization(Dense(n_components, activation = 'selu', kernel_initializer= initializer, kernel_regularizer= tf.keras.regularizers.l2(0.0001) )) (layer_u)\n",
        "layer_u = BatchNormalization() (layer_u)\n",
        "\n",
        "#Item embedding\n",
        "  # Addition information for item_info\n",
        "list_chem_gr = ['_inhibitor', '_agonist', '_agent', '_antagonist', '_blocker', '_activator']\n",
        "chemical_category = tf.transpose( tf.constant( get_list_contains_ohe( list_chem_gr, cols_target  ) ))\n",
        "  # Full item fts: addition + onehot\n",
        "item_ft = tf.concat(\n",
        "    [chemical_category ,\n",
        "     tf.eye(i_fts_num, dtype = tf.int32) # Create tensor 0-1 coresponse with chemical labels\n",
        "    ], axis = 1\n",
        ")\n",
        "layer_i = WeightNormalization(Dense(n_components, activation = 'selu', kernel_initializer= initializer, kernel_regularizer= tf.keras.regularizers.l2(0.0001) )) (item_ft)\n",
        "layer_i = BatchNormalization() (layer_i)\n",
        "\n",
        "# Dot product user - item\n",
        "def dot_2layer(x):\n",
        "    return K.dot( x[0], K.transpose(x[1]))\n",
        "dot_ui = Lambda( dot_2layer, name = 'lambda_dot' ) ([layer_u,layer_i])\n",
        "dot_ui= BatchNormalization() (dot_ui)\n",
        "dot_ui= WeightNormalization(Dense(512, activation=\"selu\" , kernel_initializer= initializer, kernel_regularizer= tf.keras.regularizers.l2(0.0001))) (dot_ui)\n",
        "dot_ui= BatchNormalization() (dot_ui)\n",
        "dot_ui= WeightNormalization(Dense(256, activation=\"selu\", kernel_initializer= initializer, kernel_regularizer= tf.keras.regularizers.l2(0.0001) )) (dot_ui)\n",
        "dot_ui= BatchNormalization() (dot_ui)\n",
        "dot_ui = Dense(i_fts_num, activation = 'sigmoid' )(dot_ui)\n",
        "\n",
        "# Compile model\n",
        "model = Model(inputs=[input_u, ], outputs= [dot_ui])\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "bce = tf.keras.losses.BinaryCrossentropy()\n",
        "model.compile(loss= BinaryCrossentropy(label_smoothing=0.001), optimizer=opt \n",
        "              , metrics= [bce])\n",
        "# print( model.summary() )\n",
        "\n",
        "# tf.keras.utils.plot_model(model,show_shapes=True)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7LdkeCTMzDh"
      },
      "source": [
        "# help(tfa.optimizers.CyclicalLearningRate)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "d15oMBhGGDpX",
        "outputId": "31996f4a-4aae-4dfd-cc7d-be1110af6c10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "reduce_lr = ReduceLROnPlateau(monitor='val_binary_crossentropy', factor=0.1, patience=5, mode='min', min_lr=1E-5, verbose= 0)\n",
        "early_stopping = EarlyStopping(monitor='val_binary_crossentropy', min_delta=1E-5, patience=15, mode='min',restore_best_weights=True, verbose= 0)\n",
        "    \n",
        "model.fit(\n",
        "        X_train, y_train, validation_split = 0.25, \n",
        "        callbacks=[reduce_lr, early_stopping], epochs=150, verbose =1,\n",
        "        batch_size=32 )"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "515/515 [==============================] - 6s 12ms/step - loss: 1.7539 - binary_crossentropy: 0.6148 - val_loss: 1.3815 - val_binary_crossentropy: 0.3970\n",
            "Epoch 2/150\n",
            "515/515 [==============================] - 6s 11ms/step - loss: 1.0631 - binary_crossentropy: 0.1998 - val_loss: 0.8560 - val_binary_crossentropy: 0.0880\n",
            "Epoch 3/150\n",
            "515/515 [==============================] - 6s 11ms/step - loss: 0.7749 - binary_crossentropy: 0.0566 - val_loss: 0.7210 - val_binary_crossentropy: 0.0371\n",
            "Epoch 4/150\n",
            "515/515 [==============================] - 6s 12ms/step - loss: 0.6996 - binary_crossentropy: 0.0311 - val_loss: 0.6833 - val_binary_crossentropy: 0.0254\n",
            "Epoch 5/150\n",
            "515/515 [==============================] - 6s 11ms/step - loss: 0.6763 - binary_crossentropy: 0.0236 - val_loss: 0.6723 - val_binary_crossentropy: 0.0236\n",
            "Epoch 6/150\n",
            "515/515 [==============================] - 6s 11ms/step - loss: 0.6672 - binary_crossentropy: 0.0206 - val_loss: 0.6658 - val_binary_crossentropy: 0.0210\n",
            "Epoch 7/150\n",
            "515/515 [==============================] - 6s 11ms/step - loss: 0.6631 - binary_crossentropy: 0.0193 - val_loss: 0.6621 - val_binary_crossentropy: 0.0191\n",
            "Epoch 8/150\n",
            "515/515 [==============================] - 6s 11ms/step - loss: 0.6610 - binary_crossentropy: 0.0185 - val_loss: 0.6606 - val_binary_crossentropy: 0.0186\n",
            "Epoch 9/150\n",
            "515/515 [==============================] - 6s 11ms/step - loss: 0.6597 - binary_crossentropy: 0.0179 - val_loss: 0.6595 - val_binary_crossentropy: 0.0180\n",
            "Epoch 10/150\n",
            "515/515 [==============================] - 6s 12ms/step - loss: 0.6589 - binary_crossentropy: 0.0176 - val_loss: 0.6592 - val_binary_crossentropy: 0.0179\n",
            "Epoch 11/150\n",
            "515/515 [==============================] - 6s 11ms/step - loss: 0.6584 - binary_crossentropy: 0.0174 - val_loss: 0.6585 - val_binary_crossentropy: 0.0175\n",
            "Epoch 12/150\n",
            "515/515 [==============================] - 6s 11ms/step - loss: 0.6581 - binary_crossentropy: 0.0171 - val_loss: 0.6584 - val_binary_crossentropy: 0.0174\n",
            "Epoch 13/150\n",
            "515/515 [==============================] - 6s 11ms/step - loss: 0.6578 - binary_crossentropy: 0.0170 - val_loss: 0.6581 - val_binary_crossentropy: 0.0173\n",
            "Epoch 14/150\n",
            "515/515 [==============================] - 6s 11ms/step - loss: 0.6575 - binary_crossentropy: 0.0168 - val_loss: 0.6579 - val_binary_crossentropy: 0.0171\n",
            "Epoch 15/150\n",
            "515/515 [==============================] - 6s 11ms/step - loss: 0.6574 - binary_crossentropy: 0.0168 - val_loss: 0.6581 - val_binary_crossentropy: 0.0175\n",
            "Epoch 16/150\n",
            "515/515 [==============================] - 6s 12ms/step - loss: 0.6573 - binary_crossentropy: 0.0166 - val_loss: 0.6581 - val_binary_crossentropy: 0.0175\n",
            "Epoch 17/150\n",
            "515/515 [==============================] - 5s 11ms/step - loss: 0.6572 - binary_crossentropy: 0.0165 - val_loss: 0.6581 - val_binary_crossentropy: 0.0175\n",
            "Epoch 18/150\n",
            "515/515 [==============================] - 6s 11ms/step - loss: 0.6571 - binary_crossentropy: 0.0165 - val_loss: 0.6582 - val_binary_crossentropy: 0.0176\n",
            "Epoch 19/150\n",
            "515/515 [==============================] - 6s 11ms/step - loss: 0.6569 - binary_crossentropy: 0.0164 - val_loss: 0.6576 - val_binary_crossentropy: 0.0170\n",
            "Epoch 20/150\n",
            "515/515 [==============================] - 6s 11ms/step - loss: 0.6569 - binary_crossentropy: 0.0163 - val_loss: 0.6581 - val_binary_crossentropy: 0.0176\n",
            "Epoch 21/150\n",
            "515/515 [==============================] - 6s 11ms/step - loss: 0.6568 - binary_crossentropy: 0.0162 - val_loss: 0.6577 - val_binary_crossentropy: 0.0172\n",
            "Epoch 22/150\n",
            "515/515 [==============================] - 6s 12ms/step - loss: 0.6567 - binary_crossentropy: 0.0162 - val_loss: 0.6578 - val_binary_crossentropy: 0.0173\n",
            "Epoch 23/150\n",
            "515/515 [==============================] - 6s 11ms/step - loss: 0.6567 - binary_crossentropy: 0.0161 - val_loss: 0.6577 - val_binary_crossentropy: 0.0171\n",
            "Epoch 24/150\n",
            "515/515 [==============================] - 6s 11ms/step - loss: 0.6567 - binary_crossentropy: 0.0161 - val_loss: 0.6579 - val_binary_crossentropy: 0.0172\n",
            "Epoch 25/150\n",
            "515/515 [==============================] - 6s 11ms/step - loss: 0.6556 - binary_crossentropy: 0.0151 - val_loss: 0.6571 - val_binary_crossentropy: 0.0167\n",
            "Epoch 26/150\n",
            "515/515 [==============================] - 6s 11ms/step - loss: 0.6551 - binary_crossentropy: 0.0147 - val_loss: 0.6570 - val_binary_crossentropy: 0.0166\n",
            "Epoch 27/150\n",
            "515/515 [==============================] - 6s 11ms/step - loss: 0.6548 - binary_crossentropy: 0.0145 - val_loss: 0.6569 - val_binary_crossentropy: 0.0167\n",
            "Epoch 28/150\n",
            "515/515 [==============================] - 6s 11ms/step - loss: 0.6545 - binary_crossentropy: 0.0144 - val_loss: 0.6568 - val_binary_crossentropy: 0.0166\n",
            "Epoch 29/150\n",
            "515/515 [==============================] - 6s 11ms/step - loss: 0.6543 - binary_crossentropy: 0.0142 - val_loss: 0.6567 - val_binary_crossentropy: 0.0167\n",
            "Epoch 30/150\n",
            "515/515 [==============================] - 6s 11ms/step - loss: 0.6541 - binary_crossentropy: 0.0141 - val_loss: 0.6567 - val_binary_crossentropy: 0.0167\n",
            "Epoch 31/150\n",
            "515/515 [==============================] - 6s 11ms/step - loss: 0.6539 - binary_crossentropy: 0.0140 - val_loss: 0.6566 - val_binary_crossentropy: 0.0167\n",
            "Epoch 32/150\n",
            "515/515 [==============================] - 6s 11ms/step - loss: 0.6538 - binary_crossentropy: 0.0139 - val_loss: 0.6566 - val_binary_crossentropy: 0.0167\n",
            "Epoch 33/150\n",
            "515/515 [==============================] - 6s 11ms/step - loss: 0.6536 - binary_crossentropy: 0.0138 - val_loss: 0.6565 - val_binary_crossentropy: 0.0167\n",
            "Epoch 34/150\n",
            "515/515 [==============================] - 6s 11ms/step - loss: 0.6535 - binary_crossentropy: 0.0138 - val_loss: 0.6565 - val_binary_crossentropy: 0.0167\n",
            "Epoch 35/150\n",
            "515/515 [==============================] - 6s 11ms/step - loss: 0.6534 - binary_crossentropy: 0.0136 - val_loss: 0.6565 - val_binary_crossentropy: 0.0168\n",
            "Epoch 36/150\n",
            "515/515 [==============================] - 6s 11ms/step - loss: 0.6532 - binary_crossentropy: 0.0135 - val_loss: 0.6565 - val_binary_crossentropy: 0.0168\n",
            "Epoch 37/150\n",
            "515/515 [==============================] - 6s 11ms/step - loss: 0.6531 - binary_crossentropy: 0.0135 - val_loss: 0.6564 - val_binary_crossentropy: 0.0168\n",
            "Epoch 38/150\n",
            "515/515 [==============================] - 6s 11ms/step - loss: 0.6530 - binary_crossentropy: 0.0134 - val_loss: 0.6564 - val_binary_crossentropy: 0.0168\n",
            "Epoch 39/150\n",
            "515/515 [==============================] - 6s 11ms/step - loss: 0.6529 - binary_crossentropy: 0.0133 - val_loss: 0.6564 - val_binary_crossentropy: 0.0168\n",
            "Epoch 40/150\n",
            "515/515 [==============================] - 6s 11ms/step - loss: 0.6528 - binary_crossentropy: 0.0132 - val_loss: 0.6564 - val_binary_crossentropy: 0.0168\n",
            "Epoch 41/150\n",
            "515/515 [==============================] - 6s 11ms/step - loss: 0.6527 - binary_crossentropy: 0.0132 - val_loss: 0.6564 - val_binary_crossentropy: 0.0168\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7b96162cf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Ksfh_xIsGDpX"
      },
      "source": [
        "prediction = model.predict(X_pred)\n",
        "df_preds_non_ctl =  pd.DataFrame(prediction, columns= cols_target, index = to_pred.index)\n",
        "\n",
        "# concat with all to pred values\n",
        "df_preds = pd.concat([ full_pred[cols_id], df_preds_non_ctl], axis = 1).fillna(0)\n",
        "\n",
        "df_preds.iloc[:,[34,82]] = 0\n",
        "# to csv\n",
        "df_preds.to_csv(\"submission.csv\", index = None)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ZRu1KZ8WGDpX",
        "outputId": "7e5e4e0e-2c65-4839-c867-1437218e5ff4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        }
      },
      "source": [
        "df_preds"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sig_id</th>\n",
              "      <th>5-alpha_reductase_inhibitor</th>\n",
              "      <th>11-beta-hsd1_inhibitor</th>\n",
              "      <th>acat_inhibitor</th>\n",
              "      <th>acetylcholine_receptor_agonist</th>\n",
              "      <th>acetylcholine_receptor_antagonist</th>\n",
              "      <th>acetylcholinesterase_inhibitor</th>\n",
              "      <th>adenosine_receptor_agonist</th>\n",
              "      <th>adenosine_receptor_antagonist</th>\n",
              "      <th>adenylyl_cyclase_activator</th>\n",
              "      <th>adrenergic_receptor_agonist</th>\n",
              "      <th>adrenergic_receptor_antagonist</th>\n",
              "      <th>akt_inhibitor</th>\n",
              "      <th>aldehyde_dehydrogenase_inhibitor</th>\n",
              "      <th>alk_inhibitor</th>\n",
              "      <th>ampk_activator</th>\n",
              "      <th>analgesic</th>\n",
              "      <th>androgen_receptor_agonist</th>\n",
              "      <th>androgen_receptor_antagonist</th>\n",
              "      <th>anesthetic_-_local</th>\n",
              "      <th>angiogenesis_inhibitor</th>\n",
              "      <th>angiotensin_receptor_antagonist</th>\n",
              "      <th>anti-inflammatory</th>\n",
              "      <th>antiarrhythmic</th>\n",
              "      <th>antibiotic</th>\n",
              "      <th>anticonvulsant</th>\n",
              "      <th>antifungal</th>\n",
              "      <th>antihistamine</th>\n",
              "      <th>antimalarial</th>\n",
              "      <th>antioxidant</th>\n",
              "      <th>antiprotozoal</th>\n",
              "      <th>antiviral</th>\n",
              "      <th>apoptosis_stimulant</th>\n",
              "      <th>aromatase_inhibitor</th>\n",
              "      <th>atm_kinase_inhibitor</th>\n",
              "      <th>atp-sensitive_potassium_channel_antagonist</th>\n",
              "      <th>atp_synthase_inhibitor</th>\n",
              "      <th>atpase_inhibitor</th>\n",
              "      <th>atr_kinase_inhibitor</th>\n",
              "      <th>aurora_kinase_inhibitor</th>\n",
              "      <th>...</th>\n",
              "      <th>protein_synthesis_inhibitor</th>\n",
              "      <th>protein_tyrosine_kinase_inhibitor</th>\n",
              "      <th>radiopaque_medium</th>\n",
              "      <th>raf_inhibitor</th>\n",
              "      <th>ras_gtpase_inhibitor</th>\n",
              "      <th>retinoid_receptor_agonist</th>\n",
              "      <th>retinoid_receptor_antagonist</th>\n",
              "      <th>rho_associated_kinase_inhibitor</th>\n",
              "      <th>ribonucleoside_reductase_inhibitor</th>\n",
              "      <th>rna_polymerase_inhibitor</th>\n",
              "      <th>serotonin_receptor_agonist</th>\n",
              "      <th>serotonin_receptor_antagonist</th>\n",
              "      <th>serotonin_reuptake_inhibitor</th>\n",
              "      <th>sigma_receptor_agonist</th>\n",
              "      <th>sigma_receptor_antagonist</th>\n",
              "      <th>smoothened_receptor_antagonist</th>\n",
              "      <th>sodium_channel_inhibitor</th>\n",
              "      <th>sphingosine_receptor_agonist</th>\n",
              "      <th>src_inhibitor</th>\n",
              "      <th>steroid</th>\n",
              "      <th>syk_inhibitor</th>\n",
              "      <th>tachykinin_antagonist</th>\n",
              "      <th>tgf-beta_receptor_inhibitor</th>\n",
              "      <th>thrombin_inhibitor</th>\n",
              "      <th>thymidylate_synthase_inhibitor</th>\n",
              "      <th>tlr_agonist</th>\n",
              "      <th>tlr_antagonist</th>\n",
              "      <th>tnf_inhibitor</th>\n",
              "      <th>topoisomerase_inhibitor</th>\n",
              "      <th>transient_receptor_potential_channel_antagonist</th>\n",
              "      <th>tropomyosin_receptor_kinase_inhibitor</th>\n",
              "      <th>trpv_agonist</th>\n",
              "      <th>trpv_antagonist</th>\n",
              "      <th>tubulin_inhibitor</th>\n",
              "      <th>tyrosine_kinase_inhibitor</th>\n",
              "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
              "      <th>vegfr_inhibitor</th>\n",
              "      <th>vitamin_b</th>\n",
              "      <th>vitamin_d_receptor_agonist</th>\n",
              "      <th>wnt_inhibitor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id_0004d9e33</td>\n",
              "      <td>0.005583</td>\n",
              "      <td>0.001314</td>\n",
              "      <td>0.001375</td>\n",
              "      <td>0.012943</td>\n",
              "      <td>0.038262</td>\n",
              "      <td>0.005347</td>\n",
              "      <td>0.004701</td>\n",
              "      <td>0.004893</td>\n",
              "      <td>0.000287</td>\n",
              "      <td>0.011884</td>\n",
              "      <td>0.019809</td>\n",
              "      <td>0.000697</td>\n",
              "      <td>0.000481</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.001719</td>\n",
              "      <td>0.000897</td>\n",
              "      <td>0.004366</td>\n",
              "      <td>0.008201</td>\n",
              "      <td>0.008962</td>\n",
              "      <td>0.002465</td>\n",
              "      <td>0.002560</td>\n",
              "      <td>0.006375</td>\n",
              "      <td>0.000394</td>\n",
              "      <td>0.002246</td>\n",
              "      <td>0.000680</td>\n",
              "      <td>0.000542</td>\n",
              "      <td>0.001457</td>\n",
              "      <td>0.001167</td>\n",
              "      <td>0.010180</td>\n",
              "      <td>0.003589</td>\n",
              "      <td>0.002711</td>\n",
              "      <td>0.003168</td>\n",
              "      <td>0.005202</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000672</td>\n",
              "      <td>0.000414</td>\n",
              "      <td>0.002110</td>\n",
              "      <td>0.000349</td>\n",
              "      <td>0.000867</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003634</td>\n",
              "      <td>0.001439</td>\n",
              "      <td>0.008521</td>\n",
              "      <td>0.000562</td>\n",
              "      <td>0.000717</td>\n",
              "      <td>0.005229</td>\n",
              "      <td>0.000731</td>\n",
              "      <td>0.001123</td>\n",
              "      <td>0.001188</td>\n",
              "      <td>0.001752</td>\n",
              "      <td>0.013692</td>\n",
              "      <td>0.015655</td>\n",
              "      <td>0.003518</td>\n",
              "      <td>0.006351</td>\n",
              "      <td>0.002917</td>\n",
              "      <td>0.001300</td>\n",
              "      <td>0.029287</td>\n",
              "      <td>0.002469</td>\n",
              "      <td>0.000672</td>\n",
              "      <td>0.000559</td>\n",
              "      <td>0.000298</td>\n",
              "      <td>0.003552</td>\n",
              "      <td>0.000776</td>\n",
              "      <td>0.001336</td>\n",
              "      <td>0.001713</td>\n",
              "      <td>0.004114</td>\n",
              "      <td>0.001161</td>\n",
              "      <td>0.001678</td>\n",
              "      <td>0.002045</td>\n",
              "      <td>0.001615</td>\n",
              "      <td>0.000935</td>\n",
              "      <td>0.000554</td>\n",
              "      <td>0.003373</td>\n",
              "      <td>0.001401</td>\n",
              "      <td>0.000506</td>\n",
              "      <td>0.000892</td>\n",
              "      <td>0.001066</td>\n",
              "      <td>0.001678</td>\n",
              "      <td>0.001374</td>\n",
              "      <td>0.001015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id_001897cda</td>\n",
              "      <td>0.000153</td>\n",
              "      <td>0.000285</td>\n",
              "      <td>0.000130</td>\n",
              "      <td>0.000326</td>\n",
              "      <td>0.000113</td>\n",
              "      <td>0.000206</td>\n",
              "      <td>0.000898</td>\n",
              "      <td>0.006111</td>\n",
              "      <td>0.026226</td>\n",
              "      <td>0.003814</td>\n",
              "      <td>0.002276</td>\n",
              "      <td>0.001464</td>\n",
              "      <td>0.000102</td>\n",
              "      <td>0.004161</td>\n",
              "      <td>0.000324</td>\n",
              "      <td>0.000481</td>\n",
              "      <td>0.000167</td>\n",
              "      <td>0.001675</td>\n",
              "      <td>0.000608</td>\n",
              "      <td>0.000335</td>\n",
              "      <td>0.001512</td>\n",
              "      <td>0.000610</td>\n",
              "      <td>0.000814</td>\n",
              "      <td>0.000488</td>\n",
              "      <td>0.000559</td>\n",
              "      <td>0.001593</td>\n",
              "      <td>0.000165</td>\n",
              "      <td>0.000060</td>\n",
              "      <td>0.000229</td>\n",
              "      <td>0.000647</td>\n",
              "      <td>0.000571</td>\n",
              "      <td>0.000906</td>\n",
              "      <td>0.000417</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000387</td>\n",
              "      <td>0.000109</td>\n",
              "      <td>0.003098</td>\n",
              "      <td>0.001348</td>\n",
              "      <td>0.002504</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000191</td>\n",
              "      <td>0.000461</td>\n",
              "      <td>0.000070</td>\n",
              "      <td>0.000590</td>\n",
              "      <td>0.001441</td>\n",
              "      <td>0.002988</td>\n",
              "      <td>0.000317</td>\n",
              "      <td>0.012005</td>\n",
              "      <td>0.000143</td>\n",
              "      <td>0.004533</td>\n",
              "      <td>0.003265</td>\n",
              "      <td>0.002142</td>\n",
              "      <td>0.000332</td>\n",
              "      <td>0.000335</td>\n",
              "      <td>0.001604</td>\n",
              "      <td>0.000362</td>\n",
              "      <td>0.001501</td>\n",
              "      <td>0.000125</td>\n",
              "      <td>0.037203</td>\n",
              "      <td>0.000179</td>\n",
              "      <td>0.011030</td>\n",
              "      <td>0.001294</td>\n",
              "      <td>0.003802</td>\n",
              "      <td>0.000623</td>\n",
              "      <td>0.000398</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>0.002493</td>\n",
              "      <td>0.000642</td>\n",
              "      <td>0.005461</td>\n",
              "      <td>0.001560</td>\n",
              "      <td>0.000421</td>\n",
              "      <td>0.000457</td>\n",
              "      <td>0.001082</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.009953</td>\n",
              "      <td>0.000258</td>\n",
              "      <td>0.007813</td>\n",
              "      <td>0.000701</td>\n",
              "      <td>0.002730</td>\n",
              "      <td>0.002760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id_002429b5b</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>id_00276f245</td>\n",
              "      <td>0.000339</td>\n",
              "      <td>0.001508</td>\n",
              "      <td>0.004201</td>\n",
              "      <td>0.024830</td>\n",
              "      <td>0.027415</td>\n",
              "      <td>0.003785</td>\n",
              "      <td>0.004537</td>\n",
              "      <td>0.003072</td>\n",
              "      <td>0.000276</td>\n",
              "      <td>0.010542</td>\n",
              "      <td>0.020311</td>\n",
              "      <td>0.002696</td>\n",
              "      <td>0.000810</td>\n",
              "      <td>0.002649</td>\n",
              "      <td>0.001252</td>\n",
              "      <td>0.003805</td>\n",
              "      <td>0.005163</td>\n",
              "      <td>0.005585</td>\n",
              "      <td>0.003117</td>\n",
              "      <td>0.001961</td>\n",
              "      <td>0.002397</td>\n",
              "      <td>0.002115</td>\n",
              "      <td>0.002040</td>\n",
              "      <td>0.003319</td>\n",
              "      <td>0.001848</td>\n",
              "      <td>0.003168</td>\n",
              "      <td>0.001286</td>\n",
              "      <td>0.004291</td>\n",
              "      <td>0.004296</td>\n",
              "      <td>0.000896</td>\n",
              "      <td>0.001365</td>\n",
              "      <td>0.003787</td>\n",
              "      <td>0.001230</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000838</td>\n",
              "      <td>0.000428</td>\n",
              "      <td>0.004539</td>\n",
              "      <td>0.001240</td>\n",
              "      <td>0.001080</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008176</td>\n",
              "      <td>0.001016</td>\n",
              "      <td>0.002726</td>\n",
              "      <td>0.000381</td>\n",
              "      <td>0.001376</td>\n",
              "      <td>0.000193</td>\n",
              "      <td>0.001682</td>\n",
              "      <td>0.001058</td>\n",
              "      <td>0.001607</td>\n",
              "      <td>0.000474</td>\n",
              "      <td>0.013486</td>\n",
              "      <td>0.061702</td>\n",
              "      <td>0.001826</td>\n",
              "      <td>0.001736</td>\n",
              "      <td>0.002394</td>\n",
              "      <td>0.001013</td>\n",
              "      <td>0.005602</td>\n",
              "      <td>0.001114</td>\n",
              "      <td>0.005080</td>\n",
              "      <td>0.001129</td>\n",
              "      <td>0.001260</td>\n",
              "      <td>0.005377</td>\n",
              "      <td>0.000486</td>\n",
              "      <td>0.001015</td>\n",
              "      <td>0.002644</td>\n",
              "      <td>0.001268</td>\n",
              "      <td>0.000461</td>\n",
              "      <td>0.000767</td>\n",
              "      <td>0.000400</td>\n",
              "      <td>0.001116</td>\n",
              "      <td>0.000586</td>\n",
              "      <td>0.002631</td>\n",
              "      <td>0.006095</td>\n",
              "      <td>0.090069</td>\n",
              "      <td>0.009991</td>\n",
              "      <td>0.001240</td>\n",
              "      <td>0.002370</td>\n",
              "      <td>0.003181</td>\n",
              "      <td>0.000486</td>\n",
              "      <td>0.002984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>id_0027f1083</td>\n",
              "      <td>0.002219</td>\n",
              "      <td>0.001370</td>\n",
              "      <td>0.001572</td>\n",
              "      <td>0.027905</td>\n",
              "      <td>0.028901</td>\n",
              "      <td>0.005148</td>\n",
              "      <td>0.004459</td>\n",
              "      <td>0.002862</td>\n",
              "      <td>0.000530</td>\n",
              "      <td>0.013277</td>\n",
              "      <td>0.027453</td>\n",
              "      <td>0.000986</td>\n",
              "      <td>0.000345</td>\n",
              "      <td>0.000489</td>\n",
              "      <td>0.001252</td>\n",
              "      <td>0.002040</td>\n",
              "      <td>0.002826</td>\n",
              "      <td>0.006799</td>\n",
              "      <td>0.005734</td>\n",
              "      <td>0.001926</td>\n",
              "      <td>0.003484</td>\n",
              "      <td>0.004632</td>\n",
              "      <td>0.000560</td>\n",
              "      <td>0.002293</td>\n",
              "      <td>0.000861</td>\n",
              "      <td>0.000454</td>\n",
              "      <td>0.001037</td>\n",
              "      <td>0.001120</td>\n",
              "      <td>0.007600</td>\n",
              "      <td>0.003658</td>\n",
              "      <td>0.001751</td>\n",
              "      <td>0.001908</td>\n",
              "      <td>0.003670</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000624</td>\n",
              "      <td>0.000296</td>\n",
              "      <td>0.001807</td>\n",
              "      <td>0.000338</td>\n",
              "      <td>0.000835</td>\n",
              "      <td>...</td>\n",
              "      <td>0.004405</td>\n",
              "      <td>0.001010</td>\n",
              "      <td>0.006172</td>\n",
              "      <td>0.001003</td>\n",
              "      <td>0.000745</td>\n",
              "      <td>0.001143</td>\n",
              "      <td>0.000757</td>\n",
              "      <td>0.000901</td>\n",
              "      <td>0.001689</td>\n",
              "      <td>0.001923</td>\n",
              "      <td>0.014402</td>\n",
              "      <td>0.015854</td>\n",
              "      <td>0.003836</td>\n",
              "      <td>0.003672</td>\n",
              "      <td>0.002404</td>\n",
              "      <td>0.001787</td>\n",
              "      <td>0.018058</td>\n",
              "      <td>0.002871</td>\n",
              "      <td>0.000395</td>\n",
              "      <td>0.001222</td>\n",
              "      <td>0.000445</td>\n",
              "      <td>0.003985</td>\n",
              "      <td>0.000225</td>\n",
              "      <td>0.001228</td>\n",
              "      <td>0.002016</td>\n",
              "      <td>0.002803</td>\n",
              "      <td>0.001454</td>\n",
              "      <td>0.001770</td>\n",
              "      <td>0.001558</td>\n",
              "      <td>0.000772</td>\n",
              "      <td>0.000809</td>\n",
              "      <td>0.000543</td>\n",
              "      <td>0.003003</td>\n",
              "      <td>0.001384</td>\n",
              "      <td>0.000894</td>\n",
              "      <td>0.000846</td>\n",
              "      <td>0.001032</td>\n",
              "      <td>0.002667</td>\n",
              "      <td>0.000518</td>\n",
              "      <td>0.002411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3977</th>\n",
              "      <td>id_ff7004b87</td>\n",
              "      <td>0.002043</td>\n",
              "      <td>0.000896</td>\n",
              "      <td>0.002036</td>\n",
              "      <td>0.001796</td>\n",
              "      <td>0.002569</td>\n",
              "      <td>0.002002</td>\n",
              "      <td>0.001937</td>\n",
              "      <td>0.016592</td>\n",
              "      <td>0.000395</td>\n",
              "      <td>0.002073</td>\n",
              "      <td>0.009302</td>\n",
              "      <td>0.000984</td>\n",
              "      <td>0.001366</td>\n",
              "      <td>0.064894</td>\n",
              "      <td>0.001582</td>\n",
              "      <td>0.000911</td>\n",
              "      <td>0.002354</td>\n",
              "      <td>0.005767</td>\n",
              "      <td>0.007967</td>\n",
              "      <td>0.003669</td>\n",
              "      <td>0.001629</td>\n",
              "      <td>0.005205</td>\n",
              "      <td>0.001331</td>\n",
              "      <td>0.003668</td>\n",
              "      <td>0.001320</td>\n",
              "      <td>0.010669</td>\n",
              "      <td>0.000819</td>\n",
              "      <td>0.002493</td>\n",
              "      <td>0.002502</td>\n",
              "      <td>0.001016</td>\n",
              "      <td>0.001388</td>\n",
              "      <td>0.007436</td>\n",
              "      <td>0.000578</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000678</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.011747</td>\n",
              "      <td>0.005090</td>\n",
              "      <td>0.018431</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006263</td>\n",
              "      <td>0.003121</td>\n",
              "      <td>0.000847</td>\n",
              "      <td>0.000067</td>\n",
              "      <td>0.001445</td>\n",
              "      <td>0.013301</td>\n",
              "      <td>0.002009</td>\n",
              "      <td>0.004706</td>\n",
              "      <td>0.000437</td>\n",
              "      <td>0.001644</td>\n",
              "      <td>0.010887</td>\n",
              "      <td>0.007552</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.001487</td>\n",
              "      <td>0.007364</td>\n",
              "      <td>0.000290</td>\n",
              "      <td>0.004812</td>\n",
              "      <td>0.000371</td>\n",
              "      <td>0.053012</td>\n",
              "      <td>0.000635</td>\n",
              "      <td>0.001610</td>\n",
              "      <td>0.003242</td>\n",
              "      <td>0.011000</td>\n",
              "      <td>0.001929</td>\n",
              "      <td>0.000435</td>\n",
              "      <td>0.003177</td>\n",
              "      <td>0.000527</td>\n",
              "      <td>0.002659</td>\n",
              "      <td>0.002053</td>\n",
              "      <td>0.007888</td>\n",
              "      <td>0.000619</td>\n",
              "      <td>0.006065</td>\n",
              "      <td>0.002772</td>\n",
              "      <td>0.008554</td>\n",
              "      <td>0.010210</td>\n",
              "      <td>0.001724</td>\n",
              "      <td>0.005813</td>\n",
              "      <td>0.001956</td>\n",
              "      <td>0.013887</td>\n",
              "      <td>0.001542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3978</th>\n",
              "      <td>id_ff925dd0d</td>\n",
              "      <td>0.001824</td>\n",
              "      <td>0.001039</td>\n",
              "      <td>0.001424</td>\n",
              "      <td>0.013572</td>\n",
              "      <td>0.035148</td>\n",
              "      <td>0.006389</td>\n",
              "      <td>0.006325</td>\n",
              "      <td>0.003257</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.023029</td>\n",
              "      <td>0.027248</td>\n",
              "      <td>0.001301</td>\n",
              "      <td>0.000354</td>\n",
              "      <td>0.000598</td>\n",
              "      <td>0.000811</td>\n",
              "      <td>0.000597</td>\n",
              "      <td>0.003997</td>\n",
              "      <td>0.007639</td>\n",
              "      <td>0.005331</td>\n",
              "      <td>0.002295</td>\n",
              "      <td>0.001311</td>\n",
              "      <td>0.003449</td>\n",
              "      <td>0.000549</td>\n",
              "      <td>0.002375</td>\n",
              "      <td>0.001250</td>\n",
              "      <td>0.000569</td>\n",
              "      <td>0.001356</td>\n",
              "      <td>0.000823</td>\n",
              "      <td>0.003880</td>\n",
              "      <td>0.004327</td>\n",
              "      <td>0.001595</td>\n",
              "      <td>0.001318</td>\n",
              "      <td>0.006148</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000421</td>\n",
              "      <td>0.000270</td>\n",
              "      <td>0.002156</td>\n",
              "      <td>0.000433</td>\n",
              "      <td>0.001544</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003718</td>\n",
              "      <td>0.001208</td>\n",
              "      <td>0.007023</td>\n",
              "      <td>0.001011</td>\n",
              "      <td>0.000789</td>\n",
              "      <td>0.000679</td>\n",
              "      <td>0.000594</td>\n",
              "      <td>0.001646</td>\n",
              "      <td>0.001264</td>\n",
              "      <td>0.002783</td>\n",
              "      <td>0.015866</td>\n",
              "      <td>0.029523</td>\n",
              "      <td>0.002812</td>\n",
              "      <td>0.003039</td>\n",
              "      <td>0.002030</td>\n",
              "      <td>0.002022</td>\n",
              "      <td>0.029872</td>\n",
              "      <td>0.000854</td>\n",
              "      <td>0.000770</td>\n",
              "      <td>0.000368</td>\n",
              "      <td>0.000865</td>\n",
              "      <td>0.004369</td>\n",
              "      <td>0.000451</td>\n",
              "      <td>0.001638</td>\n",
              "      <td>0.001169</td>\n",
              "      <td>0.002253</td>\n",
              "      <td>0.000757</td>\n",
              "      <td>0.001644</td>\n",
              "      <td>0.001671</td>\n",
              "      <td>0.000720</td>\n",
              "      <td>0.001177</td>\n",
              "      <td>0.000762</td>\n",
              "      <td>0.003477</td>\n",
              "      <td>0.001658</td>\n",
              "      <td>0.001032</td>\n",
              "      <td>0.000691</td>\n",
              "      <td>0.003784</td>\n",
              "      <td>0.001897</td>\n",
              "      <td>0.000146</td>\n",
              "      <td>0.001413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3979</th>\n",
              "      <td>id_ffb710450</td>\n",
              "      <td>0.004286</td>\n",
              "      <td>0.001245</td>\n",
              "      <td>0.001146</td>\n",
              "      <td>0.012885</td>\n",
              "      <td>0.042563</td>\n",
              "      <td>0.006226</td>\n",
              "      <td>0.004335</td>\n",
              "      <td>0.003914</td>\n",
              "      <td>0.000277</td>\n",
              "      <td>0.012340</td>\n",
              "      <td>0.023879</td>\n",
              "      <td>0.000739</td>\n",
              "      <td>0.000394</td>\n",
              "      <td>0.000526</td>\n",
              "      <td>0.001341</td>\n",
              "      <td>0.000815</td>\n",
              "      <td>0.003701</td>\n",
              "      <td>0.010694</td>\n",
              "      <td>0.007800</td>\n",
              "      <td>0.001816</td>\n",
              "      <td>0.002799</td>\n",
              "      <td>0.006547</td>\n",
              "      <td>0.000519</td>\n",
              "      <td>0.002232</td>\n",
              "      <td>0.000866</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>0.001441</td>\n",
              "      <td>0.001237</td>\n",
              "      <td>0.008829</td>\n",
              "      <td>0.003538</td>\n",
              "      <td>0.002007</td>\n",
              "      <td>0.002584</td>\n",
              "      <td>0.006183</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000549</td>\n",
              "      <td>0.000341</td>\n",
              "      <td>0.001689</td>\n",
              "      <td>0.000337</td>\n",
              "      <td>0.000788</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003562</td>\n",
              "      <td>0.001197</td>\n",
              "      <td>0.008294</td>\n",
              "      <td>0.000422</td>\n",
              "      <td>0.000818</td>\n",
              "      <td>0.002400</td>\n",
              "      <td>0.000636</td>\n",
              "      <td>0.000805</td>\n",
              "      <td>0.001234</td>\n",
              "      <td>0.002015</td>\n",
              "      <td>0.013319</td>\n",
              "      <td>0.024542</td>\n",
              "      <td>0.003698</td>\n",
              "      <td>0.006147</td>\n",
              "      <td>0.003478</td>\n",
              "      <td>0.001538</td>\n",
              "      <td>0.024966</td>\n",
              "      <td>0.002138</td>\n",
              "      <td>0.000491</td>\n",
              "      <td>0.000602</td>\n",
              "      <td>0.000293</td>\n",
              "      <td>0.005471</td>\n",
              "      <td>0.000410</td>\n",
              "      <td>0.001735</td>\n",
              "      <td>0.001828</td>\n",
              "      <td>0.003462</td>\n",
              "      <td>0.001234</td>\n",
              "      <td>0.001536</td>\n",
              "      <td>0.001706</td>\n",
              "      <td>0.001074</td>\n",
              "      <td>0.000937</td>\n",
              "      <td>0.000546</td>\n",
              "      <td>0.003513</td>\n",
              "      <td>0.001212</td>\n",
              "      <td>0.000457</td>\n",
              "      <td>0.000755</td>\n",
              "      <td>0.000890</td>\n",
              "      <td>0.002110</td>\n",
              "      <td>0.000788</td>\n",
              "      <td>0.001246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3980</th>\n",
              "      <td>id_ffbb869f2</td>\n",
              "      <td>0.002017</td>\n",
              "      <td>0.000637</td>\n",
              "      <td>0.000932</td>\n",
              "      <td>0.010708</td>\n",
              "      <td>0.019955</td>\n",
              "      <td>0.006708</td>\n",
              "      <td>0.003201</td>\n",
              "      <td>0.003336</td>\n",
              "      <td>0.000736</td>\n",
              "      <td>0.021404</td>\n",
              "      <td>0.033875</td>\n",
              "      <td>0.000916</td>\n",
              "      <td>0.000299</td>\n",
              "      <td>0.000342</td>\n",
              "      <td>0.000870</td>\n",
              "      <td>0.000678</td>\n",
              "      <td>0.002881</td>\n",
              "      <td>0.006156</td>\n",
              "      <td>0.004142</td>\n",
              "      <td>0.001493</td>\n",
              "      <td>0.002578</td>\n",
              "      <td>0.002299</td>\n",
              "      <td>0.000596</td>\n",
              "      <td>0.001303</td>\n",
              "      <td>0.001068</td>\n",
              "      <td>0.000321</td>\n",
              "      <td>0.001050</td>\n",
              "      <td>0.000967</td>\n",
              "      <td>0.008729</td>\n",
              "      <td>0.003294</td>\n",
              "      <td>0.001480</td>\n",
              "      <td>0.001671</td>\n",
              "      <td>0.005976</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000302</td>\n",
              "      <td>0.000429</td>\n",
              "      <td>0.001941</td>\n",
              "      <td>0.000228</td>\n",
              "      <td>0.000317</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002834</td>\n",
              "      <td>0.000779</td>\n",
              "      <td>0.003743</td>\n",
              "      <td>0.000536</td>\n",
              "      <td>0.000737</td>\n",
              "      <td>0.002642</td>\n",
              "      <td>0.000386</td>\n",
              "      <td>0.000634</td>\n",
              "      <td>0.001495</td>\n",
              "      <td>0.001997</td>\n",
              "      <td>0.014127</td>\n",
              "      <td>0.023513</td>\n",
              "      <td>0.002996</td>\n",
              "      <td>0.003581</td>\n",
              "      <td>0.001584</td>\n",
              "      <td>0.002494</td>\n",
              "      <td>0.019857</td>\n",
              "      <td>0.001789</td>\n",
              "      <td>0.000369</td>\n",
              "      <td>0.000309</td>\n",
              "      <td>0.000356</td>\n",
              "      <td>0.005165</td>\n",
              "      <td>0.000342</td>\n",
              "      <td>0.001217</td>\n",
              "      <td>0.002109</td>\n",
              "      <td>0.002304</td>\n",
              "      <td>0.000795</td>\n",
              "      <td>0.001303</td>\n",
              "      <td>0.001831</td>\n",
              "      <td>0.001075</td>\n",
              "      <td>0.000506</td>\n",
              "      <td>0.000666</td>\n",
              "      <td>0.003080</td>\n",
              "      <td>0.000450</td>\n",
              "      <td>0.000767</td>\n",
              "      <td>0.000438</td>\n",
              "      <td>0.001073</td>\n",
              "      <td>0.002346</td>\n",
              "      <td>0.000643</td>\n",
              "      <td>0.002790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3981</th>\n",
              "      <td>id_ffd5800b6</td>\n",
              "      <td>0.003208</td>\n",
              "      <td>0.001168</td>\n",
              "      <td>0.001561</td>\n",
              "      <td>0.008538</td>\n",
              "      <td>0.028573</td>\n",
              "      <td>0.005589</td>\n",
              "      <td>0.003272</td>\n",
              "      <td>0.006637</td>\n",
              "      <td>0.000353</td>\n",
              "      <td>0.018022</td>\n",
              "      <td>0.031868</td>\n",
              "      <td>0.001020</td>\n",
              "      <td>0.000485</td>\n",
              "      <td>0.001237</td>\n",
              "      <td>0.001844</td>\n",
              "      <td>0.000882</td>\n",
              "      <td>0.004151</td>\n",
              "      <td>0.006324</td>\n",
              "      <td>0.006842</td>\n",
              "      <td>0.003298</td>\n",
              "      <td>0.002072</td>\n",
              "      <td>0.003090</td>\n",
              "      <td>0.000585</td>\n",
              "      <td>0.002050</td>\n",
              "      <td>0.000799</td>\n",
              "      <td>0.000695</td>\n",
              "      <td>0.001345</td>\n",
              "      <td>0.001847</td>\n",
              "      <td>0.013502</td>\n",
              "      <td>0.002763</td>\n",
              "      <td>0.002078</td>\n",
              "      <td>0.004649</td>\n",
              "      <td>0.003339</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000640</td>\n",
              "      <td>0.000556</td>\n",
              "      <td>0.002255</td>\n",
              "      <td>0.000448</td>\n",
              "      <td>0.000870</td>\n",
              "      <td>...</td>\n",
              "      <td>0.004926</td>\n",
              "      <td>0.001166</td>\n",
              "      <td>0.006077</td>\n",
              "      <td>0.000359</td>\n",
              "      <td>0.001091</td>\n",
              "      <td>0.002956</td>\n",
              "      <td>0.000694</td>\n",
              "      <td>0.001206</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.001099</td>\n",
              "      <td>0.018003</td>\n",
              "      <td>0.034896</td>\n",
              "      <td>0.003107</td>\n",
              "      <td>0.004794</td>\n",
              "      <td>0.003038</td>\n",
              "      <td>0.001003</td>\n",
              "      <td>0.025407</td>\n",
              "      <td>0.001578</td>\n",
              "      <td>0.000889</td>\n",
              "      <td>0.000465</td>\n",
              "      <td>0.000399</td>\n",
              "      <td>0.005060</td>\n",
              "      <td>0.000493</td>\n",
              "      <td>0.001497</td>\n",
              "      <td>0.001943</td>\n",
              "      <td>0.004654</td>\n",
              "      <td>0.000819</td>\n",
              "      <td>0.001654</td>\n",
              "      <td>0.000999</td>\n",
              "      <td>0.002586</td>\n",
              "      <td>0.000584</td>\n",
              "      <td>0.000757</td>\n",
              "      <td>0.003442</td>\n",
              "      <td>0.001279</td>\n",
              "      <td>0.000999</td>\n",
              "      <td>0.000756</td>\n",
              "      <td>0.001073</td>\n",
              "      <td>0.002148</td>\n",
              "      <td>0.001479</td>\n",
              "      <td>0.001429</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3982 rows × 207 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            sig_id  ...  wnt_inhibitor\n",
              "0     id_0004d9e33  ...       0.001015\n",
              "1     id_001897cda  ...       0.002760\n",
              "2     id_002429b5b  ...       0.000000\n",
              "3     id_00276f245  ...       0.002984\n",
              "4     id_0027f1083  ...       0.002411\n",
              "...            ...  ...            ...\n",
              "3977  id_ff7004b87  ...       0.001542\n",
              "3978  id_ff925dd0d  ...       0.001413\n",
              "3979  id_ffb710450  ...       0.001246\n",
              "3980  id_ffbb869f2  ...       0.002790\n",
              "3981  id_ffd5800b6  ...       0.001429\n",
              "\n",
              "[3982 rows x 207 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    }
  ]
}