{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "quanvh8-raw-fts-clr.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "SMdu6HTiKiHa"
      },
      "source": [
        "# KAGGLE\n",
        "folder_path = '../input/lish-moa/'\n",
        "# !cp '../input/custom-funcs-v2/bagging_funcs.py' ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkN9W-ZfK0Am",
        "outputId": "0f315fe2-e5f8-46e4-8f67-eb324e8b2c9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# COLAB\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder_path = '/content/drive/MyDrive/Data/colabs_data/MOA_kaggle/'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "FdYGHJaxKiHb",
        "outputId": "76427959-b19a-432b-d444-01a2e5b4dc7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "'''ENSEMBLE NETS\n",
        "Inspire by https://www.kaggle.com/c/otto-group-product-classification-challenge/discussion/14335'''\n",
        "\n",
        "import numpy as np, pandas as pd, copy, tensorflow as tf, matplotlib.pyplot as plt, sklearn\n",
        "\n",
        "from tensorflow import feature_column as fc\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.layers import (Dense, DenseFeatures, Dropout, \n",
        "                                     BatchNormalization, Embedding, Input, Concatenate, Average,\n",
        "                                     InputLayer, Lambda)\n",
        "from tensorflow.keras.metrics import AUC, Precision, Recall\n",
        "from tensorflow.keras import backend as K, Sequential, Model\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow_addons.layers import WeightNormalization\n",
        "\n",
        "from sklearn.preprocessing import QuantileTransformer, StandardScaler\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# from bagging_funcs import (DerivedFeatures, kfolds_bagging_training, voting_predict,\n",
        "#                            kolds_stacked_ensemble_training, stacked_ensemble_predict )\n",
        "\n",
        "import sys\n",
        "\n",
        "def log_loss_metric(y_true, y_pred):\n",
        "    bce = tf.keras.losses.BinaryCrossentropy()\n",
        "    return bce(y_true, y_pred).numpy()\n",
        "\n",
        "print(pd.__version__)\n",
        "print(tf.__version__)\n",
        "tf.python.client.device_lib.list_local_devices()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.1.4\n",
            "2.3.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 14842231441886271854, name: \"/device:XLA_CPU:0\"\n",
              " device_type: \"XLA_CPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 16978054367748691274\n",
              " physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\n",
              " device_type: \"XLA_GPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 12817102421820215382\n",
              " physical_device_desc: \"device: XLA_GPU device\", name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 14640891840\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 6415311554296207829\n",
              " physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ehGDp2UFKiHd"
      },
      "source": [
        "\n",
        "# Loading data and encoding\n",
        "raw_test = pd.read_csv(folder_path + 'test_features.csv')\n",
        "raw_train = pd.read_csv(folder_path + 'train_features.csv')\n",
        "raw_targets = pd.read_csv(folder_path + 'train_targets_scored.csv')\n",
        "raw_nonscored = pd.read_csv(folder_path + 'train_targets_nonscored.csv')\n",
        "\n",
        "# Phân loại dữ liệu\n",
        "cols_id = ['sig_id']\n",
        "cols_to_remove = ['cp_type']\n",
        "cols_fts = [i for i in raw_train.columns if i not in cols_id +cols_to_remove]\n",
        "cols_gene = [col for col in raw_train.columns if col.startswith(\"g-\")]\n",
        "cols_cell = [col for col in raw_train.columns if col.startswith(\"c-\")]\n",
        "cols_experiment = [col for col in cols_fts if col not in cols_gene+cols_cell]\n",
        "cols_target = [i for i in raw_targets.columns if i not in cols_id]\n",
        "num_fts, num_labels = len(cols_fts), len(cols_target)\n",
        "\n",
        "# xử lý categorical\n",
        "def transform_data(input_data):\n",
        "    '''Clean data and encoding\n",
        "        * input_data: table '''\n",
        "    out = input_data.copy()\n",
        "    out['cp_dose'] = out['cp_dose'].map({'D1':0, 'D2':1})\n",
        "    out['cp_time'] = out['cp_time']/72\n",
        "    \n",
        "    return out\n",
        "\n",
        "to_train = transform_data(raw_train[raw_train['cp_type'] != 'ctl_vehicle'])\n",
        "to_train_targets = raw_targets.iloc[to_train.index]\n",
        "full_pred  = transform_data(raw_test)\n",
        "to_pred = full_pred[full_pred['cp_type'] != 'ctl_vehicle']\n",
        "\n",
        "y_train = to_train_targets[cols_target].values\n",
        "y_non_scored = raw_nonscored.iloc[to_train.index, 1:].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "x2Qvvo7yKiHd"
      },
      "source": [
        "# PARAMS\n",
        "# u_fts_num = X_train.shape[1]\n",
        "i_fts_num = num_labels\n",
        "\n",
        "initializer = tf.keras.initializers.LecunNormal()# 'he_normal'  # <-- update\n",
        "kn_reg = tf.keras.regularizers.l1(2e-7) # <-- update\n",
        "activation = 'selu'  # <-- update\n",
        "bias_init_carefully = tf.keras.initializers.Constant(np.log([16844/(21948*206 - 16844)]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "kd9KLgFrKiHd"
      },
      "source": [
        "# Define model\n",
        "def layer_BDWD( n_components, activation = 'relu',  kn_init = 'glorot_uniform', kn_reg = None, bias_init = None):\n",
        "      def layer_cpl(input_layer):\n",
        "        '''BN - DROPOUT - WEIGHTNORMAL - DENSE'''\n",
        "        layer = BatchNormalization() (input_layer)\n",
        "        layer = Dropout(0.25 ) (layer)\n",
        "        dense = Dense(n_components, activation = activation, \n",
        "                      kernel_initializer = initializer, kernel_regularizer = kn_reg ,\n",
        "                      bias_initializer = bias_init)\n",
        "        layer = WeightNormalization(dense) (layer)\n",
        "        return layer\n",
        "      return layer_cpl\n",
        "    \n",
        "def layer_dense_set(hidden_list):\n",
        "    def layer_cpl(input_layer):\n",
        "        layer = input_layer\n",
        "        for n_hid in hidden_list:\n",
        "            layer = layer_BDWD(n_hid, activation = activation, kn_init = initializer, kn_reg = kn_reg, bias_init = bias_init_carefully) (layer)\n",
        "        return layer\n",
        "    return layer_cpl\n",
        "\n",
        "# Model1 \n",
        "def model1(n_input, n_output):\n",
        "    input_u = Input(shape = (n_input,) )\n",
        "    layer_u = layer_dense_set(hidden_list = [1024, 1024, 512]) (input_u)\n",
        "    out_put = WeightNormalization(Dense(n_output, activation = 'sigmoid' ))(layer_u)\n",
        "    model1 =  Model(inputs=[input_u, ], outputs= [out_put])\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "    bce = tf.keras.losses.BinaryCrossentropy()\n",
        "    model1.compile(loss= BinaryCrossentropy(), optimizer=opt , metrics= [bce])\n",
        "    return model1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Cvrw9e5EKiHd"
      },
      "source": [
        "# CALLBACK\n",
        "def exp_decay(lr0, s, down_hill):\n",
        "    def exp_decay_fn(epoch):\n",
        "        if epoch <= 5:\n",
        "            out = lr0*2\n",
        "        elif epoch%s <= 1:\n",
        "            out = lr0\n",
        "        else:\n",
        "            out = lr0 + 0.015*(epoch%s/ (s - 1))* down_hill/ (down_hill + int(epoch/s) )\n",
        "        return np.clip( out, 0.001, 0.05)\n",
        "    return exp_decay_fn\n",
        "\n",
        "fn_lr = exp_decay(0.001, 5, 50)\n",
        "lr_schedule = tf.keras.callbacks.LearningRateScheduler ( fn_lr )\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_binary_crossentropy', factor=0.1, patience=5, mode='min', min_lr=1E-5, verbose= 0)\n",
        "early_stopping = EarlyStopping(monitor='val_binary_crossentropy', min_delta=0.5E-4, patience=15, mode='min',restore_best_weights=True, verbose= 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "b5oCScKQKiHd"
      },
      "source": [
        "# Add nonscore fts   \n",
        "model_nonscored_rep = model1(n_input = to_train[cols_fts].shape[1], n_output = y_non_scored.shape[1])\n",
        "history = model_nonscored_rep.fit(\n",
        "        to_train[cols_fts], y_non_scored, validation_split = 0.25, \n",
        "        callbacks=[early_stopping, lr_schedule], epochs= 47, verbose = 0,\n",
        "        batch_size= 128)\n",
        "\n",
        "# Graph___________________________________\n",
        "hí = history.history\n",
        "đầu_ra = {x: hí[x] for x in ['binary_crossentropy', 'val_binary_crossentropy']}\n",
        "plt.plot(pd.DataFrame(đầu_ra))\n",
        "plt.show()\n",
        "lr = []\n",
        "for i in range(47) :\n",
        "    lr.append( fn_lr(i+1) )\n",
        "plt.plot(np.array(lr))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "VtxRsEF6KiHd"
      },
      "source": [
        "# add_fts\n",
        "X_train = np.concatenate([to_train[cols_fts].values, model_nonscored_rep.predict(to_train[cols_fts])], axis = 1)\n",
        "X_pred = np.concatenate([to_pred[cols_fts].values, model_nonscored_rep.predict(to_pred[cols_fts])], axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "iZ5gPDHyKiHd"
      },
      "source": [
        "scaler = StandardScaler()\n",
        "scaler.fit(np.concatenate([X_train, X_pred]))\n",
        "X_train = scaler.transform(X_train)\n",
        "X_pred = scaler.transform(X_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "S4CLHqwmKiHe"
      },
      "source": [
        "# # Train 1st model\n",
        "\n",
        "# model_mlp = model1(n_input = X_train.shape[1], n_output = y_train.shape[1])\n",
        "# model_mlp_copy1 = tf.keras.models.clone_model(model_mlp)\n",
        "# # Fitting\n",
        "# history = model_mlp.fit(\n",
        "#         X_train, y_train, validation_split = 0.25, \n",
        "#         callbacks=[early_stopping, lr_schedule], epochs= 72, verbose =1,\n",
        "#         batch_size= 128)\n",
        "\n",
        "# model_mlp_copy2 = tf.keras.models.clone_model(model_mlp)\n",
        "\n",
        "# # Graph___________________________________\n",
        "# hí = history.history\n",
        "# đầu_ra = {x: hí[x] for x in ['binary_crossentropy', 'val_binary_crossentropy']}\n",
        "# plt.plot(pd.DataFrame(đầu_ra))\n",
        "# plt.show()\n",
        "# lr = []\n",
        "# for i in range(47) :\n",
        "#     lr.append( fn_lr(i+1) )\n",
        "# plt.plot(np.array(lr))\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "8g2kQHoeKiHe"
      },
      "source": [
        "# Log_loss return numpy values\n",
        "def log_loss_metric(y_true, y_pred):\n",
        "    bce = tf.keras.losses.BinaryCrossentropy()\n",
        "    return bce(y_true, y_pred).numpy()\n",
        "\n",
        "# Random bagging nets\n",
        "def bagging_split(X_train, y_train, alpha, n_samples):\n",
        "    ''' SPLIT TRAINING DATA TO N SAMPLES FOR BAGGING\n",
        "    :X_train:np array: data for model training only\n",
        "    :y_train:np array: labels for model training only\n",
        "    :alpha:0-1 float: poportion of data in each sample\n",
        "    return:\n",
        "        generator for bagging training set'''\n",
        "    data_length = X_train.shape[0]\n",
        "    for i in range(n_samples):\n",
        "        idx = np.random.choice( data_length, size= int(data_length * alpha), replace=0)\n",
        "        yield X_train[idx], y_train[idx]\n",
        "\n",
        "# Predict with average ouput from a list of models\n",
        "def voting_predict(model_list, X_pred ):\n",
        "    '''PREDICT OUTPUT FROM A LIST OF MODEL'''\n",
        "    pred = 0\n",
        "    for model in model_list:\n",
        "        pred += model.predict(X_pred)\n",
        "    avg_pred = pred/len(model_list)\n",
        "    avg_pred[:,[34,82]] = 0\n",
        "    return avg_pred\n",
        "\n",
        "def reset_seed_model(model):\n",
        "    seed = int( np.random.random() *100)\n",
        "    np.random.seed(seed)\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.random.set_seed(seed)\n",
        "    model_ = model1(n_input = X_train.shape[1], n_output = y_train.shape[1]) # <----- UPDATE THIS\n",
        "    return model_\n",
        "\n",
        "def bagging_training(model, X_train, y_train, X_val, y_val, alpha, n_samples, save_path = '', callbacks = [], epochs = 47, batch_size = 128):\n",
        "    '''TRAINING FOR EACH BOOSTRAP AGGREGATING (BAGGING)\n",
        "    return:\n",
        "        list of n_samples model'''\n",
        "    model_list = []\n",
        "\n",
        "    for bag_id, (x_train_bag, y_train_bag) in enumerate(bagging_split(X_train, y_train, alpha = alpha, n_samples = n_samples)):\n",
        "        model_ = reset_seed_model(model)\n",
        "        model_.fit(\n",
        "            x_train_bag, y_train_bag, validation_data = (X_val, y_val), \n",
        "            callbacks= callbacks , epochs= epochs, verbose = 0,\n",
        "            batch_size= batch_size )\n",
        "        \n",
        "        y_pred_inbag = model_.predict(X_val)\n",
        "        evaluate_at_bag = log_loss_metric(y_val, y_pred_inbag)\n",
        "        print('Logloss at bag ', bag_id, ': ', evaluate_at_bag)\n",
        "        model_list.append(model_)\n",
        "        model_.save('/content/drive/MyDrive/Data/colabs_data/MOA_kaggle/moa_feats/quanvh8_saved_model/model_bagging_net_'+str(bag_id)+'.h5')\n",
        "        del model_\n",
        "    \n",
        "    print('Evaluate bagging, log loss = ', \n",
        "        log_loss_metric(y_val,  voting_predict(model_list, X_val )  ) )\n",
        "\n",
        "    return model_list\n",
        "\n",
        "def kfolds_bagging_training(NFOLDS, model, SEEDS, X_train, y_train, bagging_alpha = 0.75, bagging_samples = 10, callbacks = [], epochs = 47, batch_size = 128):\n",
        "    ''' TRAINING FOR KFOLDS EVALUATION\n",
        "    :NFOLDS:int: số folds\n",
        "    :model:model: model dùng để train\n",
        "    :SEEDS:list: list of seeds to train\n",
        "    :X_train:np array: full data for train and evaluate\n",
        "    :y_train:np array: full labels for train and evaluate\n",
        "    return:\n",
        "        :list of list model: list NFOLDS-list trained model\n",
        "    '''\n",
        "    ouput = []\n",
        "    kf = KFold(n_splits= NFOLDS, shuffle = True)\n",
        "    fold_logloss = []\n",
        "\n",
        "    for fold_id, (train_index, val_index) in enumerate(kf.split(X_train)):\n",
        "        print('Training at fold: ', fold_id, '-'*100)\n",
        "\n",
        "        fold_X_train, fold_y_train = X_train[train_index], y_train[train_index]\n",
        "        fold_X_val, fold_y_val = X_train[val_index], y_train[val_index]\n",
        "\n",
        "        # Training bagging\n",
        "        model_list = bagging_training(model, fold_X_train, fold_y_train, fold_X_val ,fold_y_val,\n",
        "            alpha = bagging_alpha, n_samples = bagging_samples, callbacks = callbacks, epochs = epochs, batch_size = batch_size)\n",
        "\n",
        "        fold_logloss.append( log_loss_metric(fold_y_val,  voting_predict( model_list, fold_X_val )) )\n",
        "        ouput.append(model_list)\n",
        "    \n",
        "    print('AVG logloss all folds: ', np.mean(fold_logloss))\n",
        "\n",
        "    return ouput"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "I4B-4yNVKiHe"
      },
      "source": [
        "# BAGGING\n",
        "model_2list = kfolds_bagging_training(7, \n",
        "                                      1\n",
        "                                      , [], X_train, y_train, bagging_alpha = 0.75, bagging_samples = 8,  \n",
        "                                      callbacks = [early_stopping, lr_schedule], \n",
        "                                      epochs = 72, batch_size = 128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "_n0jiirnKiHe"
      },
      "source": [
        "prediction = voting_predict( sum(model_2list, []), X_pred )\n",
        "# prediction = model1.predict(X_pred)\n",
        "\n",
        "df_preds_non_ctl =  pd.DataFrame(prediction, columns= cols_target, index = to_pred.index)\n",
        "\n",
        "# concat with all to pred values\n",
        "df_preds = pd.concat([ full_pred[cols_id], df_preds_non_ctl], axis = 1).fillna(0)\n",
        "# to csv\n",
        "df_preds.to_csv(\"submission.csv\", index = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Vy6yjv6eKiHe"
      },
      "source": [
        "df_preds"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}