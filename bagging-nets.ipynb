{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "bagging-nets.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqE2TFuV1dae",
        "outputId": "f013759d-37e7-4024-8213-790fd19f79df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_execution_state": "idle",
        "_uuid": "051d70d956493feee0c6d64651c6a088724dca2a",
        "id": "5FTCve_w1XEr",
        "outputId": "e3aab28e-4695-4d1c-d7a1-31cde07dec61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "'''ENSEMBLE NETS\n",
        "Inspire by https://www.kaggle.com/demetrypascal/fork-of-2heads-looper-super-puper-plate'''\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import copy\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import feature_column as fc\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.layers import (Dense, DenseFeatures, Dropout, \n",
        "                                     BatchNormalization, Embedding, Input, Concatenate, Average,\n",
        "                                     InputLayer, Lambda)\n",
        "from tensorflow.keras.metrics import AUC, Precision, Recall\n",
        "from tensorflow.keras import backend as K, Sequential, Model\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow_addons.layers import WeightNormalization\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "import keras\n",
        "\n",
        "from sklearn.preprocessing import QuantileTransformer, StandardScaler\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from math import log2\n",
        "\n",
        "import sys\n",
        "\n",
        "def log_loss_metric(y_true, y_pred):\n",
        "#     loss = 0\n",
        "#     y_pred_clip = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
        "#     for i in range(y_true.shape[1]):\n",
        "#         loss += - np.mean(y_true[:, i] * np.log(y_pred_clip[:, i]) + (1 - y_true[:, i]) * np.log(1 - y_pred_clip[:, i]))\n",
        "#     return loss / y_true.shape[1]\n",
        "\n",
        "    y_pred = np.clip(y_pred,1e-10,1-1e-10)\n",
        "    return -np.mean(y_true*np.log(y_pred) + (1-y_true)*np.log(1-y_pred))\n",
        "\n",
        "print(pd.__version__)\n",
        "print(tf.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.1.4\n",
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLOzSk_h1XEz"
      },
      "source": [
        "# MODULE 1. DATA LOADING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqUvs-3Y1XEz"
      },
      "source": [
        "# Loading data and encoding\n",
        "\n",
        "folder_path = '/content/drive/My Drive/Data/colabs_data/MOA_kaggle/'\n",
        "raw_test = pd.read_csv(folder_path + 'test_features.csv')\n",
        "raw_train = pd.read_csv(folder_path + 'train_features.csv')\n",
        "raw_targets = pd.read_csv(folder_path + 'train_targets_scored.csv')\n",
        "\n",
        "# Phân loại dữ liệu\n",
        "cols_id = ['sig_id']\n",
        "cols_to_remove = ['cp_type']\n",
        "cols_fts = [i for i in raw_train.columns if i not in cols_id +cols_to_remove]\n",
        "cols_gene = [col for col in raw_train.columns if col.startswith(\"g-\")]\n",
        "cols_cell = [col for col in raw_train.columns if col.startswith(\"c-\")]\n",
        "cols_experiment = [col for col in cols_fts if col not in cols_gene+cols_cell]\n",
        "cols_target = [i for i in raw_targets.columns if i not in cols_id]\n",
        "num_fts, num_labels = len(cols_fts), len(cols_target)\n",
        "\n",
        "# xử lý categorical\n",
        "def transform_data(input_data):\n",
        "    '''Clean data and encoding\n",
        "        * input_data: table '''\n",
        "    out = input_data.copy()\n",
        "    out['cp_dose'] = out['cp_dose'].map({'D1':0, 'D2':1})\n",
        "    out['cp_time'] = out['cp_time']/72\n",
        "    \n",
        "    return out\n",
        "\n",
        "to_train = transform_data(raw_train[raw_train['cp_type'] != 'ctl_vehicle'])\n",
        "to_train_targets = raw_targets.iloc[to_train.index]\n",
        "full_pred  = transform_data(raw_test)\n",
        "to_pred = full_pred[full_pred['cp_type'] != 'ctl_vehicle']"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXhrnEA51XE2"
      },
      "source": [
        "# MODULE 2. DATA TRANSFORMATION PIPELINE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XJrep-T1XE3"
      },
      "source": [
        "# Add derived fts\n",
        "class DerivedFeatures(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, n_clusters= 6):\n",
        "        self.n_clusters = n_clusters\n",
        "\n",
        "    def fit(self, X, y= None):\n",
        "        try: X_ = X.values.copy() # creating a copy to avoid changes to original dataset\n",
        "        except: X_ = X.copy()\n",
        "\n",
        "        # try: y_ = y.values.copy()\n",
        "        # except: y_ = y.copy()\n",
        "\n",
        "        # self.n_labels = y_.shape[1]\n",
        "\n",
        "        # Kmeans\n",
        "        self.kmeans = KMeans(n_clusters= self.n_clusters ) \n",
        "        self.kmeans.fit(X_)\n",
        "\n",
        "        # self.closet_labels = y_[np.argmin(self.kmeans.transform(X_), axis= 0)]# closet labels\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y = None):\n",
        "        try: X_ = X.values.copy() # creating a copy to avoid changes to original dataset\n",
        "        except: X_ = X.copy()\n",
        "\n",
        "        # Non-linear infomation\n",
        "        std = np.std(X_, axis= 1)\n",
        "\n",
        "        # Kmeans features\n",
        "        distance_to_centroid = self.kmeans.transform(X_) # distance to the centroid\n",
        "        cluster_labels = self.kmeans.predict(X_)# cluster index\n",
        "\n",
        "        # column names\n",
        "        self.columns = ['std']\\\n",
        "                        + ['distance_to_centroid'+str(i) for i in range(distance_to_centroid.shape[1])]\\\n",
        "                        + ['cluster_labels']\n",
        "        ouput = np.concatenate([std.reshape(-1,1), distance_to_centroid, cluster_labels.reshape(-1,1)], axis= 1)\n",
        "        return ouput"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-cBlnqh1XE6"
      },
      "source": [
        "# preprocessing pipeline\n",
        "def pipe_line_builder(quantiles_num, pca_dims, kmean_clusters):\n",
        "    '''Dựng pipe line cho từng nhóm columns\n",
        "    :quantiles_num: int: số quantile khi normalise\n",
        "    :pca_dims: int: số chiều pca'''\n",
        "    norm = QuantileTransformer(n_quantiles=quantiles_num,random_state=0, output_distribution=\"normal\")\n",
        "    pca = PCA(n_components = pca_dims)\n",
        "    derived_ft = DerivedFeatures(n_clusters = kmean_clusters)\n",
        "\n",
        "    p_derived_ft = Pipeline([\n",
        "        ('norm', norm), \n",
        "        ('derived', derived_ft)])\n",
        "\n",
        "    p_norm_pca = Pipeline([ \n",
        "        ('norm', norm),\n",
        "        ('pca', pca) ])\n",
        "    return FeatureUnion([\n",
        "        ('norm', norm), \n",
        "        ('norm_pca', p_norm_pca),\n",
        "        ('derived', p_derived_ft)])\n",
        "\n",
        "# \n",
        "\n",
        "pipe = Pipeline([\n",
        "    ('norm_pca', ColumnTransformer([\n",
        "                     ('gene', pipe_line_builder(quantiles_num = 200, pca_dims = 600, kmean_clusters = 5), cols_gene),\n",
        "                     ('cell', pipe_line_builder(quantiles_num = 200, pca_dims = 50, kmean_clusters = 5), cols_cell),\n",
        "                    ]) \n",
        "    ), \n",
        "    ('var', VarianceThreshold(0.5)) \n",
        "])\n",
        "\n",
        "pipe = ColumnTransformer([\n",
        "    ('gene_cell', pipe, cols_gene+ cols_cell),\n",
        "    ('experiment', 'passthrough', cols_experiment)\n",
        "])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdZH_psq1XE8"
      },
      "source": [
        "# Transform data\n",
        "pipe.fit(to_train[cols_fts].append(to_pred[cols_fts]))\n",
        "X_train = pipe.transform(to_train[cols_fts])\n",
        "X_pred = pipe.transform(to_pred[cols_fts])\n",
        "y_train = to_train_targets[cols_target].values"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOLUCHyy1XFA",
        "outputId": "2baf21ae-ef9b-475e-d7a9-fd0be6fccf23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(X_train.shape, X_pred.shape, y_train.shape)\n",
        "print(type(X_train), type(X_pred), type(y_train))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(21948, 1241) (3624, 1241) (21948, 206)\n",
            "<class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQpPUfYf1XFC"
      },
      "source": [
        "# MODULE 3: BAGGING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzAPc8kK1XFD"
      },
      "source": [
        "# Random bagging nets\n",
        "def bagging_split(X_train, y_train, alpha, n_samples):\n",
        "    ''' SPLIT TRAINING DATA TO N SAMPLES FOR BAGGING\n",
        "    :X_train:np array: data for model training only\n",
        "    :y_train:np array: labels for model training only\n",
        "    :alpha:0-1 float: poportion of data in each sample\n",
        "    return:\n",
        "        generator for bagging training set'''\n",
        "    data_length = X_train.shape[0]\n",
        "    for i in range(n_samples):\n",
        "        idx = np.random.choice( data_length, size= int(data_length * alpha), replace=0)\n",
        "        yield X_train[idx], y_train[idx]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNFo0NhW1XFG"
      },
      "source": [
        "def voting_predict(model_list, X_pred, num_labels):\n",
        "    '''PREDICT OUTPUT FROM A LIST OF MODEL'''\n",
        "    pred = np.zeros((X_pred.shape[0], num_labels))\n",
        "    for model in model_list:\n",
        "        pred_i = model.predict(X_pred)\n",
        "        pred += pred_i\n",
        "    avg_pred = pred/len(model_list)\n",
        "    avg_pred[:,[34,82]] = 0\n",
        "    return avg_pred"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8raBuUi91XFI"
      },
      "source": [
        "def bagging_training(model, X_train, y_train, X_val, y_val, alpha, n_samples):\n",
        "    '''TRAINING FOR EACH BOOSTRAP AGGREGATING (BAGGING)\n",
        "    return:\n",
        "        list of n_samples model'''\n",
        "    ouput = []\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=5, mode='min', min_lr=1E-5, verbose=1)\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', min_delta=1E-5, patience=15, mode='min',restore_best_weights=True, verbose=1)\n",
        "\n",
        "    for bag_id, (x_train_bag, y_train_bag) in enumerate(bagging_split(X_train, y_train, alpha = alpha, n_samples = n_samples)):\n",
        "        tf.keras.backend.clear_session()\n",
        "        tf.random.set_seed(np.random.random())\n",
        "#         print('Training at Bag ', bag_id, '_'*100)\n",
        "\n",
        "        model_ = copy.deepcopy(model)\n",
        "        model_.fit(\n",
        "            x_train_bag, y_train_bag,# validation_data = (X_val, y_val), \n",
        "            callbacks=[reduce_lr, early_stopping], epochs=150, verbose = 0,\n",
        "            batch_size=BATCH_SIZE )\n",
        "        ouput.append(model_)\n",
        "        y_pred_inbag = model_.predict(X_val)\n",
        "#         print(y_pred_inbag, y_val)\n",
        "        evaluate_at_bag = log_loss_metric(y_val, y_pred_inbag)\n",
        "        print('Logloss at bag ', bag_id, ': ', evaluate_at_bag)\n",
        "        del model_\n",
        "    logloss_all_bag = log_loss_metric(y_val, \n",
        "                voting_predict(ouput, X_val, num_labels = y_val.shape[1])  )\n",
        "    print('Evaluate bagging, log loss = ', logloss_all_bag )\n",
        "\n",
        "    return ouput"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YKMieAm1XFL"
      },
      "source": [
        "# log_loss_metric(np.array([[0,0.00001],[.1,.2]]), np.array([[0,0.0002], [0.1, 0.2]]))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyTLDJxr1XFN"
      },
      "source": [
        "def kfolds_training(NFOLDS, model, SEEDS, X_train, y_train, bagging_alpha = 0.75, bagging_samples = 10):\n",
        "    ''' TRAINING FOR KFOLDS EVALUATION\n",
        "    :NFOLDS:int: số folds\n",
        "    :model:model: model dùng để train\n",
        "    :SEEDS:list: list of seeds to train\n",
        "    :X_train:np array: full data for train and evaluate\n",
        "    :y_train:np array: full labels for train and evaluate\n",
        "    return:\n",
        "        :list of list model: list NFOLDS-list trained model\n",
        "    '''\n",
        "    ouput = []\n",
        "    kf = KFold(n_splits= NFOLDS, shuffle = True)\n",
        "\n",
        "    for fols_id, (train_index, val_index) in enumerate(kf.split(to_train)):\n",
        "        print('Training at fold: ', fols_id, '#'*100)\n",
        "        tf.keras.backend.clear_session()\n",
        "\n",
        "        fold_X_train, fold_y_train = X_train[train_index], y_train[train_index]\n",
        "        fold_X_val, fold_y_val = X_train[val_index], y_train[val_index]\n",
        "\n",
        "        # Training bagging\n",
        "        model_list = bagging_training(model, fold_X_train, fold_y_train, fold_X_val ,fold_y_val,\n",
        "            alpha = bagging_alpha, n_samples = bagging_samples)\n",
        "\n",
        "        # fold_logloss = log_loss_metric(fold_y_val, \n",
        "        #     voting_predict( model_list, fold_X_val, 206))\n",
        "        ouput.append(model_list)\n",
        "    fold_logloss = log_loss_metric(fold_y_val, \n",
        "            voting_predict( sum(ouput, []), fold_X_val, 206))\n",
        "    print('AVG logloss all folds: ', fold_logloss)\n",
        "\n",
        "    return ouput"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWpqTIh-1XFQ"
      },
      "source": [
        "# Hyper params\n",
        "NFOLDS = 8\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 150\n",
        "BAGGING_ALPHA = 0.75\n",
        "SEEDS = [23, 228, 1488, 1998, 2208, 2077, 404]\n",
        "KFOLDS = 10\n",
        "label_smoothing_alpha = 0.00005\n",
        "P_MIN = 1e-10\n",
        "P_MAX = 1 - P_MIN"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvVe5RdK1XFS"
      },
      "source": [
        "# Define model\n",
        "model = Sequential([\n",
        "    BatchNormalization(),\n",
        "    WeightNormalization(Dense(1024, activation=\"selu\")),\n",
        "\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.25),\n",
        "    WeightNormalization(Dense(1024, activation=\"selu\")),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.25),\n",
        "    WeightNormalization(Dense(512, activation=\"selu\")),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.25),\n",
        "    WeightNormalization(Dense(num_labels, activation=\"sigmoid\"))\n",
        "])\n",
        "\n",
        "def logloss(y_true, y_pred):\n",
        "    y_pred = tf.clip_by_value(y_pred,P_MIN,P_MAX)\n",
        "    return -K.mean(y_true*K.log(y_pred) + (1-y_true)*K.log(1-y_pred))\n",
        "\n",
        "model.compile(optimizer='adam', loss= 'binary_crossentropy' #BinaryCrossentropy(label_smoothing= label_smoothing_alpha)\n",
        "              , metrics=logloss)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZAqvmmP1XFV"
      },
      "source": [
        "# n_components = 256\n",
        "\n",
        "# layer_u = Sequential([\n",
        "#     BatchNormalization(),\n",
        "#     WeightNormalization(Dense(1024, activation=\"selu\")),\n",
        "#     BatchNormalization(),\n",
        "#     Dropout(0.25),\n",
        "#     WeightNormalization(Dense(512, activation=\"selu\")),\n",
        "#     BatchNormalization(),\n",
        "#     Dropout(0.25),\n",
        "#     WeightNormalization(Dense(n_components, activation=\"selu\")),\n",
        "#     BatchNormalization(),\n",
        "#     Dropout(0.25)\n",
        "# ])\n",
        "\n",
        "# #2.1. Addition information for item_info\n",
        "# chemical_category = tf.transpose(\n",
        "#         tf.constant(\n",
        "#             [[1 if '_inhibitor' in i else 0 for i in cols_target],\n",
        "#                [1 if '_agonist' in i else 0 for i in cols_target],\n",
        "#                [1 if '_agent' in i else 0 for i in cols_target],\n",
        "#                [1 if '_antagonist' in i else 0 for i in cols_target],\n",
        "#                [1 if '_blocker' in i else 0 for i in cols_target],\n",
        "#                [1 if '_activator' in i else 0 for i in cols_target] \n",
        "#              ]))\n",
        "\n",
        "# #2.2 Full item fts: addition + onehot\n",
        "# item_ft = tf.concat(\n",
        "#     [chemical_category ,\n",
        "#      tf.eye(num_labels, dtype = tf.int32) # Create tensor 0-1 coresponse with chemical labels\n",
        "#     ], axis = 1\n",
        "# )\n",
        "# layer_i = Dense(n_components, activation = 'relu', kernel_initializer='he_normal', name ='layer_u1') (item_ft)\n",
        "\n",
        "\n",
        "# #3. Dot product user - item\n",
        "# def dot_2layer(x):\n",
        "#     return K.dot( x[0], K.transpose(x[1]))\n",
        "# dot_ui = Lambda( dot_2layer, name = 'lambda_dot' ) ([layer_u,layer_i])\n",
        "# dot_ui= WeightNormalization(Dense(512, activation=\"selu\", kernel_initializer='he_normal')) (dot_ui)\n",
        "# dot_ui= BatchNormalization() (dot_ui)\n",
        "# dot_ui= Dropout(0.25) (dot_ui),\n",
        "# dot_ui = WeightNormalization(Dense(i_fts_num, activation = 'sigmoid', kernel_initializer='he_normal', name = 'labels'))(dot_ui)\n",
        "\n",
        "# model = Model(inputs=[ layer_u, ], outputs= [dot_ui])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0XZWdvS1XFY"
      },
      "source": [
        "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=5, mode='min', min_lr=1E-5, verbose=1)\n",
        "# early_stopping = EarlyStopping(monitor='val_loss', min_delta=1E-5, patience=15, mode='min',restore_best_weights=True, verbose=1)\n",
        "    \n",
        "# model.fit(\n",
        "#         X_train, y_train, validation_split = 0.3, \n",
        "#         callbacks=[reduce_lr, early_stopping], epochs=50, verbose =1,\n",
        "#         batch_size=BATCH_SIZE)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxXFH-Hg1XFa",
        "outputId": "8de2ee9b-6be4-49e4-ad09-b8e8dc4e6707",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model_2list = kfolds_training(NFOLDS, model, SEEDS, X_train, y_train, bagging_alpha = 0.75, bagging_samples = 20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training at fold:  0 ####################################################################################################\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Logloss at bag  0 :  nan\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "Logloss at bag  1 :  0.023056370765873097\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "Logloss at bag  2 :  0.017737772645874762\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "Logloss at bag  3 :  0.017913439984359732\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "Logloss at bag  4 :  0.017878910783300363\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "Logloss at bag  5 :  0.017705819252674667\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "Logloss at bag  6 :  0.017824665113641034\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "Logloss at bag  7 :  0.01776722765369193\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "Logloss at bag  8 :  0.017717362464163472\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  9 :  0.017462609146580068\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  10 :  0.017441142737883372\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  11 :  0.017507379518799958\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  12 :  0.01748733563310963\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  13 :  0.017473305162440382\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  14 :  0.017493237138535827\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  15 :  0.01742066334067548\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  16 :  0.01747917266193308\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  17 :  0.017532917982709027\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  18 :  0.01743669272083925\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  19 :  0.017470237416931676\n",
            "Evaluate bagging, log loss =  0.016699880143033398\n",
            "Training at fold:  1 ####################################################################################################\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  0 :  0.01710274245242369\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  1 :  0.01713891873041811\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  2 :  0.01709332612169617\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  3 :  0.01703404936823131\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  4 :  0.017061507639818233\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  5 :  0.017087416119888788\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  6 :  0.016973064191100114\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  7 :  0.01703385784563636\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  8 :  0.017116513914636713\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  9 :  0.01705420913862879\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  10 :  0.017115423208092548\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  11 :  0.017060453273097038\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  12 :  0.017057181036149963\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  13 :  0.01707334480395039\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  14 :  0.017078348605539213\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  15 :  0.01709413093543045\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  16 :  0.017081148241892143\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  17 :  0.017092777728817798\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  18 :  0.017109850494187832\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  19 :  0.017078950831263196\n",
            "Evaluate bagging, log loss =  0.016787209072156657\n",
            "Training at fold:  2 ####################################################################################################\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  0 :  0.017123910482066122\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  1 :  0.017076855733647055\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  2 :  0.017026099485478736\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  3 :  0.016992430685632557\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  4 :  0.01705140347424823\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  5 :  0.017056171653598406\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  6 :  0.01697533587573863\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  7 :  0.017049927852699164\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  8 :  0.017018071211286432\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  9 :  0.01709912615799757\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  10 :  0.017038777485266617\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  11 :  0.017080843113042716\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  12 :  0.017061414556008122\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  13 :  0.0170922762310528\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  14 :  0.01699165089196764\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  15 :  0.01710481014655839\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  16 :  0.01707557284831009\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  17 :  0.017019073867760345\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  18 :  0.01710127339793225\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  19 :  0.017116350710703998\n",
            "Evaluate bagging, log loss =  0.016812065519258648\n",
            "Training at fold:  3 ####################################################################################################\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  0 :  0.017340548638483798\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  1 :  0.017324369864361634\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  2 :  0.01729246059463628\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  3 :  0.01730045670053435\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  4 :  0.01736528156082879\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  5 :  0.017343299534244676\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  6 :  0.01727628420488032\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  7 :  0.017324464606650335\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  8 :  0.017296909128596757\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  9 :  0.01723789938609379\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  10 :  0.017179433998099913\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  11 :  0.01727944764863663\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  12 :  0.017218644303621747\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  13 :  0.01733584914084401\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  14 :  0.01738002644914795\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  15 :  0.017275484187242975\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  16 :  0.0172861976911213\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  17 :  0.017333555353447404\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  18 :  0.017295878541568558\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  19 :  0.017314858012578916\n",
            "Evaluate bagging, log loss =  0.01700663189202251\n",
            "Training at fold:  4 ####################################################################################################\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  0 :  0.017548830617157528\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  1 :  0.01751017243260747\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  2 :  0.017588074578184707\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  3 :  0.017451022672332145\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  4 :  0.017481326730925784\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  5 :  0.01759211736537055\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  6 :  0.017474323635004525\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  7 :  0.01747022404820177\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  8 :  0.01752370090351719\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  9 :  0.01744844154032678\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  10 :  0.017531052692171128\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  11 :  0.017538052167060104\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  12 :  0.01763246451277778\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  13 :  0.017569024637271682\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  14 :  0.017570452103913826\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  15 :  0.017490057167423974\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  16 :  0.01748457023879984\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  17 :  0.017511047563552845\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  18 :  0.017513753109998708\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  19 :  0.017493484754737505\n",
            "Evaluate bagging, log loss =  0.01723123006355741\n",
            "Training at fold:  5 ####################################################################################################\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  0 :  0.016934838416057012\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  1 :  0.016845146337298257\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  2 :  0.016898355738647817\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  3 :  0.016886270094641997\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  4 :  0.01691517298866068\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  5 :  0.01692306403423766\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  6 :  0.01692468919169964\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  7 :  0.016843863341105814\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  8 :  0.016907632478984608\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  9 :  0.016854343534185974\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  10 :  0.016849130583627806\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  11 :  0.01687845585797303\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  12 :  0.016821600388319712\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  13 :  0.016942503809192273\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  14 :  0.016905610866785516\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  15 :  0.01693892090013678\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  16 :  0.01690328727457017\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  17 :  0.016853300893090856\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  18 :  0.016791131103487417\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  19 :  0.01690862198535337\n",
            "Evaluate bagging, log loss =  0.016602273727863184\n",
            "Training at fold:  6 ####################################################################################################\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  0 :  0.017005047098880247\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  1 :  0.01699293830592869\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  2 :  0.017020934186130647\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  3 :  0.01688517025739298\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00016: early stopping\n",
            "Logloss at bag  4 :  0.13399874779195786\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  5 :  0.016967414013983485\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  6 :  0.016912300657349284\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  7 :  0.01696615525391353\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00016: early stopping\n",
            "Logloss at bag  8 :  0.13339532582761846\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  9 :  0.01694716068436144\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00016: early stopping\n",
            "Logloss at bag  10 :  0.12391899324506717\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Logloss at bag  11 :  0.016968179818775173\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00016: early stopping\n",
            "Logloss at bag  12 :  0.12803193386644732\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00016: early stopping\n",
            "Logloss at bag  13 :  0.12493894329877227\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-05.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fRhcYjV1XFd"
      },
      "source": [
        "# cols_target_low_score = to_train_targets[cols_target].sum().sort_values(ascending = True).head(2).index\n",
        "# cols_target_low_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SP5rAhV51XFf"
      },
      "source": [
        "# nn_pred_train1 = voting_predict( sum(model_2list, []), X_train, 206)\n",
        "# np.savetxt(\"NN_fts_pred_1.csv\", nn_pred_train1, delimiter=\",\")\n",
        "# single_model = model_2list[0][0]\n",
        "# represent = tf.keras.models.Sequential(single_model.layers[:-1])\n",
        "# nn_rept = represent.predict(X_train)\n",
        "# np.savetxt(\"NN_fts_representation.csv\", nn_rept, delimiter=\",\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foKXKUpV1XFi"
      },
      "source": [
        "prediction = voting_predict( sum(model_2list, []), X_pred, 206)\n",
        "\n",
        "df_preds_non_ctl =  pd.DataFrame(prediction, columns= cols_target, index = to_pred.index)\n",
        "\n",
        "# concat with all to pred values\n",
        "df_preds = pd.concat([ full_pred[cols_id], df_preds_non_ctl], axis = 1).fillna(0)\n",
        "# to csv\n",
        "df_preds.to_csv(\"submission.csv\", index = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW7JVOiQ1XFk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}