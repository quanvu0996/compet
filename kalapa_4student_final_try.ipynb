{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kalapa_4student_traditional_ML.ipynb",
      "provenance": [],
      "mount_file_id": "https://github.com/quanvu0996/compet/blob/master/kalapa_4student_final_try.ipynb",
      "authorship_tag": "ABX9TyMSWYNm2SJOGdVA2+EH8lgP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quanvu0996/compet/blob/master/kalapa_4student_final_try.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnLk6f0nuwj-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, DenseFeatures, Dropout, BatchNormalization, Embedding\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.metrics import AUC, Precision, Recall\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow import feature_column as fc\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set(style=\"ticks\", color_codes=True)\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,roc_curve, auc, confusion_matrix\n",
        "from sklearn.utils import check_random_state\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "\n",
        "import xgboost as xgb\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-qcYZ8Fv1TH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ebdae1c4-7773-47f9-e8a8-a49f9e184b53"
      },
      "source": [
        "print(pd.__version__)\n",
        "print(tf.__version__)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0.5\n",
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1j7u_eUMv9Zc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submision_path = '/content/drive/My Drive/Data/colabs_data/kalapa_4students/simple_submission.csv'\n",
        "train_path = '/content/drive/My Drive/Data/colabs_data/kalapa_4students/train.csv'\n",
        "test_path = '/content/drive/My Drive/Data/colabs_data/kalapa_4students/test.csv'\n",
        "\n",
        "added_data_path= '/content/drive/My Drive/Data/colabs_data/kalapa_4students/exdata.txt'"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMOScD7JKhg7",
        "colab_type": "text"
      },
      "source": [
        "**Load và clean dữ liệu**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqoJojArwAM6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "6c837cb9-0690-480b-9966-d476e48835a7"
      },
      "source": [
        "def load_data(train_path, test_path, label_col = 'label'):\n",
        "    train_data = pd.read_csv(train_path)\n",
        "    predict_data = pd.read_csv(test_path)\n",
        "    return train_data, predict_data\n",
        "\n",
        "train_data, predict_data = load_data( train_path, test_path )\n",
        "train_data = train_data[train_data['label'].isin( [0,1] )]\n",
        "cols = train_data.columns"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2822: DtypeWarning: Columns (35,43) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  if self.run_code(code, result):\n",
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2822: DtypeWarning: Columns (34,42) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  if self.run_code(code, result):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbBLkOqGN7F4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "added_data = pd.read_csv(added_data_path, sep='\\t', encoding='utf-8', names = ['homeTownState', 'area', 'population', 'pop_density'])"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TT9zCpWXTtO",
        "colab_type": "text"
      },
      "source": [
        "Clean data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSYdoBTpXXx6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vals_homeTownState = { 'Hanoi' : 'Hà Nội' , 'Ho Chi Minh City' : 'TP.Hồ Chí Minh' , 'Thanh Hóa Province' : 'Thanh Hoá' , 'Nghệ An Province' : 'Nghệ An' , 'Đồng Nai Province' : 'Đồng Nai' , 'Haiphong' : 'Hải Phòng' , \n",
        "'Khánh Hòa Province' : 'Khánh Hoà' , 'Thái Bình Province' : 'Thái Bình' , 'Bắc Giang Province' : 'Bắc Giang' , 'Hà Tĩnh Province' : 'Hà Tĩnh' , 'Bình Định Province' : 'Bình Định' , 'Nam Định Province' : 'Nam Định' , 'Bắc Ninh Province' : 'Bắc Ninh' , 'Quảng Ngãi Province' : 'Quảng Ngãi' , 'Hải Dương Province' : 'Hải Dương' , 'Quảng Nam Province' : 'Quảng Nam' , \n",
        "'Thái Nguyên Province' : 'Thái Nguyên' , 'An Giang Province' : 'An Giang' , 'Cần Thơ' : 'Cần Thơ' , 'Đắk Lắk Province' : 'Đắk Lắk' , 'Da Nang' : 'Đà Nẵng' , 'Tiền Giang Province' : 'Tiền Giang' , 'Phú Thọ Province' : 'Phú Thọ' , 'Quảng Ninh Province' : 'Quảng Ninh' , 'Ninh Bình Province' : 'Ninh Bình' , 'Thừa Thiên–Huế Province' : 'Thừa Thiên Huế' , 'Bến Tre Province' : 'Bến Tre' , 'Hưng Yên Province' : 'Hưng Yên' , 'Bà Rịa–Vũng Tàu Province' : 'Bà Rịa - Vũng Tàu' , \n",
        "'Kiên Giang Province' : 'Kiên Giang' , 'Quảng Trị Province' : 'Quảng Trị' , 'Đồng Tháp Province' : 'Đồng Tháp' , 'Tây Ninh Province' : 'Tây Ninh' , 'Sóc Trăng Province' : 'Sóc Trăng' , 'Lâm Đồng Province' : 'Lâm Đồng' , 'Lạng Sơn Province' : 'Lạng Sơn' , 'Bình Thuận Province' : 'Bình Thuận' , 'Gia Lai Province' : 'Gia Lai' , 'Sơn La Province' : 'Sơn La' , 'Cà Mau Province' : 'Cà Mau' , 'Tuyên Quang Province' : 'Tuyên Quang' , \n",
        "'Quảng Bình Province' : 'Quảng Bình' , 'Vĩnh Long Province' : 'Vĩnh Long' , 'Long An Province' : 'Long An' , 'Yên Bái Province' : 'Yên Bái' , 'Bạc Liêu Province' : 'Bạc Liêu' , 'Hòa Bình Province' : 'Hoà Bình' , 'Trà Vinh Province' : 'Trà Vinh' , 'Bình Dương Province' : 'Bình Dương' , 'Phú Yên Province' : 'Phú Yên' , 'Hà Nam Province' : 'Hà Nam' , \n",
        "'Vĩnh Phúc Province' : 'Vĩnh Phúc' , 'Lào Cai Province' : 'Lào Cai' , 'Điện Biên Province' : 'Điện Biên' , 'Ninh Thuận Province' : 'Ninh Thuận' , 'Cao Bằng Province' : 'Cao Bằng' , 'Hà Giang Province' : 'Hà Giang' , \n",
        "'Bắc Kạn Province' : 'Bắc Kạn' , 'Kon Tum Province' : 'Kon Tum' , 'Hậu Giang Province' : 'Hậu Giang' , 'Sarawak' : 'Sarawak' , 'Đắk Nông Province' : 'Đắk Nông' , 'Seoul' : 'Seoul' , 'Bình Phước Province' : 'Bình Phước' , 'New York' : 'New York' , 'England' : 'England' , \n",
        "'Lai Châu Province' : 'Lai Châu' , 'California' : 'California' , 'Hong Kong' : 'Hong Kong' , 'Orientale' : 'Orientale' , 'Dubai' : 'Dubai' , 'Guangdong' : 'Guangdong' , 'Île-de-France' : 'Île-de-France' }"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1xCtPneDej3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "f353b3b2-dbb8-4f2f-b0db-e224429d1514"
      },
      "source": [
        "def clean_fn(df_):\n",
        "  df = df_.copy()\n",
        "\n",
        "  df = df.replace('notfound','missing')\n",
        "  df['homeTownState'] = df['homeTownState'].fillna('other').map(vals_homeTownState ).fillna('Foreign')\n",
        "  df = df.merge(added_data, how = 'left', on = 'homeTownState')\n",
        "  df[[ 'area', 'population', 'pop_density']] = df[[ 'area', 'population', 'pop_density']].astype('float64')\n",
        "  \n",
        "  df[\"subscriberCount\"].replace(0, np.nan, inplace=True)\n",
        "  df[\"friendCount\"].replace(0, np.nan, inplace=True)\n",
        "  df[\"Field_13\"] = df[\"Field_13\"].apply(lambda x: 1 if x == x else 0)\n",
        "  df[\"Field_38\"] = df[\"Field_38\"].map({0: 0.0, 1: 1.0, \"DN\": np.nan, \"TN\": np.nan, \"GD\": np.nan})\n",
        "  df[\"Field_62\"] = df[\"Field_62\"].map({\"I\": 1, \"II\": 2, \"III\": 3, \"IV\": 4, \"V\": 5, \"Ngoài quốc doanh Quận 7\": np.nan})\n",
        "  df[\"Field_47\"] = df[\"Field_47\"].map({\"Zezo\": 0, \"One\": 1, \"Two\": 2, \"Three\": 3, \"Four\": 4})\n",
        "  df[\"Field_27\"] = df[\"Field_27\"].replace({0.0: np.nan})\n",
        "  df[\"Field_28\"] = df[\"Field_28\"].replace({0.0: np.nan})\n",
        "\n",
        "\n",
        "  df['Field_45_Q'] = df['Field_45'].str[:-3].astype('category')\n",
        "  df['Field_45_TP_55'] = df['Field_45'].str[:2] == df['Field_55']\n",
        "  df['is_homeTown_diaChi'] = df['homeTownCity'] == df['diaChi']\n",
        "  df['is_homeTown_current_city'] = df['homeTownCity'] == df['currentLocationCity']\n",
        "  df['is_homeTown_current_state'] = df['homeTownState'] == df['currentLocationState']\n",
        "  df['F48_49'] = df['Field_48'] == df['Field_49']\n",
        "\n",
        "  df[[\"Field_27\", \"Field_28\"]].replace(0.0, np.nan, inplace=True)\n",
        "  df['F18_isnumeric'] = df['Field_18'].str.isnumeric()\n",
        "  df['F18_isalpha'] = df['Field_18'].str.isalpha()\n",
        "\n",
        "  return df\n",
        "\n",
        "train_data = clean_fn(train_data)\n",
        "predict_data = clean_fn(predict_data)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:4172: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  method=method,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPqdE9DReOLi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d1db5d6c-7a44-46ae-9768-5b70bf488612"
      },
      "source": [
        "len(train_data.columns)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "206"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_G2jTnngKY2p",
        "colab_type": "text"
      },
      "source": [
        "**Phân loại nhóm dữ liệu tương ứng với hướng xử lý**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrXATFIdwCZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Nhãn\n",
        "cols_label = ['label']\n",
        "# Đặc trưng binary\n",
        "cols_fts_binary = [i for i in train_data.select_dtypes(include=['float64','int64']).columns \n",
        "                   if len(set(train_data[i].fillna(0)) - {0,1} ) == 0 and i not in cols_label]\n",
        "# định dang datte yyyy-mm-dd\n",
        "cols_date = ['Field_1','Field_2','Field_5','Field_6','Field_7','Field_8','Field_9','Field_11'\n",
        "             ,'Field_15','Field_25','Field_32','Field_33','Field_35','Field_40','Field_43','Field_44'\n",
        "             ,'F_startDate','F_endDate','E_startDate','E_endDate','C_startDate','C_endDate','G_startDate','G_endDate'\n",
        "             ,'A_startDate','A_endDate']\n",
        "# định dạng date yyymmdd\n",
        "cols_date2 = ['ngaySinh', 'Field_34']\n",
        "# Đặc trưng dạng văn bản\n",
        "cols_docs = ['Field_46','diaChi','Field_48','Field_49','currentLocationName','homeTownName','Field_56']\n",
        "# Định danh bản ghi\n",
        "cols_id = ['id','Field_45'] +[i for i in train_data.select_dtypes(include = ['object']).columns \n",
        "                              if len(train_data[i].unique()) >=350 and i not in cols_date + cols_date2+cols_docs]\n",
        "# Đặc trưng dạng categorical\n",
        "cols_categorical = [i for i in train_data.select_dtypes(include = ['object']).columns\n",
        "                    if i not in cols_id + cols_label+cols_date2+cols_date+cols_docs]\n",
        "# Đặc trưng số\n",
        "cols_fts_num = [ i for i in predict_data.select_dtypes(include=['float64','int64']).columns \n",
        "                if i not in cols_id + cols_label + cols_fts_binary + cols_date2+cols_date+cols_docs]"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JD5nD91PwGlL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Đặc trưng số có dạng như categorical\n",
        "cols_num_like_cat = [i for i in cols_fts_num if len(train_data[i].unique()) <= 15 ]\n",
        "# Đặc trưng số đã kiểm chứng\n",
        "cols_num = [i for i in cols_fts_num if i not in cols_num_like_cat]\n",
        "\n",
        "# cols_categorical = list(set(cols_categorical + cols_num_like_cat)) # Không cần thiết, convert qua lại ordinalencoder\n",
        "cols_fts_num = [i for i in cols_fts_num if i not in cols_num_like_cat ]"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VKHHN7nwqvD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(train_data\n",
        "                                                   ,train_data['label'].values\n",
        "                                                   ,stratify = train_data['label'].values \n",
        "                                                   ,test_size = 0.2\n",
        "                                                   )"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QaLF849KxaJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "72545ed5-e67e-48cf-b68a-e71a0c94b54a"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(42424, 206)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sM5JDQMY5_l6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# OneHotEncoder(handle_unknown='ignore').fit(X_train[cols_categorical].astype(str).fillna('ms'))\n",
        "# X_train[cols_categorical].astype(str).info()\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-pFSxh8C1FQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lấy list value cho ordinalencoder\n",
        "categories_val = []\n",
        "for i in cols_categorical:\n",
        "  categories_val.append(list( \n",
        "      pd.concat([train_data[i], predict_data[i]]).fillna('missing').apply(lambda x: str(x)).drop_duplicates( \n",
        "  ).sort_values().values) )"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTPdZjYZwuzD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Xây dựng pipeline và fit với tree-based\n",
        "selected_columns = cols_categorical+cols_fts_num + cols_date + cols_date2 + cols_fts_binary + cols_num_like_cat #+ cols_docs\n",
        "\n",
        "class ToString(BaseEstimator, TransformerMixin):\n",
        "  '''Def 1 trans để convert các cột dạng số như categorical thành dạng cat => dùng được onehot'''\n",
        "  def fit(self, X, y = None):\n",
        "    return self\n",
        "  def transform(self, X, y = None):\n",
        "    X_ = X.copy() # creating a copy to avoid changes to original dataset\n",
        "    X_ = X_.astype(str)\n",
        "    return X_\n",
        "\n",
        "# Xử lý datetime thành year, month, date như là dạng số - chỉ phù hợp cho tree-based\n",
        "class DateToNum(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self, format = '%Y-%m-%d' ):\n",
        "    self.format = format\n",
        "  def fit(self, X, y = None):\n",
        "    return self\n",
        "  def transform(self, X, y = None):\n",
        "    X_ = X.copy() # creating a copy to avoid changes to original dataset\n",
        "\n",
        "    if self.format == '%Y%m%d':\n",
        "      str_num = 8\n",
        "    else:\n",
        "      str_num = 10\n",
        "    cols = X_.columns\n",
        "    for i in X.columns:\n",
        "      X_[i+'year_as_number'] = pd.to_datetime(X_[i].apply(lambda i: str(i)[0:str_num])\n",
        "                                        , format= self.format , errors= 'coerce').dt.year -2010\n",
        "      X_[i+'month_as_number'] = pd.to_datetime(X_[i].apply(lambda i: str(i)[0:str_num])\n",
        "                                        , format= self.format , errors= 'coerce').dt.month\n",
        "      X_[i+'day_as_number'] = pd.to_datetime(X_[i].apply(lambda i: str(i)[0:str_num])\n",
        "                                        , format= self.format , errors= 'coerce').dt.day\n",
        "    X_[cols] = X_[cols].apply(lambda x: ( pd.to_datetime(x.apply(lambda i: str(i)[0:str_num]) , format= self.format , errors= 'coerce')  \n",
        "                              - pd.to_datetime('20170101', format='%Y%m%d')).apply(lambda x: x.days)\n",
        "                       , axis = 1)\n",
        "    return X_\n",
        "\n",
        "class DocsProcessing(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self, max_features = None):\n",
        "    self.max_features_ = max_features\n",
        "  def fit(self, X, y = None):\n",
        "    self.X = X.copy()\n",
        "    try:\n",
        "      self.X['docs_combine_all_column'] = ''\n",
        "    except:\n",
        "      self.X = pd.DataFrame(data= self.X)\n",
        "      self.X['docs_combine_all_column'] = ''\n",
        "    for i in self.X.columns:\n",
        "      self.X['docs_combine_all_column'] += self.X[i]\n",
        "    self.tokenizer = CountVectorizer(max_features = self.max_features_)\n",
        "    self.tokenizer.fit(self.X['docs_combine_all_column'])\n",
        "    return self\n",
        "\n",
        "  def transform(self, X, y = None):\n",
        "    return self.tokenizer.transform(self.X['docs_combine_all_column']).todense()\n",
        "\n",
        "# docs_pipe= Pipeline([\n",
        "#       ('imputer', SimpleImputer(strategy='constant', fill_value='missing'))\n",
        "#     , ('to_string', ToString())\n",
        "#     , ('vectorize', DocsProcessing(max_features = 512))\n",
        "# ])\n",
        "\n",
        "categorical_pipe = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing'))\n",
        "    , ('to_string', ToString())\n",
        "    , ('encode', OneHotEncoder(handle_unknown='ignore')) #OrdinalEncoder(categories=categories_val))\n",
        "])\n",
        "\n",
        "ordinal_pipe = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value= -1 ))\n",
        "    , ('standard', StandardScaler()) # Với tree thì không cần standard\n",
        "])\n",
        "\n",
        "numerical_pipe = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median'))\n",
        "    , ('standard', StandardScaler()) # Với tree thì không cần standard\n",
        "])\n",
        "\n",
        "binary_pipe = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value= -1 ))\n",
        "    , ('standard', StandardScaler()) # Với tree thì không cần standard\n",
        "])\n",
        "\n",
        "datetime_pipe1 = Pipeline([\n",
        "    ('datetime1', DateToNum(format='%Y-%m-%d'))\n",
        "    , ('imputer', SimpleImputer(strategy='median'))\n",
        "    , ('standard', StandardScaler()) # Với tree thì không cần standard\n",
        "])\n",
        "\n",
        "datetime_pipe2 = Pipeline([\n",
        "    ('datetime2', DateToNum(format='%Y%m%d'))\n",
        "    , ('imputer', SimpleImputer(strategy='median'))\n",
        "    , ('standard', StandardScaler()) # Với tree thì không cần standard\n",
        "])\n",
        "\n",
        "preprocessing = ColumnTransformer(\n",
        "    [\n",
        "    #  ('docs', docs_pipe, cols_docs),\n",
        "     ('cat', categorical_pipe, cols_categorical),\n",
        "     ('ordinal', ordinal_pipe, cols_num_like_cat),\n",
        "     ('num', numerical_pipe, cols_fts_num), \n",
        "     ('binary', binary_pipe, cols_fts_binary), \n",
        "     ('date1', datetime_pipe1, cols_date),\n",
        "     ('date2', datetime_pipe2, cols_date2)\n",
        "     ])\n",
        "\n",
        "#xgb.XGBRegressor(objective=\"reg:linear\", random_state=42, n_estimators =64, max_depth =7)\n",
        "# LGBMClassifier(\n",
        "#             nthread=4,\n",
        "#             n_estimators=10000,\n",
        "#             learning_rate=0.02,\n",
        "#             num_leaves=128,\n",
        "#             colsample_bytree=0.9497036,\n",
        "#             subsample=0.8715623,\n",
        "#             max_depth=8,\n",
        "#             reg_alpha=0.041545473,\n",
        "#             reg_lambda=0.0735294,\n",
        "#             min_split_gain=0.0222415,\n",
        "#             min_child_weight=39.3259775,\n",
        "#             silent=-1,\n",
        "#             verbose=-1\n",
        "#         )\n",
        "model = GradientBoostingClassifier(random_state=42, max_depth = 8 , n_estimators = 200\n",
        "                                   , subsample = 0.5 ) #min_samples_leaf =100   , loss = 'exponential' )\n",
        "\n",
        "pipe = Pipeline([\n",
        "    ('preprocess', preprocessing),\n",
        "    # ('pca', PCA(n_components = 150)),\n",
        "    ('classifier', model)\n",
        "])\n",
        "\n",
        "pipe.fit(X_train[selected_columns], y_train)\n",
        "\n",
        "def evaluate_model(model, X, y):\n",
        "  try:\n",
        "    y_pred = model.predict_proba(X)[:, 1]\n",
        "  except:\n",
        "    y_pred = model.predict(X)\n",
        "\n",
        "  fpr_rt_lm, tpr_rt_lm, _ = roc_curve(y, y_pred)\n",
        "  auc_s = auc(fpr_rt_lm, tpr_rt_lm)\n",
        "  print('AUC: ', auc_s)\n",
        "  print('Gini: ', 2*auc_s - 1)\n",
        "\n",
        "  plt.plot([0, 1], [0, 1], 'k--')\n",
        "  plt.plot(fpr_rt_lm, tpr_rt_lm, label='RT + LR')\n",
        "  plt.xlabel('False positive rate')\n",
        "  plt.ylabel('True positive rate')\n",
        "  plt.title('ROC curve')\n",
        "  plt.legend(loc='best')\n",
        "  plt.show()\n",
        "print('Test AUC')\n",
        "evaluate_model(pipe, X_test[selected_columns], y_test)\n",
        "\n",
        "print('Train AUC')\n",
        "evaluate_model(pipe, X_train[selected_columns], y_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWrKJNfU9aDO",
        "colab_type": "text"
      },
      "source": [
        "Dự đoán kết quả"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D41J0WI22g05",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  prediction = pipe.predict_proba(predict_data[selected_columns])[:, 1]\n",
        "except:\n",
        "  prediction = pipe.predict(predict_data[selected_columns])\n",
        "res_df = pd.DataFrame({'id': predict_data.id, 'label': prediction})\n",
        "res_df.to_csv('submission.csv', index=False)"
      ],
      "execution_count": 33,
      "outputs": []
    }
  ]
}