{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.5\n",
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import feature_column as fc\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.layers import (Dense, DenseFeatures, Dropout, \n",
    "                                     BatchNormalization, Embedding, Input, Concatenate, Average,\n",
    "                                     InputLayer, Lambda)\n",
    "from tensorflow.keras.metrics import AUC, Precision, Recall\n",
    "from tensorflow.keras import backend as K, Sequential, Model\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow_addons.layers import WeightNormalization\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "import keras\n",
    "\n",
    "from sklearn.preprocessing import QuantileTransformer, StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from math import log2\n",
    "\n",
    "print(pd.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data and encoding\n",
    "\n",
    "folder_path = './'\n",
    "raw_test = pd.read_csv(folder_path + 'test_features.csv')\n",
    "raw_train = pd.read_csv(folder_path + 'train_features.csv')\n",
    "raw_targets = pd.read_csv(folder_path + 'train_targets_scored.csv')\n",
    "\n",
    "# Phân loại dữ liệu\n",
    "cols_id = ['sig_id']\n",
    "cols_to_remove = ['cp_type']\n",
    "cols_fts = [i for i in raw_train.columns if i not in cols_id +cols_to_remove]\n",
    "cols_gene = [col for col in raw_train.columns if col.startswith(\"g-\")]\n",
    "cols_cell = [col for col in raw_train.columns if col.startswith(\"c-\")]\n",
    "cols_experiment = [col for col in cols_fts if col not in cols_gene+cols_cell]\n",
    "cols_target = [i for i in raw_targets.columns if i not in cols_id]\n",
    "num_fts, num_labels = len(cols_fts), len(cols_target)\n",
    "\n",
    "# xử lý categorical\n",
    "def transform_data(input_data):\n",
    "    '''Clean data and encoding\n",
    "        * input_data: table '''\n",
    "    out = input_data.copy()\n",
    "    out['cp_dose'] = out['cp_dose'].map({'D1':0, 'D2':1})\n",
    "    out['cp_time'] = out['cp_time']/72\n",
    "    \n",
    "    return out\n",
    "\n",
    "to_train = transform_data(raw_train[raw_train['cp_type'] != 'ctl_vehicle'])\n",
    "to_train_targets = raw_targets.iloc[to_train.index]\n",
    "to_pred  = transform_data(raw_test)\n",
    "to_pred_non_ctl = to_pred[to_pred['cp_type'] != 'ctl_vehicle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-1</th>\n",
       "      <th>g-2</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-5</th>\n",
       "      <th>g-6</th>\n",
       "      <th>g-7</th>\n",
       "      <th>...</th>\n",
       "      <th>c-90</th>\n",
       "      <th>c-91</th>\n",
       "      <th>c-92</th>\n",
       "      <th>c-93</th>\n",
       "      <th>c-94</th>\n",
       "      <th>c-95</th>\n",
       "      <th>c-96</th>\n",
       "      <th>c-97</th>\n",
       "      <th>c-98</th>\n",
       "      <th>c-99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0620</td>\n",
       "      <td>0.5577</td>\n",
       "      <td>-0.2479</td>\n",
       "      <td>-0.6208</td>\n",
       "      <td>-0.1944</td>\n",
       "      <td>-1.0120</td>\n",
       "      <td>-1.0220</td>\n",
       "      <td>-0.0326</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2862</td>\n",
       "      <td>0.2584</td>\n",
       "      <td>0.8076</td>\n",
       "      <td>0.5523</td>\n",
       "      <td>-0.1912</td>\n",
       "      <td>0.6584</td>\n",
       "      <td>-0.3981</td>\n",
       "      <td>0.2139</td>\n",
       "      <td>0.3801</td>\n",
       "      <td>0.4176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0743</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.2991</td>\n",
       "      <td>0.0604</td>\n",
       "      <td>1.0190</td>\n",
       "      <td>0.5207</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>0.3372</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4265</td>\n",
       "      <td>0.7543</td>\n",
       "      <td>0.4708</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.2957</td>\n",
       "      <td>0.4899</td>\n",
       "      <td>0.1522</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>0.6077</td>\n",
       "      <td>0.7371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6280</td>\n",
       "      <td>0.5817</td>\n",
       "      <td>1.5540</td>\n",
       "      <td>-0.0764</td>\n",
       "      <td>-0.0323</td>\n",
       "      <td>1.2390</td>\n",
       "      <td>0.1715</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.7250</td>\n",
       "      <td>-0.6297</td>\n",
       "      <td>0.6103</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>-1.3240</td>\n",
       "      <td>-0.3174</td>\n",
       "      <td>-0.6417</td>\n",
       "      <td>-0.2187</td>\n",
       "      <td>-1.4080</td>\n",
       "      <td>0.6931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.5138</td>\n",
       "      <td>-0.2491</td>\n",
       "      <td>-0.2656</td>\n",
       "      <td>0.5288</td>\n",
       "      <td>4.0620</td>\n",
       "      <td>-0.8095</td>\n",
       "      <td>-1.9590</td>\n",
       "      <td>0.1792</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0990</td>\n",
       "      <td>-0.6441</td>\n",
       "      <td>-5.6300</td>\n",
       "      <td>-1.3780</td>\n",
       "      <td>-0.8632</td>\n",
       "      <td>-1.2880</td>\n",
       "      <td>-1.6210</td>\n",
       "      <td>-0.8784</td>\n",
       "      <td>-0.3876</td>\n",
       "      <td>-0.8154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.3254</td>\n",
       "      <td>-0.4009</td>\n",
       "      <td>0.9700</td>\n",
       "      <td>0.6919</td>\n",
       "      <td>1.4180</td>\n",
       "      <td>-0.8244</td>\n",
       "      <td>-0.2800</td>\n",
       "      <td>-0.1498</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.6670</td>\n",
       "      <td>1.0690</td>\n",
       "      <td>0.5523</td>\n",
       "      <td>-0.3031</td>\n",
       "      <td>0.1094</td>\n",
       "      <td>0.2885</td>\n",
       "      <td>-0.3786</td>\n",
       "      <td>0.7125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23808</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1608</td>\n",
       "      <td>-1.0500</td>\n",
       "      <td>0.2551</td>\n",
       "      <td>-0.2239</td>\n",
       "      <td>-0.2431</td>\n",
       "      <td>0.4256</td>\n",
       "      <td>-0.1166</td>\n",
       "      <td>-0.1777</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0789</td>\n",
       "      <td>0.3538</td>\n",
       "      <td>0.0558</td>\n",
       "      <td>0.3377</td>\n",
       "      <td>-0.4753</td>\n",
       "      <td>-0.2504</td>\n",
       "      <td>-0.7415</td>\n",
       "      <td>0.8413</td>\n",
       "      <td>-0.4259</td>\n",
       "      <td>0.2434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23809</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1394</td>\n",
       "      <td>-0.0636</td>\n",
       "      <td>-0.1112</td>\n",
       "      <td>-0.5080</td>\n",
       "      <td>-0.4713</td>\n",
       "      <td>0.7201</td>\n",
       "      <td>0.5773</td>\n",
       "      <td>0.3055</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1969</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>-0.8121</td>\n",
       "      <td>0.3434</td>\n",
       "      <td>0.5372</td>\n",
       "      <td>-0.3246</td>\n",
       "      <td>0.0631</td>\n",
       "      <td>0.9171</td>\n",
       "      <td>0.5258</td>\n",
       "      <td>0.4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23810</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.3260</td>\n",
       "      <td>0.3478</td>\n",
       "      <td>-0.3743</td>\n",
       "      <td>0.9905</td>\n",
       "      <td>-0.7178</td>\n",
       "      <td>0.6621</td>\n",
       "      <td>-0.2252</td>\n",
       "      <td>-0.5565</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4286</td>\n",
       "      <td>0.4426</td>\n",
       "      <td>0.0423</td>\n",
       "      <td>-0.3195</td>\n",
       "      <td>-0.8086</td>\n",
       "      <td>-0.9798</td>\n",
       "      <td>-0.2084</td>\n",
       "      <td>-0.1224</td>\n",
       "      <td>-0.2715</td>\n",
       "      <td>0.3689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23812</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6660</td>\n",
       "      <td>0.2324</td>\n",
       "      <td>0.4392</td>\n",
       "      <td>0.2044</td>\n",
       "      <td>0.8531</td>\n",
       "      <td>-0.0343</td>\n",
       "      <td>0.0323</td>\n",
       "      <td>0.0463</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1105</td>\n",
       "      <td>0.4258</td>\n",
       "      <td>-0.2012</td>\n",
       "      <td>0.1506</td>\n",
       "      <td>1.5230</td>\n",
       "      <td>0.7101</td>\n",
       "      <td>0.1732</td>\n",
       "      <td>0.7015</td>\n",
       "      <td>-0.6290</td>\n",
       "      <td>0.0740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23813</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.8598</td>\n",
       "      <td>1.0240</td>\n",
       "      <td>-0.1361</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>-0.3611</td>\n",
       "      <td>-3.6750</td>\n",
       "      <td>-1.2420</td>\n",
       "      <td>0.9146</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.3890</td>\n",
       "      <td>-1.7450</td>\n",
       "      <td>-6.6300</td>\n",
       "      <td>-4.0950</td>\n",
       "      <td>-7.3860</td>\n",
       "      <td>-1.4160</td>\n",
       "      <td>-3.5770</td>\n",
       "      <td>-0.4775</td>\n",
       "      <td>-2.1500</td>\n",
       "      <td>-4.2520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21948 rows × 874 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        cp_time  cp_dose     g-0     g-1     g-2     g-3     g-4     g-5  \\\n",
       "0      0.333333        0  1.0620  0.5577 -0.2479 -0.6208 -0.1944 -1.0120   \n",
       "1      1.000000        0  0.0743  0.4087  0.2991  0.0604  1.0190  0.5207   \n",
       "2      0.666667        0  0.6280  0.5817  1.5540 -0.0764 -0.0323  1.2390   \n",
       "3      0.666667        0 -0.5138 -0.2491 -0.2656  0.5288  4.0620 -0.8095   \n",
       "4      1.000000        1 -0.3254 -0.4009  0.9700  0.6919  1.4180 -0.8244   \n",
       "...         ...      ...     ...     ...     ...     ...     ...     ...   \n",
       "23808  1.000000        0  0.1608 -1.0500  0.2551 -0.2239 -0.2431  0.4256   \n",
       "23809  0.333333        1  0.1394 -0.0636 -0.1112 -0.5080 -0.4713  0.7201   \n",
       "23810  0.333333        1 -1.3260  0.3478 -0.3743  0.9905 -0.7178  0.6621   \n",
       "23812  0.333333        0  0.6660  0.2324  0.4392  0.2044  0.8531 -0.0343   \n",
       "23813  1.000000        0 -0.8598  1.0240 -0.1361  0.7952 -0.3611 -3.6750   \n",
       "\n",
       "          g-6     g-7  ...    c-90    c-91    c-92    c-93    c-94    c-95  \\\n",
       "0     -1.0220 -0.0326  ...  0.2862  0.2584  0.8076  0.5523 -0.1912  0.6584   \n",
       "1      0.2341  0.3372  ... -0.4265  0.7543  0.4708  0.0230  0.2957  0.4899   \n",
       "2      0.1715  0.2155  ... -0.7250 -0.6297  0.6103  0.0223 -1.3240 -0.3174   \n",
       "3     -1.9590  0.1792  ... -2.0990 -0.6441 -5.6300 -1.3780 -0.8632 -1.2880   \n",
       "4     -0.2800 -0.1498  ...  0.0042  0.0048  0.6670  1.0690  0.5523 -0.3031   \n",
       "...       ...     ...  ...     ...     ...     ...     ...     ...     ...   \n",
       "23808 -0.1166 -0.1777  ...  0.0789  0.3538  0.0558  0.3377 -0.4753 -0.2504   \n",
       "23809  0.5773  0.3055  ...  0.1969  0.0262 -0.8121  0.3434  0.5372 -0.3246   \n",
       "23810 -0.2252 -0.5565  ...  0.4286  0.4426  0.0423 -0.3195 -0.8086 -0.9798   \n",
       "23812  0.0323  0.0463  ... -0.1105  0.4258 -0.2012  0.1506  1.5230  0.7101   \n",
       "23813 -1.2420  0.9146  ... -3.3890 -1.7450 -6.6300 -4.0950 -7.3860 -1.4160   \n",
       "\n",
       "         c-96    c-97    c-98    c-99  \n",
       "0     -0.3981  0.2139  0.3801  0.4176  \n",
       "1      0.1522  0.1241  0.6077  0.7371  \n",
       "2     -0.6417 -0.2187 -1.4080  0.6931  \n",
       "3     -1.6210 -0.8784 -0.3876 -0.8154  \n",
       "4      0.1094  0.2885 -0.3786  0.7125  \n",
       "...       ...     ...     ...     ...  \n",
       "23808 -0.7415  0.8413 -0.4259  0.2434  \n",
       "23809  0.0631  0.9171  0.5258  0.4680  \n",
       "23810 -0.2084 -0.1224 -0.2715  0.3689  \n",
       "23812  0.1732  0.7015 -0.6290  0.0740  \n",
       "23813 -3.5770 -0.4775 -2.1500 -4.2520  \n",
       "\n",
       "[21948 rows x 874 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_train[cols_fts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addition information for item_info\n",
    "chemical_category = tf.transpose(\n",
    "        tf.constant(\n",
    "            [[1 if '_inhibitor' in i else 0 for i in cols_target],\n",
    "               [1 if '_agonist' in i else 0 for i in cols_target],\n",
    "               [1 if '_agent' in i else 0 for i in cols_target],\n",
    "               [1 if '_antagonist' in i else 0 for i in cols_target],\n",
    "               [1 if '_blocker' in i else 0 for i in cols_target],\n",
    "               [1 if '_activator' in i else 0 for i in cols_target]\n",
    "            ]    \n",
    "        )\n",
    "    )\n",
    "\n",
    "# Full item fts: addition + onehot\n",
    "item_ft = tf.concat(\n",
    "    [chemical_category ,\n",
    "     tf.eye(i_fts_num, dtype = tf.int32) # Create tensor 0-1 coresponse with chemical labels\n",
    "    ], axis = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_u1 (InputLayer)        [(None, 874)]             0         \n",
      "_________________________________________________________________\n",
      "layer_u1 (Dense)             (None, 256)               224000    \n",
      "_________________________________________________________________\n",
      "tf_op_layer_MatMul_33 (Tenso [(None, 206)]             0         \n",
      "_________________________________________________________________\n",
      "weight_normalization_56 (Wei (None, 512)               212481    \n",
      "_________________________________________________________________\n",
      "weight_normalization_57 (Wei (None, 256)               262913    \n",
      "_________________________________________________________________\n",
      "labels (Dense)               (None, 206)               52942     \n",
      "=================================================================\n",
      "Total params: 752,336\n",
      "Trainable params: 515,022\n",
      "Non-trainable params: 237,314\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Tiếp cận theo hướng recommend - cell -> chemical | cell/gene: user, chemial: item\n",
    "n_components = 256\n",
    "\n",
    "u_fts_num = num_fts\n",
    "i_fts_num = num_labels\n",
    "\n",
    "#User embedding\n",
    "input_u = Input(shape = (u_fts_num,) , name ='input_u1' )\n",
    "# layer_u= WeightNormalization(Dense(700, activation=\"relu\", kernel_initializer='he_normal')) (input_u)\n",
    "layer_u = Dense(n_components, activation = 'relu', kernel_initializer='he_normal', name ='layer_u1') (input_u)\n",
    "\n",
    "#Item embedding\n",
    "layer_i = Dense(n_components, activation = 'relu', kernel_initializer='he_normal', name ='layer_u1') (item_ft)\n",
    "\n",
    "# Dot product user - item\n",
    "def dot_2layer(x):\n",
    "    return K.dot( x[0], K.transpose(x[1]))\n",
    "dot_ui = Lambda( dot_2layer, name = 'lambda_dot' ) ([layer_u,layer_i])\n",
    "# dot_ui= BatchNormalization() (dot_ui)\n",
    "dot_ui= WeightNormalization(Dense(512, activation=\"relu\", kernel_initializer='he_normal')) (dot_ui)\n",
    "# dot_ui= BatchNormalization() (dot_ui)\n",
    "dot_ui= WeightNormalization(Dense(256, activation=\"relu\", kernel_initializer='he_normal')) (dot_ui)\n",
    "# dot_ui= BatchNormalization() (dot_ui)\n",
    "dot_ui = Dense(i_fts_num, activation = 'sigmoid', kernel_initializer='he_normal', name = 'labels')(dot_ui)\n",
    "\n",
    "# Compile model\n",
    "model = Model(inputs=[input_u, ], outputs= [dot_ui])\n",
    "opt = Adam(lr=0.0005)\n",
    "model.compile(loss= BinaryCrossentropy(label_smoothing=0.0005), optimizer=opt)\n",
    "print( model.summary() )\n",
    "\n",
    "# tf.keras.utils.plot_model(model,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "241/241 [==============================] - 1s 6ms/step - loss: 0.0438 - val_loss: 0.0217 - lr: 5.0000e-04\n",
      "Epoch 2/150\n",
      "241/241 [==============================] - 1s 5ms/step - loss: 0.0208 - val_loss: 0.0206 - lr: 5.0000e-04\n",
      "Epoch 3/150\n",
      "241/241 [==============================] - 1s 5ms/step - loss: 0.0197 - val_loss: 0.0202 - lr: 5.0000e-04\n",
      "Epoch 4/150\n",
      "241/241 [==============================] - 1s 5ms/step - loss: 0.0189 - val_loss: 0.0199 - lr: 5.0000e-04\n",
      "Epoch 5/150\n",
      "241/241 [==============================] - 1s 5ms/step - loss: 0.0182 - val_loss: 0.0197 - lr: 5.0000e-04\n",
      "Epoch 6/150\n",
      "241/241 [==============================] - 1s 5ms/step - loss: 0.0175 - val_loss: 0.0199 - lr: 5.0000e-04\n",
      "Epoch 7/150\n",
      "241/241 [==============================] - 1s 5ms/step - loss: 0.0168 - val_loss: 0.0200 - lr: 5.0000e-04\n",
      "Epoch 8/150\n",
      "241/241 [==============================] - 1s 6ms/step - loss: 0.0160 - val_loss: 0.0204 - lr: 5.0000e-04\n",
      "Epoch 9/150\n",
      "241/241 [==============================] - 1s 5ms/step - loss: 0.0152 - val_loss: 0.0206 - lr: 5.0000e-04\n",
      "Epoch 10/150\n",
      "230/241 [===========================>..] - ETA: 0s - loss: 0.0143\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001500000071246177.\n",
      "241/241 [==============================] - 1s 5ms/step - loss: 0.0143 - val_loss: 0.0208 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "241/241 [==============================] - 1s 5ms/step - loss: 0.0126 - val_loss: 0.0211 - lr: 1.5000e-04\n",
      "Epoch 12/150\n",
      "241/241 [==============================] - 1s 5ms/step - loss: 0.0120 - val_loss: 0.0215 - lr: 1.5000e-04\n",
      "Epoch 13/150\n",
      "241/241 [==============================] - 1s 5ms/step - loss: 0.0116 - val_loss: 0.0220 - lr: 1.5000e-04\n",
      "Epoch 14/150\n",
      "241/241 [==============================] - 1s 5ms/step - loss: 0.0111 - val_loss: 0.0226 - lr: 1.5000e-04\n",
      "Epoch 15/150\n",
      "230/241 [===========================>..] - ETA: 0s - loss: 0.0108\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 4.500000213738531e-05.\n",
      "241/241 [==============================] - 1s 5ms/step - loss: 0.0107 - val_loss: 0.0230 - lr: 1.5000e-04\n",
      "Epoch 16/150\n",
      "241/241 [==============================] - 1s 5ms/step - loss: 0.0101 - val_loss: 0.0230 - lr: 4.5000e-05\n",
      "Epoch 17/150\n",
      "241/241 [==============================] - 1s 5ms/step - loss: 0.0099 - val_loss: 0.0231 - lr: 4.5000e-05\n",
      "Epoch 18/150\n",
      "241/241 [==============================] - 1s 5ms/step - loss: 0.0098 - val_loss: 0.0233 - lr: 4.5000e-05\n",
      "Epoch 19/150\n",
      "241/241 [==============================] - 1s 5ms/step - loss: 0.0096 - val_loss: 0.0236 - lr: 4.5000e-05\n",
      "Epoch 20/150\n",
      "235/241 [============================>.] - ETA: 0s - loss: 0.0095\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.3500000204658135e-05.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "241/241 [==============================] - 1s 5ms/step - loss: 0.0095 - val_loss: 0.0239 - lr: 4.5000e-05\n",
      "Epoch 00020: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x166b8556148>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=5, mode='min', min_lr=1E-5, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=1E-5, patience=15, mode='min',restore_best_weights=True, verbose=1)\n",
    "\n",
    "model.fit(to_train[cols_fts], to_train_targets[cols_target], batch_size=64, epochs=150,validation_split = 0.3\n",
    "         ,callbacks=[reduce_lr, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dùng batch 64 -> dùng tiếp 128 thì thấy loss giảm\n",
    "Tăng batch_size lên 256 thì thấy val_loss tăng\n",
    "Training không với batch 128 thì thấy loss vẫn cao\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
