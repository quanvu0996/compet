{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport copy\n\nimport tensorflow as tf\nfrom tensorflow import feature_column as fc\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.layers import (Dense, DenseFeatures, Dropout, \n                                     BatchNormalization, Embedding, Input, Concatenate, Average,\n                                     InputLayer, Lambda)\nfrom tensorflow.keras.metrics import AUC, Precision, Recall\nfrom tensorflow.keras import backend as K, Sequential, Model\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.optimizers import Adam, RMSprop\n\nimport tensorflow_addons as tfa\nfrom tensorflow_addons.layers import WeightNormalization\nfrom keras.wrappers.scikit_learn import KerasRegressor\nimport keras\n\nfrom sklearn.preprocessing import QuantileTransformer, StandardScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\n\nimport matplotlib.pyplot as plt\nfrom math import log2\n\nimport sys\nsys.path.append('../input/iterativestratification')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nprint(pd.__version__)\nprint(tf.__version__)","execution_count":5,"outputs":[{"output_type":"stream","text":"1.1.3\n2.3.1\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Loading data and encoding\n\nfolder_path = '../input/lish-moa/'\nraw_test = pd.read_csv(folder_path + 'test_features.csv')\nraw_train = pd.read_csv(folder_path + 'train_features.csv')\nraw_targets = pd.read_csv(folder_path + 'train_targets_scored.csv')\n\n# Phân loại dữ liệu\ncols_id = ['sig_id']\ncols_to_remove = ['cp_type']\ncols_fts = [i for i in raw_train.columns if i not in cols_id +cols_to_remove]\ncols_gene = [col for col in raw_train.columns if col.startswith(\"g-\")]\ncols_cell = [col for col in raw_train.columns if col.startswith(\"c-\")]\ncols_experiment = [col for col in cols_fts if col not in cols_gene+cols_cell]\ncols_target = [i for i in raw_targets.columns if i not in cols_id]\nnum_fts, num_labels = len(cols_fts), len(cols_target)\n\n# xử lý categorical\ndef transform_data(input_data):\n    '''Clean data and encoding\n        * input_data: table '''\n    out = input_data.copy()\n    out['cp_dose'] = out['cp_dose'].map({'D1':0, 'D2':1})\n    out['cp_time'] = out['cp_time']/72\n    \n    return out\n\nto_train = transform_data(raw_train[raw_train['cp_type'] != 'ctl_vehicle'])\nto_train_targets = raw_targets.iloc[to_train.index]\nto_pred  = transform_data(raw_test)\nto_pred_non_ctl = to_pred[to_pred['cp_type'] != 'ctl_vehicle']","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# preprocessing pipeline\ndef pipe_line_builder(quantiles_num, pca_dims):\n    '''Dựng pipe line cho từng nhóm columns\n    :quantiles_num: int: số quantile khi normalise\n    :pca_dims: int: số chiều pca'''\n    norm = QuantileTransformer(n_quantiles=quantiles_num,random_state=0, output_distribution=\"normal\")\n    pca = PCA(n_components = pca_dims)\n    \n    p_var_norm = Pipeline([ \n        ('norm', norm) ])\n    p_var_norm_pca = Pipeline([ \n        ('norm1', norm),\n        ('pca', pca),\n        ('norm2', norm)\n    ])\n    return FeatureUnion([\n        ('norm', p_var_norm)\n        , ('norm_pca', p_var_norm_pca) \n        ])\n\n\npipe = ColumnTransformer([\n     ('gene', pipe_line_builder(quantiles_num = 100, pca_dims = 600), cols_gene),\n     ('cell', pipe_line_builder(quantiles_num = 100, pca_dims = 50), cols_cell),\n     ('experiment', 'passthrough', cols_experiment)\n    ])\n\npipe = Pipeline([\n    ('norm_and_pca', pipe),\n    ('variance',  VarianceThreshold(0.))\n])","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential([\n    BatchNormalization(),\n    WeightNormalization(Dense( 1500, activation = 'elu', kernel_initializer='he_normal')),\n    \n    BatchNormalization(),\n    Dropout(0.2619422201258426),\n    WeightNormalization(Dense( 1500, activation = 'elu', kernel_initializer='he_normal')),\n    \n    BatchNormalization(),\n    Dropout(0.2619422201258426),\n    WeightNormalization(Dense( 1500, activation = 'elu', kernel_initializer='he_normal')),\n    \n    Dense(num_labels, activation = 'sigmoid', kernel_initializer='he_normal')\n])\n\nstep = tf.Variable(0, trainable=False)\nschedule = tf.optimizers.schedules.PiecewiseConstantDecay(\n    [10000, 15000], [1e-0, 1e-1, 1e-2])\nlr = 1e-1 * schedule(step)\nwd = lambda: 1e-3 * schedule(step)\nopt = tfa.optimizers.AdamW(learning_rate=lr, weight_decay=wd)\n\nmodel.compile(loss= BinaryCrossentropy(label_smoothing=0.01), optimizer='adam')\n# tf.keras.utils.plot_model(model,show_shapes=True)","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NFOLDS = 7\n\ndef df_by_index(df, indexes, cols = None):\n    if cols is None:\n        cols = df.columns\n    return df[df.index.isin(indexes)][cols]\n\nkf = KFold(n_splits= NFOLDS, shuffle = True)\n\nss= np.zeros([to_pred_non_ctl.shape[0], num_labels])\n\nfrom_fols =0\nfor train_index, val_index in kf.split(to_train):\n    print('Training at fold: ', from_fols)\n    from_fols += 1\n    tf.keras.backend.clear_session()\n    \n    fold_X_train = df_by_index(to_train, train_index)\n    fold_y_train = df_by_index(to_train_targets, train_index, cols_target)\n    \n    fold_X_val = df_by_index(to_train, val_index)\n    fold_y_val = df_by_index(to_train_targets, val_index, cols_target)\n    \n    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=5, mode='min', min_lr=1E-5, verbose=1)\n    early_stopping = EarlyStopping(monitor='val_loss', min_delta=1E-5, patience=15, mode='min',restore_best_weights=True, verbose=1)\n    \n    model.fit(\n        pipe.fit_transform(fold_X_train), \n        fold_y_train, \n        validation_data = (pipe.transform(fold_X_val), fold_y_val),\n        batch_size=64, \n        epochs=150,\n        callbacks=[reduce_lr, early_stopping]\n        )\n    \n    ss += model.predict(pred_list_arr)\n\nss = ss/NFOLDS","execution_count":null,"outputs":[{"output_type":"stream","text":"Epoch 1/150\n272/272 [==============================] - 29s 105ms/step - loss: 0.0407 - val_loss: 0.0218\nEpoch 2/150\n272/272 [==============================] - 27s 100ms/step - loss: 0.0200 - val_loss: 0.0225\nEpoch 3/150\n272/272 [==============================] - 29s 106ms/step - loss: 0.0182 - val_loss: 0.0226\nEpoch 4/150\n272/272 [==============================] - 27s 100ms/step - loss: 0.0159 - val_loss: 0.0246\nEpoch 5/150\n272/272 [==============================] - 28s 102ms/step - loss: 0.0213 - val_loss: 0.0206\nEpoch 6/150\n272/272 [==============================] - 27s 99ms/step - loss: 0.0150 - val_loss: 0.0212\nEpoch 7/150\n272/272 [==============================] - 27s 101ms/step - loss: 0.0135 - val_loss: 0.0218\nEpoch 8/150\n272/272 [==============================] - 27s 101ms/step - loss: 0.0124 - val_loss: 0.0239\nEpoch 9/150\n222/272 [=======================>......] - ETA: 5s - loss: 0.0124","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Final data\npipe = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\npipe.fit(to_train[cols_fts].append(to_pred[cols_fts]))\n\nX_train = pipe.transform(to_train[cols_fts])\nX_pred =  pipe.transform(to_pred_non_ctl[cols_fts])\ny_train = to_train_targets[cols_target]\nX_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Thử nghiệm ResFMnet**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tiếp cận theo hướng recommend - cell -> chemical | cell/gene: user, chemial: item\nn_components = 256\n\nu_fts_num = to_pred.shape[1]#num_fts\ni_fts_num = num_labels\n\ng_ft_num = len(cols_gene)\nc_ft_num = len(cols_cell)\ne_ft_num = len(cols_experiment)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#1. User embedding\n#1.1. Gene fts\ninput_g = Input(shape = (g_ft_num,) )\nlayer_g = WeightNormalization(Dense( 512, activation = 'elu', kernel_initializer='he_normal')) (input_g)\nlayer_g = Dropout(0.2619422201258426) (layer_g)\nlayer_g = BatchNormalization() (layer_g)\n\nlayer_g = WeightNormalization(Dense( 320, activation = 'elu', kernel_initializer='he_normal')) (layer_g)\nlayer_g = Dropout(0.2619422201258426) (layer_g)\nlayer_g = BatchNormalization() (layer_g)\n\n#1.2. Cell fts\ninput_c = Input(shape = (c_ft_num,) )\nlayer_c = WeightNormalization(Dense( 80, activation = 'elu', kernel_initializer='he_normal')) (input_c)\nlayer_c = Dropout(0.2619422201258426) (layer_c)\nlayer_c = BatchNormalization() (layer_c)\n\n#1.3. Experiment fts\nlayer_e = Input(shape = (e_ft_num,) )\n\n#1.4 user full fts with residual connection\nlayer_u = Concatenate() ([layer_g,input_g, layer_c,input_c, layer_e])\n\nlayer_u = WeightNormalization(Dense( n_components*2, activation = 'elu', kernel_initializer='he_normal')) (layer_u)\nlayer_u = Dropout(0.2619422201258426) (layer_u)\nlayer_u = BatchNormalization() (layer_u)\n\nlayer_u = WeightNormalization(Dense( n_components, activation = 'elu', kernel_initializer='he_normal')) (layer_u)\nlayer_u = Dropout(0.2619422201258426) (layer_u)\nlayer_u = BatchNormalization() (layer_u)\n\n\n\n#2. Item embedding\n#2.1. Addition information for item_info\nchemical_category = tf.transpose(\n        tf.constant(\n            [[1 if '_inhibitor' in i else 0 for i in cols_target],\n               [1 if '_agonist' in i else 0 for i in cols_target],\n               [1 if '_agent' in i else 0 for i in cols_target],\n               [1 if '_antagonist' in i else 0 for i in cols_target],\n               [1 if '_blocker' in i else 0 for i in cols_target],\n               [1 if '_activator' in i else 0 for i in cols_target] \n             ]))\n\n#2.2 Full item fts: addition + onehot\nitem_ft = tf.concat(\n    [chemical_category ,\n     tf.eye(i_fts_num, dtype = tf.int32) # Create tensor 0-1 coresponse with chemical labels\n    ], axis = 1\n)\nlayer_i = Dense(n_components, activation = 'relu', kernel_initializer='he_normal', name ='layer_u1') (item_ft)\n\n\n#3. Dot product user - item\ndef dot_2layer(x):\n    return K.dot( x[0], K.transpose(x[1]))\ndot_ui = Lambda( dot_2layer, name = 'lambda_dot' ) ([layer_u,layer_i])\ndot_ui= WeightNormalization(Dense(512, activation=\"relu\", kernel_initializer='he_normal')) (dot_ui)\ndot_ui= BatchNormalization() (dot_ui)\ndot_ui = WeightNormalization(Dense(i_fts_num, activation = 'sigmoid', kernel_initializer='he_normal', name = 'labels'))(dot_ui)\n\n# Compile model\nmodel = Model(inputs=[layer_e, input_g, input_c, ], outputs= [dot_ui])\n\nstep = tf.Variable(0, trainable=False)\nschedule = tf.optimizers.schedules.PiecewiseConstantDecay(\n    [10000, 15000], [1e-0, 1e-1, 1e-2])\nlr = 1e-1 * schedule(step)\nwd = lambda: 1e-3 * schedule(step)\nopt = tfa.optimizers.AdamW(learning_rate=lr, weight_decay=wd)\n\nmodel.compile(loss= BinaryCrossentropy(label_smoothing=0.0005), optimizer='adam')\nprint( model.summary() )\n\ntf.keras.utils.plot_model(model,show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_train_set(X_train):\n    X_train_e = X_train[ :, :e_ft_num]\n    X_train_g = X_train[ :, e_ft_num: (e_ft_num+ g_ft_num)]\n    X_train_c = X_train[ :, (e_ft_num+ g_ft_num): (e_ft_num+ g_ft_num+ c_ft_num)]\n    return [ X_train_e, X_train_g, X_train_c]\n\ntrain_list_arr = get_train_set(X_train)\npred_list_arr = get_train_set(X_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=5, mode='min', min_lr=1E-5, verbose=1)\nearly_stopping = EarlyStopping(monitor='val_loss', min_delta=1E-5, patience=15, mode='min',restore_best_weights=True, verbose=1)\n\nss= np.zeros([to_pred_non_ctl.shape[0], num_labels])\n\nN_STARTS = 3 # <-- change it\nfor seed in range(N_STARTS):\n    print('Trainging at seed: ', seed)\n    history = model.fit(\n                    train_list_arr, \n                    y_train, \n                    batch_size=64*(seed+1), \n                    epochs=150,\n                    validation_split = 0.3,\n                    callbacks=[reduce_lr, early_stopping])\n    ss += model.predict(pred_list_arr)\n    \n    K.clear_session()\n#     del model, history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict non ctl vehicle\ndf_preds_non_ctl =  pd.DataFrame(ss, columns= cols_target, index = to_pred_non_ctl.index)\n\n# concat with all to pred values\ndf_preds = pd.concat([ to_pred[cols_id], df_preds_non_ctl], axis = 1).fillna(0)\n\n# to csv\ndf_preds.to_csv(\"submission.csv\", index = None)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}