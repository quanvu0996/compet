{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "quanvh8-mlp-nets.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vW6BvYLEFyiS"
      },
      "source": [
        "## **Using sklearn to preprocessing data and tensorflow for modeling**\n",
        "***Preprocessing:***\n",
        "* Focus only in feature scaling (normalise) and feature combine (PCA)\n",
        "* Building pipeline\n",
        "\n",
        "***Modeling:***\n",
        "* MLP\n",
        "inspired by https://www.kaggle.com/riadalmadani/pytorch-cv-0-0145-lb-0-01839#Single-fold-training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "1JySdxYBFyiU",
        "outputId": "79927fa6-5d5e-4d0e-e35e-abdaaa237404",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import copy\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import feature_column as fc\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.layers import Dense, DenseFeatures, Dropout, BatchNormalization, Embedding, Input, Concatenate, Average\n",
        "from tensorflow.keras.metrics import AUC, Precision, Recall\n",
        "from tensorflow.keras import backend as K, Sequential, Model\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "from tensorflow_addons.layers import WeightNormalization\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "\n",
        "from sklearn.preprocessing import QuantileTransformer, StandardScaler\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from math import log2\n",
        "\n",
        "print(pd.__version__)\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.1.4\n",
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-l_B67thF2My",
        "outputId": "3ce3f224-e25b-4585-a163-cbe2c7a2b98f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_XxEH65Fyid"
      },
      "source": [
        "## Data loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "MGeFv-8-Fyie"
      },
      "source": [
        "# Loading data and encoding\n",
        "\n",
        "folder_path = '/content/drive/My Drive/Data/colabs_data/MOA_kaggle/'\n",
        "raw_test = pd.read_csv(folder_path + 'test_features.csv')\n",
        "raw_train = pd.read_csv(folder_path + 'train_features.csv')\n",
        "raw_targets = pd.read_csv(folder_path + 'train_targets_scored.csv')\n",
        "\n",
        "# Phân loại dữ liệu\n",
        "cols_id = ['sig_id']\n",
        "cols_to_remove = ['cp_type']\n",
        "cols_fts = [i for i in raw_train.columns if i not in cols_id +cols_to_remove]\n",
        "cols_gene = [col for col in raw_train.columns if col.startswith(\"g-\")]\n",
        "cols_cell = [col for col in raw_train.columns if col.startswith(\"c-\")]\n",
        "cols_experiment = [col for col in cols_fts if col not in cols_gene+cols_cell]\n",
        "cols_target = [i for i in raw_targets.columns if i not in cols_id]\n",
        "num_fts, num_labels = len(cols_fts), len(cols_target)\n",
        "\n",
        "# xử lý categorical\n",
        "def transform_data(input_data):\n",
        "    '''Clean data and encoding\n",
        "        * input_data: table '''\n",
        "    out = input_data.copy()\n",
        "    out['cp_dose'] = out['cp_dose'].map({'D1':0, 'D2':1})\n",
        "    out['cp_time'] = out['cp_time']/72\n",
        "    \n",
        "    return out\n",
        "\n",
        "to_train = transform_data(raw_train ) #[raw_train['cp_type'] != 'ctl_vehicle'])\n",
        "to_train_targets = raw_targets.iloc[to_train.index]\n",
        "to_pred  = transform_data(raw_test)\n",
        "to_pred_non_ctl = to_pred[to_pred['cp_type'] != 'ctl_vehicle']"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLPClXrcFyik"
      },
      "source": [
        "## Feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "O49vGPqIFyim"
      },
      "source": [
        "# preprocessing pipeline\n",
        "def pipe_line_builder( quantiles_num):\n",
        "    '''Dựng pipe line cho từng nhóm columns\n",
        "    :quantiles_num: int: số quantile khi normalise\n",
        "    :pca_dims: int: số chiều pca'''\n",
        "#     variance = VarianceThreshold(variance_threshold)\n",
        "    # norm = StandardScaler()\n",
        "    norm = QuantileTransformer(n_quantiles=quantiles_num,random_state=0, output_distribution=\"normal\")\n",
        "    # pca = PCA(n_components = pca_dims)\n",
        "    p_var_norm = Pipeline([ \n",
        "#         ('var', variance),\n",
        "        ('norm', norm) ])\n",
        "#     p_var_norm_pca = Pipeline([ \n",
        "# #         ('var', variance),\n",
        "#         ('pca', pca),\n",
        "#         ('norm', norm)\n",
        "#     ])\n",
        "    return FeatureUnion([\n",
        "        ('norm', p_var_norm)\n",
        "        , ('norm_pca', p_var_norm_pca) \n",
        "        ])\n",
        "\n",
        "pipe = ColumnTransformer([\n",
        "     ('gene', pipe_line_builder(quantiles_num = 100), cols_gene),\n",
        "     ('cell', pipe_line_builder(quantiles_num = 100), cols_cell),\n",
        "     ('experiment', pipe_line_builder(quantiles_num = 100), cols_experiment)\n",
        "#      ('all', Pipeline([ \n",
        "#                 ('norm', QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")), \n",
        "#                 ('pca', PCA(n_components = int(len(cols_fts)*0.8)) ) ]), cols_fts)\n",
        "    ])"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "6EsGPkpmFyis",
        "outputId": "f44109fe-dec8-4d14-e0e5-ca9f63e3aff0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Final data\n",
        "# Trick\n",
        "pipe.fit(to_train[cols_cell+cols_gene])\n",
        "X_train = pipe.transform(to_train[cols_cell+cols_gene])\n",
        "y_train = to_train_targets[cols_target]\n",
        "X_train"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.13484916,  0.90768744, -0.41638451, ...,  0.31798879,\n",
              "         0.54566219,  0.6413394 ],\n",
              "       [ 0.11928153,  0.68173822,  0.27239921, ...,  0.17968379,\n",
              "         0.91916084,  1.16583255],\n",
              "       [ 0.77997254,  0.94646298,  1.42534985, ..., -0.27763452,\n",
              "        -1.12308772,  1.08923459],\n",
              "       ...,\n",
              "       [ 0.52514873,  0.63122535,  0.28817292, ...,  1.16710363,\n",
              "         1.02059289,  0.5843928 ],\n",
              "       [ 0.8164071 ,  0.4176183 ,  0.43163123, ...,  1.0779754 ,\n",
              "        -0.70199778,  0.13396695],\n",
              "       [-1.24309611,  1.56773029, -0.26957347, ..., -0.58170296,\n",
              "        -1.29840708, -1.84722499]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPMTjraAMYBP",
        "outputId": "48baa413-2bcf-4e10-aedf-13152b0dd171",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "to_train[col]"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        1.0620\n",
              "1        0.0743\n",
              "2        0.6280\n",
              "3       -0.5138\n",
              "4       -0.3254\n",
              "          ...  \n",
              "23809    0.1394\n",
              "23810   -1.3260\n",
              "23811    0.3942\n",
              "23812    0.6660\n",
              "23813   -0.8598\n",
              "Name: g-0, Length: 23814, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHw20zjjK_s5",
        "outputId": "97899633-3b4c-4953-ac8f-195b6dbe3a36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "col = 'g-0'\n",
        "transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n",
        "vec_len = len(to_train[col].values)\n",
        "raw_vec = to_train[col].values.reshape(vec_len, 1)\n",
        "transformer.fit(raw_vec)\n",
        "\n",
        "transformer.transform(raw_vec).reshape(1, vec_len)[0]"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.13484916,  0.11928153,  0.77997254, ...,  0.52514873,\n",
              "        0.8164071 , -1.24309611])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "02AwCbjCFyix",
        "outputId": "28d2d8d4-3887-400e-f9d1-f09bde146f58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "norm = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n",
        "norm.fit_transform(to_train['g-0'].values.reshape(21948, 1))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.11180063],\n",
              "       [ 0.10566707],\n",
              "       [ 0.76703558],\n",
              "       ...,\n",
              "       [-1.94607703],\n",
              "       [ 0.80391134],\n",
              "       [-1.27070334]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "P-pPczbDFyi2",
        "outputId": "b2017f04-4c45-40f2-f1ca-469a99c950da"
      },
      "source": [
        "len(cols_gene+cols_cell+cols_experiment)+640+72+int(len(cols_experiment)*0.8)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1587"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpjZ6Z3cMot3",
        "outputId": "1194055b-5c16-4a89-de5e-ce8c2385acf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import sklearn\n",
        "print(sklearn.__version__)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.22.2.post1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sK0UCefdFyi8"
      },
      "source": [
        "## Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "d3-PfnZDFyi9"
      },
      "source": [
        "# Define model\n",
        "model = Sequential([\n",
        "    BatchNormalization(),\n",
        "    WeightNormalization(Dense(1024, activation=\"relu\")),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "    WeightNormalization(Dense(512, activation=\"relu\")),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "    WeightNormalization(Dense(256, activation=\"relu\")),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "    WeightNormalization(Dense(num_labels, activation=\"sigmoid\"))\n",
        "])\n",
        "\n",
        "p_min = 0.001\n",
        "p_max = 0.999\n",
        "def logloss(y_true, y_pred):\n",
        "    y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n",
        "    return -K.mean(y_true*K.log(y_pred) + (1-y_true)*K.log(1-y_pred))\n",
        "\n",
        "model.compile(optimizer='adam', loss=BinaryCrossentropy(label_smoothing=0.0001), metrics=logloss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Wg-C19m4FyjE",
        "outputId": "54c9eb56-f5fb-44ba-faca-3f15503f39a0"
      },
      "source": [
        "# Training and evaluate\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_logloss', factor=0.3, patience=5, mode='min', min_lr=1E-5, verbose=1)\n",
        "early_stopping = EarlyStopping(monitor='val_logloss', min_delta=1E-5, patience=15, mode='min',restore_best_weights=True, verbose=1)\n",
        "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=3,epsilon = 1e-4, mode = 'min',verbose=1)\n",
        "# early_stopping = EarlyStopping(monitor='val_loss',min_delta=0,patience=10,mode='auto',verbose=1,baseline=None,restore_best_weights=True)\n",
        "\n",
        "hist = model.fit(X_train,y_train, batch_size=64, epochs=150,validation_split = 0.2\n",
        "                 ,callbacks=[reduce_lr, early_stopping]\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "275/275 [==============================] - 3s 10ms/step - loss: 0.2852 - logloss: 0.2845 - val_loss: 0.0354 - val_logloss: 0.0352\n",
            "Epoch 2/150\n",
            "275/275 [==============================] - 3s 10ms/step - loss: 0.0275 - logloss: 0.0272 - val_loss: 0.0222 - val_logloss: 0.0219\n",
            "Epoch 3/150\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 0.0216 - logloss: 0.0213 - val_loss: 0.0201 - val_logloss: 0.0199\n",
            "Epoch 4/150\n",
            "275/275 [==============================] - 2s 9ms/step - loss: 0.0197 - logloss: 0.0194 - val_loss: 0.0189 - val_logloss: 0.0187\n",
            "Epoch 5/150\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 0.0186 - logloss: 0.0183 - val_loss: 0.0184 - val_logloss: 0.0181\n",
            "Epoch 6/150\n",
            "275/275 [==============================] - 2s 9ms/step - loss: 0.0177 - logloss: 0.0174 - val_loss: 0.0179 - val_logloss: 0.0177\n",
            "Epoch 7/150\n",
            "275/275 [==============================] - 2s 9ms/step - loss: 0.0169 - logloss: 0.0167 - val_loss: 0.0179 - val_logloss: 0.0176\n",
            "Epoch 8/150\n",
            "275/275 [==============================] - 2s 8ms/step - loss: 0.0162 - logloss: 0.0161 - val_loss: 0.0177 - val_logloss: 0.0174\n",
            "Epoch 9/150\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 0.0155 - logloss: 0.0153 - val_loss: 0.0177 - val_logloss: 0.0175\n",
            "Epoch 10/150\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 0.0145 - logloss: 0.0144 - val_loss: 0.0178 - val_logloss: 0.0175\n",
            "Epoch 11/150\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 0.0133 - logloss: 0.0133 - val_loss: 0.0180 - val_logloss: 0.0177\n",
            "Epoch 12/150\n",
            "275/275 [==============================] - 2s 8ms/step - loss: 0.0120 - logloss: 0.0121 - val_loss: 0.0186 - val_logloss: 0.0181\n",
            "Epoch 13/150\n",
            "274/275 [============================>.] - ETA: 0s - loss: 0.0106 - logloss: 0.0107\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 0.0106 - logloss: 0.0107 - val_loss: 0.0190 - val_logloss: 0.0183\n",
            "Epoch 14/150\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 0.0084 - logloss: 0.0085 - val_loss: 0.0190 - val_logloss: 0.0182\n",
            "Epoch 15/150\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 0.0071 - logloss: 0.0073 - val_loss: 0.0194 - val_logloss: 0.0184\n",
            "Epoch 16/150\n",
            "275/275 [==============================] - 2s 8ms/step - loss: 0.0063 - logloss: 0.0065 - val_loss: 0.0197 - val_logloss: 0.0185\n",
            "Epoch 17/150\n",
            "275/275 [==============================] - 3s 10ms/step - loss: 0.0056 - logloss: 0.0058 - val_loss: 0.0202 - val_logloss: 0.0187\n",
            "Epoch 18/150\n",
            "274/275 [============================>.] - ETA: 0s - loss: 0.0051 - logloss: 0.0053\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 0.0051 - logloss: 0.0053 - val_loss: 0.0206 - val_logloss: 0.0188\n",
            "Epoch 19/150\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 0.0045 - logloss: 0.0047 - val_loss: 0.0207 - val_logloss: 0.0188\n",
            "Epoch 20/150\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 0.0042 - logloss: 0.0045 - val_loss: 0.0209 - val_logloss: 0.0188\n",
            "Epoch 21/150\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 0.0040 - logloss: 0.0042 - val_loss: 0.0210 - val_logloss: 0.0189\n",
            "Epoch 22/150\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 0.0038 - logloss: 0.0041 - val_loss: 0.0213 - val_logloss: 0.0190\n",
            "Epoch 23/150\n",
            "269/275 [============================>.] - ETA: 0s - loss: 0.0036 - logloss: 0.0039\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "Restoring model weights from the end of the best epoch.\n",
            "275/275 [==============================] - 2s 8ms/step - loss: 0.0036 - logloss: 0.0039 - val_loss: 0.0215 - val_logloss: 0.0190\n",
            "Epoch 00023: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "uw4-Ka39FyjJ"
      },
      "source": [
        "# Test thử resnet\n",
        "p_min = 0.001\n",
        "p_max = 0.999\n",
        "def logloss(y_true, y_pred):\n",
        "    y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n",
        "    return -K.mean(y_true*K.log(y_pred) + (1-y_true)*K.log(1-y_pred))\n",
        "\n",
        "def build_model(n_features, n_features_2, n_labels, label_smoothing = 0.0005):    \n",
        "    input_1 = Input(shape = (n_features,), name = 'Input1')\n",
        "    input_2 = Input(shape = (n_features_2,), name = 'Input2')\n",
        "\n",
        "    head_1 = Sequential([\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.2),\n",
        "        Dense(512, activation=\"elu\"), \n",
        "        BatchNormalization(),\n",
        "        Dense(256, activation = \"elu\")\n",
        "        ],name='Head1') \n",
        "\n",
        "    input_3 = head_1(input_1)\n",
        "    input_3_concat = Concatenate()([input_2, input_3])\n",
        "\n",
        "    head_2 = Sequential([\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(512, \"relu\"),\n",
        "        BatchNormalization(),\n",
        "        Dense(512, \"elu\"),\n",
        "        BatchNormalization(),\n",
        "        Dense(256, \"relu\"),\n",
        "        BatchNormalization(),\n",
        "        Dense(256, \"elu\")\n",
        "        ],name='Head2')\n",
        "\n",
        "    input_4 = head_2(input_3_concat)\n",
        "    input_4_avg = Average()([input_3, input_4]) \n",
        "\n",
        "    head_3 = Sequential([\n",
        "        BatchNormalization(),\n",
        "        Dense(256, kernel_initializer='lecun_normal', activation='selu'),\n",
        "        BatchNormalization(),\n",
        "        Dense(n_labels, kernel_initializer='lecun_normal', activation='selu'),\n",
        "        BatchNormalization(),\n",
        "        Dense(n_labels, activation=\"sigmoid\")\n",
        "        ],name='Head3')\n",
        "\n",
        "    output = head_3(input_4_avg)\n",
        "\n",
        "\n",
        "    model = Model(inputs = [input_1, input_2], outputs = output)\n",
        "    model.compile(optimizer='adam', loss= BinaryCrossentropy(label_smoothing=label_smoothing), metrics=logloss)\n",
        "    \n",
        "    return model\n",
        "\n",
        "num_all_ft = X_train.shape[1]\n",
        "num_ft_1 = len(cols_gene)+640\n",
        "num_ft_2 = num_all_ft - num_ft_1\n",
        "\n",
        "model = build_model(num_ft_1, num_ft_2, len(cols_target))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "4-2Qa0SpFyjO",
        "outputId": "c378261f-4dd1-49ea-d82e-822f303eeaf9"
      },
      "source": [
        "num_ft_2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "175"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "SUOkkGZjFyjS",
        "outputId": "ee5d7536-8fab-46c3-e9c0-fc5f79eb222e"
      },
      "source": [
        "# Training and evaluate\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_logloss', factor=0.3, patience=5, mode='min', min_lr=1E-5, verbose=1)\n",
        "early_stopping = EarlyStopping(monitor='val_logloss', min_delta=1E-5, patience=15, mode='min',restore_best_weights=True, verbose=1)\n",
        "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=3,epsilon = 1e-4, mode = 'min',verbose=1)\n",
        "# early_stopping = EarlyStopping(monitor='val_loss',min_delta=0,patience=10,mode='auto',verbose=1,baseline=None,restore_best_weights=True)\n",
        "\n",
        "hist = model.fit([X_train[:,:num_ft_1], X_train[:,num_ft_1:]],y_train, batch_size=64, epochs=150,validation_split = 0.2\n",
        "                 ,callbacks=[reduce_lr, early_stopping]\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "275/275 [==============================] - 3s 10ms/step - loss: 0.2116 - logloss: 0.2105 - val_loss: 0.0248 - val_logloss: 0.0236\n",
            "Epoch 2/150\n",
            "275/275 [==============================] - 2s 9ms/step - loss: 0.0216 - logloss: 0.0202 - val_loss: 0.0203 - val_logloss: 0.0188\n",
            "Epoch 3/150\n",
            "275/275 [==============================] - 3s 12ms/step - loss: 0.0191 - logloss: 0.0177 - val_loss: 0.0192 - val_logloss: 0.0177\n",
            "Epoch 4/150\n",
            "275/275 [==============================] - 3s 11ms/step - loss: 0.0180 - logloss: 0.0165 - val_loss: 0.0191 - val_logloss: 0.0176\n",
            "Epoch 5/150\n",
            "275/275 [==============================] - 3s 10ms/step - loss: 0.0171 - logloss: 0.0156 - val_loss: 0.0190 - val_logloss: 0.0174\n",
            "Epoch 6/150\n",
            "275/275 [==============================] - 3s 9ms/step - loss: 0.0162 - logloss: 0.0147 - val_loss: 0.0190 - val_logloss: 0.0173\n",
            "Epoch 7/150\n",
            "275/275 [==============================] - 2s 9ms/step - loss: 0.0152 - logloss: 0.0137 - val_loss: 0.0193 - val_logloss: 0.0176\n",
            "Epoch 8/150\n",
            "275/275 [==============================] - 3s 9ms/step - loss: 0.0142 - logloss: 0.0127 - val_loss: 0.0196 - val_logloss: 0.0176\n",
            "Epoch 9/150\n",
            "275/275 [==============================] - 2s 9ms/step - loss: 0.0130 - logloss: 0.0116 - val_loss: 0.0202 - val_logloss: 0.0180\n",
            "Epoch 10/150\n",
            "275/275 [==============================] - 2s 8ms/step - loss: 0.0117 - logloss: 0.0103 - val_loss: 0.0210 - val_logloss: 0.0186\n",
            "Epoch 11/150\n",
            "272/275 [============================>.] - ETA: 0s - loss: 0.0104 - logloss: 0.0089\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "275/275 [==============================] - 2s 8ms/step - loss: 0.0104 - logloss: 0.0089 - val_loss: 0.0220 - val_logloss: 0.0189\n",
            "Epoch 12/150\n",
            "275/275 [==============================] - 3s 10ms/step - loss: 0.0079 - logloss: 0.0064 - val_loss: 0.0218 - val_logloss: 0.0186\n",
            "Epoch 13/150\n",
            "275/275 [==============================] - 2s 8ms/step - loss: 0.0068 - logloss: 0.0053 - val_loss: 0.0224 - val_logloss: 0.0187\n",
            "Epoch 14/150\n",
            "275/275 [==============================] - 2s 8ms/step - loss: 0.0063 - logloss: 0.0047 - val_loss: 0.0230 - val_logloss: 0.0189\n",
            "Epoch 15/150\n",
            "275/275 [==============================] - 3s 10ms/step - loss: 0.0058 - logloss: 0.0043 - val_loss: 0.0235 - val_logloss: 0.0191\n",
            "Epoch 16/150\n",
            "272/275 [============================>.] - ETA: 0s - loss: 0.0055 - logloss: 0.0039\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "275/275 [==============================] - 3s 9ms/step - loss: 0.0055 - logloss: 0.0039 - val_loss: 0.0239 - val_logloss: 0.0192\n",
            "Epoch 17/150\n",
            "275/275 [==============================] - 2s 9ms/step - loss: 0.0050 - logloss: 0.0034 - val_loss: 0.0240 - val_logloss: 0.0190\n",
            "Epoch 18/150\n",
            "275/275 [==============================] - 2s 9ms/step - loss: 0.0048 - logloss: 0.0032 - val_loss: 0.0242 - val_logloss: 0.0191\n",
            "Epoch 19/150\n",
            "275/275 [==============================] - 2s 8ms/step - loss: 0.0047 - logloss: 0.0031 - val_loss: 0.0245 - val_logloss: 0.0191\n",
            "Epoch 20/150\n",
            "275/275 [==============================] - 2s 8ms/step - loss: 0.0046 - logloss: 0.0030 - val_loss: 0.0246 - val_logloss: 0.0191\n",
            "Epoch 21/150\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.0045 - logloss: 0.0029\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "Restoring model weights from the end of the best epoch.\n",
            "275/275 [==============================] - 3s 10ms/step - loss: 0.0045 - logloss: 0.0029 - val_loss: 0.0248 - val_logloss: 0.0192\n",
            "Epoch 00021: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDu-78_cFyjX"
      },
      "source": [
        "## **Hoàn thành dev model với tên biến model => submit**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "2CU8sM1qFyjY"
      },
      "source": [
        "# predict non ctl vehicle\n",
        "X_pred = pipe.transform( to_pred_non_ctl[cols_fts] )\n",
        "arr_preds_non_ctl = model.predict( [X_pred[:,:num_ft_1], X_pred[:,num_ft_1:]] )\n",
        "#     model.predict(pipe.transform(to_pred_non_ctl[cols_fts]))\n",
        "\n",
        "\n",
        "df_preds_non_ctl =  pd.DataFrame(arr_preds_non_ctl, columns= cols_target, index = to_pred_non_ctl.index)\n",
        "\n",
        "# concat with all to pred values\n",
        "df_preds = pd.concat([ to_pred[cols_id], df_preds_non_ctl], axis = 1).fillna(0)\n",
        "# to csv\n",
        "df_preds.to_csv(\"submission.csv\", index = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "aK8Gz4sdFyje"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}