{"cells":[{"metadata":{},"cell_type":"markdown","source":"## **Using sklearn to preprocessing data and tensorflow for modeling**\n***Preprocessing:***\n* Focus only in feature scaling (normalise) and feature combine (PCA)\n* Building pipeline\n\n***Modeling:***\n* MLP\ninspired by https://www.kaggle.com/riadalmadani/pytorch-cv-0-0145-lb-0-01839#Single-fold-training"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport copy\n\nimport tensorflow as tf\nfrom tensorflow import feature_column as fc\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.layers import Dense, DenseFeatures, Dropout, BatchNormalization, Embedding, Input, Concatenate, Average\nfrom tensorflow.keras.metrics import AUC, Precision, Recall\nfrom tensorflow.keras import backend as K, Sequential, Model\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n\nfrom tensorflow_addons.layers import WeightNormalization\nfrom keras.wrappers.scikit_learn import KerasRegressor\n\nfrom sklearn.preprocessing import QuantileTransformer, StandardScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.decomposition import PCA\n\nimport matplotlib.pyplot as plt\nfrom math import log2\n\nprint(pd.__version__)\nprint(tf.__version__)","execution_count":24,"outputs":[{"output_type":"stream","text":"1.1.3\n2.3.1\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Data loading"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Loading data and encoding\n\nfolder_path = '../input/lish-moa/'\nraw_test = pd.read_csv(folder_path + 'test_features.csv')\nraw_train = pd.read_csv(folder_path + 'train_features.csv')\nraw_targets = pd.read_csv(folder_path + 'train_targets_scored.csv')\n\n# Phân loại dữ liệu\ncols_id = ['sig_id']\ncols_to_remove = ['cp_type']\ncols_fts = [i for i in raw_train.columns if i not in cols_id +cols_to_remove]\ncols_gene = [col for col in raw_train.columns if col.startswith(\"g-\")]\ncols_cell = [col for col in raw_train.columns if col.startswith(\"c-\")]\ncols_experiment = [col for col in cols_fts if col not in cols_gene+cols_cell]\ncols_target = [i for i in raw_targets.columns if i not in cols_id]\nnum_fts, num_labels = len(cols_fts), len(cols_target)\n\n# xử lý categorical\ndef transform_data(input_data):\n    '''Clean data and encoding\n        * input_data: table '''\n    out = input_data.copy()\n    out['cp_dose'] = out['cp_dose'].map({'D1':0, 'D2':1})\n    out['cp_time'] = out['cp_time']/72\n    \n    return out\n\nto_train = transform_data(raw_train[raw_train['cp_type'] != 'ctl_vehicle'])\nto_train_targets = raw_targets.iloc[to_train.index]\nto_pred  = transform_data(raw_test)\nto_pred_non_ctl = to_pred[to_pred['cp_type'] != 'ctl_vehicle']","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"# preprocessing pipeline\ndef pipe_line_builder( quantiles_num, pca_dims):\n    '''Dựng pipe line cho từng nhóm columns\n    :quantiles_num: int: số quantile khi normalise\n    :pca_dims: int: số chiều pca'''\n#     variance = VarianceThreshold(variance_threshold)\n    norm = StandardScaler()\n    #QuantileTransformer(n_quantiles=quantiles_num,random_state=0, output_distribution=\"normal\")\n    pca = PCA(n_components = pca_dims)\n    p_var_norm = Pipeline([ \n#         ('var', variance),\n        ('norm', norm) ])\n    p_var_norm_pca = Pipeline([ \n#         ('var', variance),\n        ('pca', pca),\n        ('norm', norm)\n    ])\n    return FeatureUnion([\n        ('norm', p_var_norm), \n        ('norm_pca', p_var_norm_pca) ])\n\npipe = ColumnTransformer([\n     ('gene', pipe_line_builder(quantiles_num = 100, pca_dims = 640), cols_gene),\n     ('cell', pipe_line_builder(quantiles_num = 100, pca_dims = 72), cols_cell),\n     ('experiment', pipe_line_builder(quantiles_num = 100, pca_dims = int(len(cols_experiment)*0.8)), cols_experiment)\n#      ('all', Pipeline([ \n#                 ('norm', QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")), \n#                 ('pca', PCA(n_components = int(len(cols_fts)*0.8)) ) ]), cols_fts)\n    ])","execution_count":50,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Final data\n# Trick\npipe.fit(to_train[cols_fts].append(to_pred[cols_fts]))\nX_train = pipe.transform(to_train[cols_fts])\ny_train = to_train_targets[cols_target]","execution_count":51,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":52,"outputs":[{"output_type":"execute_result","execution_count":52,"data":{"text/plain":"(21948, 1587)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(cols_gene+cols_cell+cols_experiment)+640+72+int(len(cols_experiment)*0.8)","execution_count":41,"outputs":[{"output_type":"execute_result","execution_count":41,"data":{"text/plain":"1587"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define model\nmodel = Sequential([\n    BatchNormalization(),\n    WeightNormalization(Dense(1024, activation=\"relu\")),\n    BatchNormalization(),\n    Dropout(0.2),\n    WeightNormalization(Dense(512, activation=\"relu\")),\n    BatchNormalization(),\n    Dropout(0.2),\n    WeightNormalization(Dense(256, activation=\"relu\")),\n    BatchNormalization(),\n    Dropout(0.2),\n    WeightNormalization(Dense(num_labels, activation=\"sigmoid\"))\n])\n\np_min = 0.001\np_max = 0.999\ndef logloss(y_true, y_pred):\n    y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n    return -K.mean(y_true*K.log(y_pred) + (1-y_true)*K.log(1-y_pred))\n\nmodel.compile(optimizer='adam', loss=BinaryCrossentropy(label_smoothing=0.0001), metrics=logloss)","execution_count":53,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training and evaluate\nreduce_lr = ReduceLROnPlateau(monitor='val_logloss', factor=0.3, patience=5, mode='min', min_lr=1E-5, verbose=1)\nearly_stopping = EarlyStopping(monitor='val_logloss', min_delta=1E-5, patience=15, mode='min',restore_best_weights=True, verbose=1)\n# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=3,epsilon = 1e-4, mode = 'min',verbose=1)\n# early_stopping = EarlyStopping(monitor='val_loss',min_delta=0,patience=10,mode='auto',verbose=1,baseline=None,restore_best_weights=True)\n\nhist = model.fit(X_train,y_train, batch_size=64, epochs=150,validation_split = 0.2\n                 ,callbacks=[reduce_lr, early_stopping]\n                )","execution_count":54,"outputs":[{"output_type":"stream","text":"Epoch 1/150\n275/275 [==============================] - 3s 10ms/step - loss: 0.2852 - logloss: 0.2845 - val_loss: 0.0354 - val_logloss: 0.0352\nEpoch 2/150\n275/275 [==============================] - 3s 10ms/step - loss: 0.0275 - logloss: 0.0272 - val_loss: 0.0222 - val_logloss: 0.0219\nEpoch 3/150\n275/275 [==============================] - 2s 7ms/step - loss: 0.0216 - logloss: 0.0213 - val_loss: 0.0201 - val_logloss: 0.0199\nEpoch 4/150\n275/275 [==============================] - 2s 9ms/step - loss: 0.0197 - logloss: 0.0194 - val_loss: 0.0189 - val_logloss: 0.0187\nEpoch 5/150\n275/275 [==============================] - 2s 7ms/step - loss: 0.0186 - logloss: 0.0183 - val_loss: 0.0184 - val_logloss: 0.0181\nEpoch 6/150\n275/275 [==============================] - 2s 9ms/step - loss: 0.0177 - logloss: 0.0174 - val_loss: 0.0179 - val_logloss: 0.0177\nEpoch 7/150\n275/275 [==============================] - 2s 9ms/step - loss: 0.0169 - logloss: 0.0167 - val_loss: 0.0179 - val_logloss: 0.0176\nEpoch 8/150\n275/275 [==============================] - 2s 8ms/step - loss: 0.0162 - logloss: 0.0161 - val_loss: 0.0177 - val_logloss: 0.0174\nEpoch 9/150\n275/275 [==============================] - 2s 7ms/step - loss: 0.0155 - logloss: 0.0153 - val_loss: 0.0177 - val_logloss: 0.0175\nEpoch 10/150\n275/275 [==============================] - 2s 7ms/step - loss: 0.0145 - logloss: 0.0144 - val_loss: 0.0178 - val_logloss: 0.0175\nEpoch 11/150\n275/275 [==============================] - 2s 7ms/step - loss: 0.0133 - logloss: 0.0133 - val_loss: 0.0180 - val_logloss: 0.0177\nEpoch 12/150\n275/275 [==============================] - 2s 8ms/step - loss: 0.0120 - logloss: 0.0121 - val_loss: 0.0186 - val_logloss: 0.0181\nEpoch 13/150\n274/275 [============================>.] - ETA: 0s - loss: 0.0106 - logloss: 0.0107\nEpoch 00013: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n275/275 [==============================] - 2s 7ms/step - loss: 0.0106 - logloss: 0.0107 - val_loss: 0.0190 - val_logloss: 0.0183\nEpoch 14/150\n275/275 [==============================] - 2s 7ms/step - loss: 0.0084 - logloss: 0.0085 - val_loss: 0.0190 - val_logloss: 0.0182\nEpoch 15/150\n275/275 [==============================] - 2s 7ms/step - loss: 0.0071 - logloss: 0.0073 - val_loss: 0.0194 - val_logloss: 0.0184\nEpoch 16/150\n275/275 [==============================] - 2s 8ms/step - loss: 0.0063 - logloss: 0.0065 - val_loss: 0.0197 - val_logloss: 0.0185\nEpoch 17/150\n275/275 [==============================] - 3s 10ms/step - loss: 0.0056 - logloss: 0.0058 - val_loss: 0.0202 - val_logloss: 0.0187\nEpoch 18/150\n274/275 [============================>.] - ETA: 0s - loss: 0.0051 - logloss: 0.0053\nEpoch 00018: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n275/275 [==============================] - 2s 7ms/step - loss: 0.0051 - logloss: 0.0053 - val_loss: 0.0206 - val_logloss: 0.0188\nEpoch 19/150\n275/275 [==============================] - 2s 7ms/step - loss: 0.0045 - logloss: 0.0047 - val_loss: 0.0207 - val_logloss: 0.0188\nEpoch 20/150\n275/275 [==============================] - 2s 7ms/step - loss: 0.0042 - logloss: 0.0045 - val_loss: 0.0209 - val_logloss: 0.0188\nEpoch 21/150\n275/275 [==============================] - 2s 7ms/step - loss: 0.0040 - logloss: 0.0042 - val_loss: 0.0210 - val_logloss: 0.0189\nEpoch 22/150\n275/275 [==============================] - 2s 7ms/step - loss: 0.0038 - logloss: 0.0041 - val_loss: 0.0213 - val_logloss: 0.0190\nEpoch 23/150\n269/275 [============================>.] - ETA: 0s - loss: 0.0036 - logloss: 0.0039\nEpoch 00023: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\nRestoring model weights from the end of the best epoch.\n275/275 [==============================] - 2s 8ms/step - loss: 0.0036 - logloss: 0.0039 - val_loss: 0.0215 - val_logloss: 0.0190\nEpoch 00023: early stopping\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test thử resnet\np_min = 0.001\np_max = 0.999\ndef logloss(y_true, y_pred):\n    y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n    return -K.mean(y_true*K.log(y_pred) + (1-y_true)*K.log(1-y_pred))\n\ndef build_model(n_features, n_features_2, n_labels, label_smoothing = 0.0005):    \n    input_1 = Input(shape = (n_features,), name = 'Input1')\n    input_2 = Input(shape = (n_features_2,), name = 'Input2')\n\n    head_1 = Sequential([\n        BatchNormalization(),\n        Dropout(0.2),\n        Dense(512, activation=\"elu\"), \n        BatchNormalization(),\n        Dense(256, activation = \"elu\")\n        ],name='Head1') \n\n    input_3 = head_1(input_1)\n    input_3_concat = Concatenate()([input_2, input_3])\n\n    head_2 = Sequential([\n        BatchNormalization(),\n        Dropout(0.3),\n        Dense(512, \"relu\"),\n        BatchNormalization(),\n        Dense(512, \"elu\"),\n        BatchNormalization(),\n        Dense(256, \"relu\"),\n        BatchNormalization(),\n        Dense(256, \"elu\")\n        ],name='Head2')\n\n    input_4 = head_2(input_3_concat)\n    input_4_avg = Average()([input_3, input_4]) \n\n    head_3 = Sequential([\n        BatchNormalization(),\n        Dense(256, kernel_initializer='lecun_normal', activation='selu'),\n        BatchNormalization(),\n        Dense(n_labels, kernel_initializer='lecun_normal', activation='selu'),\n        BatchNormalization(),\n        Dense(n_labels, activation=\"sigmoid\")\n        ],name='Head3')\n\n    output = head_3(input_4_avg)\n\n\n    model = Model(inputs = [input_1, input_2], outputs = output)\n    model.compile(optimizer='adam', loss= BinaryCrossentropy(label_smoothing=label_smoothing), metrics=logloss)\n    \n    return model\n\nnum_all_ft = X_train.shape[1]\nnum_ft_1 = len(cols_gene)+640\nnum_ft_2 = num_all_ft - num_ft_1\n\nmodel = build_model(num_ft_1, num_ft_2, len(cols_target))","execution_count":57,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_ft_2","execution_count":58,"outputs":[{"output_type":"execute_result","execution_count":58,"data":{"text/plain":"175"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training and evaluate\nreduce_lr = ReduceLROnPlateau(monitor='val_logloss', factor=0.3, patience=5, mode='min', min_lr=1E-5, verbose=1)\nearly_stopping = EarlyStopping(monitor='val_logloss', min_delta=1E-5, patience=15, mode='min',restore_best_weights=True, verbose=1)\n# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=3,epsilon = 1e-4, mode = 'min',verbose=1)\n# early_stopping = EarlyStopping(monitor='val_loss',min_delta=0,patience=10,mode='auto',verbose=1,baseline=None,restore_best_weights=True)\n\nhist = model.fit([X_train[:,:num_ft_1], X_train[:,num_ft_1:]],y_train, batch_size=64, epochs=150,validation_split = 0.2\n                 ,callbacks=[reduce_lr, early_stopping]\n                )","execution_count":59,"outputs":[{"output_type":"stream","text":"Epoch 1/150\n275/275 [==============================] - 3s 10ms/step - loss: 0.2116 - logloss: 0.2105 - val_loss: 0.0248 - val_logloss: 0.0236\nEpoch 2/150\n275/275 [==============================] - 2s 9ms/step - loss: 0.0216 - logloss: 0.0202 - val_loss: 0.0203 - val_logloss: 0.0188\nEpoch 3/150\n275/275 [==============================] - 3s 12ms/step - loss: 0.0191 - logloss: 0.0177 - val_loss: 0.0192 - val_logloss: 0.0177\nEpoch 4/150\n275/275 [==============================] - 3s 11ms/step - loss: 0.0180 - logloss: 0.0165 - val_loss: 0.0191 - val_logloss: 0.0176\nEpoch 5/150\n275/275 [==============================] - 3s 10ms/step - loss: 0.0171 - logloss: 0.0156 - val_loss: 0.0190 - val_logloss: 0.0174\nEpoch 6/150\n275/275 [==============================] - 3s 9ms/step - loss: 0.0162 - logloss: 0.0147 - val_loss: 0.0190 - val_logloss: 0.0173\nEpoch 7/150\n275/275 [==============================] - 2s 9ms/step - loss: 0.0152 - logloss: 0.0137 - val_loss: 0.0193 - val_logloss: 0.0176\nEpoch 8/150\n275/275 [==============================] - 3s 9ms/step - loss: 0.0142 - logloss: 0.0127 - val_loss: 0.0196 - val_logloss: 0.0176\nEpoch 9/150\n275/275 [==============================] - 2s 9ms/step - loss: 0.0130 - logloss: 0.0116 - val_loss: 0.0202 - val_logloss: 0.0180\nEpoch 10/150\n275/275 [==============================] - 2s 8ms/step - loss: 0.0117 - logloss: 0.0103 - val_loss: 0.0210 - val_logloss: 0.0186\nEpoch 11/150\n272/275 [============================>.] - ETA: 0s - loss: 0.0104 - logloss: 0.0089\nEpoch 00011: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n275/275 [==============================] - 2s 8ms/step - loss: 0.0104 - logloss: 0.0089 - val_loss: 0.0220 - val_logloss: 0.0189\nEpoch 12/150\n275/275 [==============================] - 3s 10ms/step - loss: 0.0079 - logloss: 0.0064 - val_loss: 0.0218 - val_logloss: 0.0186\nEpoch 13/150\n275/275 [==============================] - 2s 8ms/step - loss: 0.0068 - logloss: 0.0053 - val_loss: 0.0224 - val_logloss: 0.0187\nEpoch 14/150\n275/275 [==============================] - 2s 8ms/step - loss: 0.0063 - logloss: 0.0047 - val_loss: 0.0230 - val_logloss: 0.0189\nEpoch 15/150\n275/275 [==============================] - 3s 10ms/step - loss: 0.0058 - logloss: 0.0043 - val_loss: 0.0235 - val_logloss: 0.0191\nEpoch 16/150\n272/275 [============================>.] - ETA: 0s - loss: 0.0055 - logloss: 0.0039\nEpoch 00016: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n275/275 [==============================] - 3s 9ms/step - loss: 0.0055 - logloss: 0.0039 - val_loss: 0.0239 - val_logloss: 0.0192\nEpoch 17/150\n275/275 [==============================] - 2s 9ms/step - loss: 0.0050 - logloss: 0.0034 - val_loss: 0.0240 - val_logloss: 0.0190\nEpoch 18/150\n275/275 [==============================] - 2s 9ms/step - loss: 0.0048 - logloss: 0.0032 - val_loss: 0.0242 - val_logloss: 0.0191\nEpoch 19/150\n275/275 [==============================] - 2s 8ms/step - loss: 0.0047 - logloss: 0.0031 - val_loss: 0.0245 - val_logloss: 0.0191\nEpoch 20/150\n275/275 [==============================] - 2s 8ms/step - loss: 0.0046 - logloss: 0.0030 - val_loss: 0.0246 - val_logloss: 0.0191\nEpoch 21/150\n275/275 [==============================] - ETA: 0s - loss: 0.0045 - logloss: 0.0029\nEpoch 00021: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\nRestoring model weights from the end of the best epoch.\n275/275 [==============================] - 3s 10ms/step - loss: 0.0045 - logloss: 0.0029 - val_loss: 0.0248 - val_logloss: 0.0192\nEpoch 00021: early stopping\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## **Hoàn thành dev model với tên biến model => submit**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict non ctl vehicle\nX_pred = pipe.transform( to_pred_non_ctl[cols_fts] )\narr_preds_non_ctl = model.predict( [X_pred[:,:num_ft_1], X_pred[:,num_ft_1:]] )\n#     model.predict(pipe.transform(to_pred_non_ctl[cols_fts]))\n\n\ndf_preds_non_ctl =  pd.DataFrame(arr_preds_non_ctl, columns= cols_target, index = to_pred_non_ctl.index)\n\n# concat with all to pred values\ndf_preds = pd.concat([ to_pred[cols_id], df_preds_non_ctl], axis = 1).fillna(0)\n# to csv\ndf_preds.to_csv(\"submission.csv\", index = None)","execution_count":60,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}