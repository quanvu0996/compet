{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!cp '../input/quanvh8-code/quanvh8_funcs.py' .","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''ENSEMBLE NETS\nInspire by https://www.kaggle.com/c/otto-group-product-classification-challenge/discussion/14335'''\n\nimport numpy as np, pandas as pd, copy, tensorflow as tf, matplotlib.pyplot as plt\n\nfrom tensorflow import feature_column as fc\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.layers import (Dense, DenseFeatures, Dropout, \n                                     BatchNormalization, Embedding, Input, Concatenate, Average,\n                                     InputLayer, Lambda)\nfrom tensorflow.keras.metrics import AUC, Precision, Recall\nfrom tensorflow.keras import backend as K, Sequential, Model\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.optimizers import Adam, RMSprop\n\nimport tensorflow_addons as tfa\nfrom tensorflow_addons.layers import WeightNormalization\n\nfrom sklearn.preprocessing import QuantileTransformer, StandardScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom sklearn.cluster import KMeans\n\nfrom quanvh8_funcs import (DerivedFeatures)#, kfolds_bagging_training, voting_predict,\n#                            kolds_stacked_ensemble_training, stacked_ensemble_predict )\n\nimport sys\n\ndef log_loss_metric(y_true, y_pred):\n    bce = tf.keras.losses.BinaryCrossentropy()\n    return bce(y_true, y_pred).numpy()\n\nprint(pd.__version__)\nprint(tf.__version__)","execution_count":2,"outputs":[{"output_type":"stream","text":"1.1.4\n2.3.1\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Loading data and encoding\n\nfolder_path = '../input/lish-moa/'\nraw_test = pd.read_csv(folder_path + 'test_features.csv')\nraw_train = pd.read_csv(folder_path + 'train_features.csv')\nraw_targets = pd.read_csv(folder_path + 'train_targets_scored.csv')\n\n# Phân loại dữ liệu\ncols_id = ['sig_id']\ncols_to_remove = ['cp_type']\ncols_fts = [i for i in raw_train.columns if i not in cols_id +cols_to_remove]\ncols_gene = [col for col in raw_train.columns if col.startswith(\"g-\")]\ncols_cell = [col for col in raw_train.columns if col.startswith(\"c-\")]\ncols_experiment = [col for col in cols_fts if col not in cols_gene+cols_cell]\ncols_target = [i for i in raw_targets.columns if i not in cols_id]\nnum_fts, num_labels = len(cols_fts), len(cols_target)\n\n# xử lý categorical\ndef transform_data(input_data):\n    '''Clean data and encoding\n        * input_data: table '''\n    out = input_data.copy()\n    out['cp_dose'] = out['cp_dose'].map({'D1':0, 'D2':1})\n    out['cp_time'] = out['cp_time']/72\n    \n    return out\n\nto_train = transform_data(raw_train[raw_train['cp_type'] != 'ctl_vehicle'])\nto_train_targets = raw_targets.iloc[to_train.index]\nfull_pred  = transform_data(raw_test)\nto_pred = full_pred[full_pred['cp_type'] != 'ctl_vehicle']","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# preprocessing pipeline\ndef pipe_line_builder(quantiles_num, pca_dims, kmean_clusters):\n    '''Dựng pipe line cho từng nhóm columns\n    :quantiles_num: int: số quantile khi normalise\n    :pca_dims: int: số chiều pca'''\n    norm = QuantileTransformer(n_quantiles=quantiles_num,random_state=0, output_distribution=\"normal\")\n    pca = PCA(n_components = pca_dims)\n    derived_ft = DerivedFeatures(n_clusters = kmean_clusters)\n\n    p_derived_ft = Pipeline([\n        ('norm', norm), \n        ('derived', derived_ft)])\n\n    p_norm_pca = Pipeline([ \n        ('norm', norm),\n        ('pca', pca) ])\n    return FeatureUnion([\n        ('norm', norm), \n        ('norm_pca', p_norm_pca),\n        ('derived', p_derived_ft)])\n\n# \n\npipe = Pipeline([\n    ('norm_pca', ColumnTransformer([\n                     ('gene', pipe_line_builder(quantiles_num = 200, pca_dims = 600, kmean_clusters = 5), cols_gene),\n                     ('cell', pipe_line_builder(quantiles_num = 200, pca_dims = 50, kmean_clusters = 5), cols_cell),\n                    ]) \n    ), \n    ('var', VarianceThreshold(0.5)) \n])\n\npipe = ColumnTransformer([\n    ('gene_cell', pipe, cols_gene+ cols_cell),\n    ('experiment', 'passthrough', cols_experiment)\n])","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transform data\npipe.fit(to_train[cols_fts].append(to_pred[cols_fts]))\nX_train = pipe.transform(to_train[cols_fts])\nX_pred = pipe.transform(to_pred[cols_fts])\ny_train = to_train_targets[cols_target].values","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyper params\nNFOLDS = 8\nBATCH_SIZE = 128\nEPOCHS = 150\nBAGGING_ALPHA = 0.75\nSEEDS = [23, 228, 1488, 1998, 2208, 2077, 404]\nKFOLDS = 10\nEARLY_STOPPING_STEPS = 10\nWEIGHT_DECAY = 1e-5\nlabel_smoothing_alpha = 0.001\nP_MIN = label_smoothing_alpha\nP_MAX = 1 - P_MIN","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define model\nmodel = Sequential([\n    BatchNormalization(),\n    WeightNormalization(Dense(1024 )),\n    tf.keras.layers.PReLU(),\n\n    BatchNormalization(),\n    Dropout(0.25),\n    WeightNormalization(Dense(1024)),\n    tf.keras.layers.PReLU(),\n    BatchNormalization(),\n    Dropout(0.25),\n    WeightNormalization(Dense(512)),\n    tf.keras.layers.PReLU(),\n    BatchNormalization(),\n    Dropout(0.25),\n    WeightNormalization(Dense(num_labels, activation=\"sigmoid\"))\n])\n\nbce = tf.keras.losses.BinaryCrossentropy() #val_binary_crossentropy\nmodel.compile(optimizer='adam', loss= BinaryCrossentropy(label_smoothing= label_smoothing_alpha)\n              , metrics= [bce])","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduce_lr = ReduceLROnPlateau(monitor='val_binary_crossentropy', factor=0.3, patience=5, mode='min', min_lr=1E-5, verbose=1)\nearly_stopping = EarlyStopping(monitor='val_binary_crossentropy', min_delta=1E-5, patience=15, mode='min',restore_best_weights=True, verbose=1)\n    \nmodel.fit(\n        X_train, y_train, validation_split = 0.3, \n        callbacks=[reduce_lr, early_stopping], epochs=150, verbose =1,\n        batch_size=128)","execution_count":18,"outputs":[{"output_type":"stream","text":"Epoch 1/150\n121/121 [==============================] - 9s 72ms/step - loss: 0.4816 - binary_crossentropy: 0.4784 - val_loss: 0.1107 - val_binary_crossentropy: 0.1107\nEpoch 2/150\n121/121 [==============================] - 8s 67ms/step - loss: 0.0605 - binary_crossentropy: 0.0602 - val_loss: 0.0326 - val_binary_crossentropy: 0.0325\nEpoch 3/150\n121/121 [==============================] - 8s 69ms/step - loss: 0.0289 - binary_crossentropy: 0.0288 - val_loss: 0.0241 - val_binary_crossentropy: 0.0239\nEpoch 4/150\n121/121 [==============================] - 8s 70ms/step - loss: 0.0235 - binary_crossentropy: 0.0233 - val_loss: 0.0217 - val_binary_crossentropy: 0.0215\nEpoch 5/150\n121/121 [==============================] - 8s 68ms/step - loss: 0.0211 - binary_crossentropy: 0.0209 - val_loss: 0.0199 - val_binary_crossentropy: 0.0198\nEpoch 6/150\n121/121 [==============================] - 8s 69ms/step - loss: 0.0198 - binary_crossentropy: 0.0200 - val_loss: 0.0197 - val_binary_crossentropy: 0.0195\nEpoch 7/150\n121/121 [==============================] - 9s 73ms/step - loss: 0.0190 - binary_crossentropy: 0.0190 - val_loss: 0.0188 - val_binary_crossentropy: 0.0186\nEpoch 8/150\n121/121 [==============================] - 8s 67ms/step - loss: 0.0185 - binary_crossentropy: 0.0183 - val_loss: 0.0182 - val_binary_crossentropy: 0.0181\nEpoch 9/150\n121/121 [==============================] - 8s 67ms/step - loss: 0.0179 - binary_crossentropy: 0.0178 - val_loss: 0.0180 - val_binary_crossentropy: 0.0178\nEpoch 10/150\n121/121 [==============================] - 8s 68ms/step - loss: 0.0174 - binary_crossentropy: 0.0173 - val_loss: 0.0178 - val_binary_crossentropy: 0.0176\nEpoch 11/150\n121/121 [==============================] - 9s 75ms/step - loss: 0.0171 - binary_crossentropy: 0.0170 - val_loss: 0.0177 - val_binary_crossentropy: 0.0175\nEpoch 12/150\n121/121 [==============================] - 8s 68ms/step - loss: 0.0166 - binary_crossentropy: 0.0166 - val_loss: 0.0176 - val_binary_crossentropy: 0.0174\nEpoch 13/150\n121/121 [==============================] - 8s 66ms/step - loss: 0.0162 - binary_crossentropy: 0.0162 - val_loss: 0.0175 - val_binary_crossentropy: 0.0173\nEpoch 14/150\n121/121 [==============================] - 8s 67ms/step - loss: 0.0160 - binary_crossentropy: 0.0159 - val_loss: 0.0175 - val_binary_crossentropy: 0.0173\nEpoch 15/150\n121/121 [==============================] - 8s 69ms/step - loss: 0.0156 - binary_crossentropy: 0.0155 - val_loss: 0.0174 - val_binary_crossentropy: 0.0172\nEpoch 16/150\n121/121 [==============================] - 8s 66ms/step - loss: 0.0151 - binary_crossentropy: 0.0149 - val_loss: 0.0175 - val_binary_crossentropy: 0.0172\nEpoch 17/150\n121/121 [==============================] - 8s 66ms/step - loss: 0.0145 - binary_crossentropy: 0.0144 - val_loss: 0.0175 - val_binary_crossentropy: 0.0173\nEpoch 18/150\n120/121 [============================>.] - ETA: 0s - loss: 0.0141 - binary_crossentropy: 0.0139\nEpoch 00018: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n121/121 [==============================] - 8s 68ms/step - loss: 0.0141 - binary_crossentropy: 0.0140 - val_loss: 0.0177 - val_binary_crossentropy: 0.0174\nEpoch 19/150\n121/121 [==============================] - 9s 71ms/step - loss: 0.0130 - binary_crossentropy: 0.0130 - val_loss: 0.0176 - val_binary_crossentropy: 0.0173\nEpoch 20/150\n121/121 [==============================] - 8s 66ms/step - loss: 0.0127 - binary_crossentropy: 0.0128 - val_loss: 0.0176 - val_binary_crossentropy: 0.0174\nEpoch 21/150\n121/121 [==============================] - 8s 66ms/step - loss: 0.0123 - binary_crossentropy: 0.0123 - val_loss: 0.0177 - val_binary_crossentropy: 0.0175\nEpoch 22/150\n121/121 [==============================] - 8s 66ms/step - loss: 0.0121 - binary_crossentropy: 0.0119 - val_loss: 0.0178 - val_binary_crossentropy: 0.0175\nEpoch 23/150\n120/121 [============================>.] - ETA: 0s - loss: 0.0117 - binary_crossentropy: 0.0115\nEpoch 00023: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n121/121 [==============================] - 8s 68ms/step - loss: 0.0117 - binary_crossentropy: 0.0116 - val_loss: 0.0178 - val_binary_crossentropy: 0.0176\nEpoch 24/150\n121/121 [==============================] - 8s 66ms/step - loss: 0.0113 - binary_crossentropy: 0.0111 - val_loss: 0.0178 - val_binary_crossentropy: 0.0176\nEpoch 25/150\n121/121 [==============================] - 8s 66ms/step - loss: 0.0111 - binary_crossentropy: 0.0110 - val_loss: 0.0179 - val_binary_crossentropy: 0.0176\nEpoch 26/150\n121/121 [==============================] - 9s 71ms/step - loss: 0.0109 - binary_crossentropy: 0.0109 - val_loss: 0.0179 - val_binary_crossentropy: 0.0177\nEpoch 27/150\n121/121 [==============================] - 8s 68ms/step - loss: 0.0109 - binary_crossentropy: 0.0108 - val_loss: 0.0179 - val_binary_crossentropy: 0.0177\nEpoch 28/150\n120/121 [============================>.] - ETA: 0s - loss: 0.0107 - binary_crossentropy: 0.0106\nEpoch 00028: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n121/121 [==============================] - 8s 66ms/step - loss: 0.0107 - binary_crossentropy: 0.0106 - val_loss: 0.0180 - val_binary_crossentropy: 0.0178\nEpoch 29/150\n121/121 [==============================] - 8s 65ms/step - loss: 0.0105 - binary_crossentropy: 0.0104 - val_loss: 0.0180 - val_binary_crossentropy: 0.0178\nEpoch 30/150\n120/121 [============================>.] - ETA: 0s - loss: 0.0105 - binary_crossentropy: 0.0103Restoring model weights from the end of the best epoch.\n121/121 [==============================] - 8s 67ms/step - loss: 0.0105 - binary_crossentropy: 0.0105 - val_loss: 0.0180 - val_binary_crossentropy: 0.0178\nEpoch 00030: early stopping\n","name":"stdout"},{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f27f4864110>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Un-Use code"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Define model\n# model = Sequential([\n#     # BatchNormalization(),\n#     WeightNormalization(Dense(1024, activation=\"selu\", kernel_regularizer= tf.keras.regularizers.l2(0.0005))),\n\n#     # BatchNormalization(),\n#     Dropout(0.25),\n#     WeightNormalization(Dense(1024, activation=\"selu\", kernel_regularizer= tf.keras.regularizers.l2(0.0005))),\n#     # BatchNormalization(),\n#     Dropout(0.25),\n#     WeightNormalization(Dense(512, activation=\"selu\", kernel_regularizer= tf.keras.regularizers.l2(0.0005))),\n#     # BatchNormalization(),\n#     Dropout(0.25),\n#     WeightNormalization(Dense(num_labels, activation=\"sigmoid\"))\n# ])\n\n# bce = tf.keras.losses.BinaryCrossentropy() #val_binary_crossentropy\n# model.compile(optimizer='adam', loss= BinaryCrossentropy(label_smoothing= label_smoothing_alpha)\n#               , metrics= [bce])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}