{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "bagging-nets.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqE2TFuV1dae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26d332f1-21f8-4270-b040-f78d2b8423d1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFr0jM5kG_EX"
      },
      "source": [
        "!cp '/content/drive/MyDrive/Data/colabs_data/MOA_kaggle/quanvh8_funcs.py' ."
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_execution_state": "idle",
        "_uuid": "051d70d956493feee0c6d64651c6a088724dca2a",
        "id": "5FTCve_w1XEr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1b448e2-a885-4b3b-baae-496af2613e37"
      },
      "source": [
        "'''ENSEMBLE NETS\n",
        "Inspire by https://www.kaggle.com/c/otto-group-product-classification-challenge/discussion/14335'''\n",
        "\n",
        "import numpy as np, pandas as pd, copy, tensorflow as tf, matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow import feature_column as fc\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.layers import (Dense, DenseFeatures, Dropout, \n",
        "                                     BatchNormalization, Embedding, Input, Concatenate, Average,\n",
        "                                     InputLayer, Lambda)\n",
        "from tensorflow.keras.metrics import AUC, Precision, Recall\n",
        "from tensorflow.keras import backend as K, Sequential, Model\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow_addons.layers import WeightNormalization\n",
        "\n",
        "from sklearn.preprocessing import QuantileTransformer, StandardScaler\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from quanvh8_funcs import (DerivedFeatures, kfolds_bagging_training, voting_predict,\n",
        "                           kolds_stacked_ensemble_training, stacked_ensemble_predict )\n",
        "\n",
        "import sys\n",
        "\n",
        "def log_loss_metric(y_true, y_pred):\n",
        "    bce = tf.keras.losses.BinaryCrossentropy()\n",
        "    return bce(y_true, y_pred).numpy()\n",
        "\n",
        "print(pd.__version__)\n",
        "print(tf.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.1.4\n",
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLOzSk_h1XEz"
      },
      "source": [
        "# MODULE 1. DATA LOADING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqUvs-3Y1XEz"
      },
      "source": [
        "# Loading data and encoding\n",
        "\n",
        "folder_path = '/content/drive/My Drive/Data/colabs_data/MOA_kaggle/'\n",
        "raw_test = pd.read_csv(folder_path + 'test_features.csv')\n",
        "raw_train = pd.read_csv(folder_path + 'train_features.csv')\n",
        "raw_targets = pd.read_csv(folder_path + 'train_targets_scored.csv')\n",
        "\n",
        "# Phân loại dữ liệu\n",
        "cols_id = ['sig_id']\n",
        "cols_to_remove = ['cp_type']\n",
        "cols_fts = [i for i in raw_train.columns if i not in cols_id +cols_to_remove]\n",
        "cols_gene = [col for col in raw_train.columns if col.startswith(\"g-\")]\n",
        "cols_cell = [col for col in raw_train.columns if col.startswith(\"c-\")]\n",
        "cols_experiment = [col for col in cols_fts if col not in cols_gene+cols_cell]\n",
        "cols_target = [i for i in raw_targets.columns if i not in cols_id]\n",
        "num_fts, num_labels = len(cols_fts), len(cols_target)\n",
        "\n",
        "# xử lý categorical\n",
        "def transform_data(input_data):\n",
        "    '''Clean data and encoding\n",
        "        * input_data: table '''\n",
        "    out = input_data.copy()\n",
        "    out['cp_dose'] = out['cp_dose'].map({'D1':0, 'D2':1})\n",
        "    out['cp_time'] = out['cp_time']/72\n",
        "    \n",
        "    return out\n",
        "\n",
        "to_train = transform_data(raw_train[raw_train['cp_type'] != 'ctl_vehicle'])\n",
        "to_train_targets = raw_targets.iloc[to_train.index]\n",
        "full_pred  = transform_data(raw_test)\n",
        "to_pred = full_pred[full_pred['cp_type'] != 'ctl_vehicle']"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXhrnEA51XE2"
      },
      "source": [
        "# MODULE 2. DATA TRANSFORMATION PIPELINE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-cBlnqh1XE6"
      },
      "source": [
        "# preprocessing pipeline\n",
        "def pipe_line_builder(quantiles_num, pca_dims, kmean_clusters):\n",
        "    '''Dựng pipe line cho từng nhóm columns\n",
        "    :quantiles_num: int: số quantile khi normalise\n",
        "    :pca_dims: int: số chiều pca'''\n",
        "    norm = QuantileTransformer(n_quantiles=quantiles_num,random_state=0, output_distribution=\"normal\")\n",
        "    pca = PCA(n_components = pca_dims)\n",
        "    derived_ft = DerivedFeatures(n_clusters = kmean_clusters)\n",
        "\n",
        "    p_derived_ft = Pipeline([\n",
        "        ('norm', norm), \n",
        "        ('derived', derived_ft)])\n",
        "\n",
        "    p_norm_pca = Pipeline([ \n",
        "        ('norm', norm),\n",
        "        ('pca', pca) ])\n",
        "    return FeatureUnion([\n",
        "        ('norm', norm), \n",
        "        ('norm_pca', p_norm_pca),\n",
        "        ('derived', p_derived_ft)])\n",
        "\n",
        "# \n",
        "\n",
        "pipe = Pipeline([\n",
        "    ('norm_pca', ColumnTransformer([\n",
        "                     ('gene', pipe_line_builder(quantiles_num = 200, pca_dims = 600, kmean_clusters = 5), cols_gene),\n",
        "                     ('cell', pipe_line_builder(quantiles_num = 200, pca_dims = 50, kmean_clusters = 5), cols_cell),\n",
        "                    ]) \n",
        "    ), \n",
        "    ('var', VarianceThreshold(0.5)) \n",
        "])\n",
        "\n",
        "pipe = ColumnTransformer([\n",
        "    ('gene_cell', pipe, cols_gene+ cols_cell),\n",
        "    ('experiment', 'passthrough', cols_experiment)\n",
        "])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdZH_psq1XE8"
      },
      "source": [
        "# Transform data\n",
        "pipe.fit(to_train[cols_fts].append(to_pred[cols_fts]))\n",
        "X_train = pipe.transform(to_train[cols_fts])\n",
        "X_pred = pipe.transform(to_pred[cols_fts])\n",
        "y_train = to_train_targets[cols_target].values"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOLUCHyy1XFA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51a0dfe6-6a6b-4ec5-d2ce-2f70d76eeedb"
      },
      "source": [
        "print(X_train.shape, X_pred.shape, y_train.shape)\n",
        "print(type(X_train), type(X_pred), type(y_train))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(21948, 1240) (3624, 1240) (21948, 206)\n",
            "<class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQpPUfYf1XFC"
      },
      "source": [
        "# MODULE 3: BAGGING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWpqTIh-1XFQ"
      },
      "source": [
        "# Hyper params\n",
        "NFOLDS = 8\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 150\n",
        "BAGGING_ALPHA = 0.75\n",
        "SEEDS = [23, 228, 1488, 1998, 2208, 2077, 404]\n",
        "KFOLDS = 10\n",
        "label_smoothing_alpha = 0.00005\n",
        "P_MIN = label_smoothing_alpha\n",
        "P_MAX = 1 - P_MIN"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvVe5RdK1XFS",
        "outputId": "1adb38e1-53f0-444c-839a-d2a5acf5505b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        }
      },
      "source": [
        "# Define model\n",
        "model = Sequential([\n",
        "    # BatchNormalization(),\n",
        "    WeightNormalization(Dense(1024, activation=\"selu\", kernel_regularizer= tf.keras.regularizers.l2(0.0005))),\n",
        "\n",
        "    # BatchNormalization(),\n",
        "    Dropout(0.25),\n",
        "    WeightNormalization(Dense(1024, activation=\"selu\", kernel_regularizer= tf.keras.regularizers.l2(0.0005))),\n",
        "    # BatchNormalization(),\n",
        "    Dropout(0.25),\n",
        "    WeightNormalization(Dense(512, activation=\"selu\", kernel_regularizer= tf.keras.regularizers.l2(0.0005))),\n",
        "    # BatchNormalization(),\n",
        "    Dropout(0.25),\n",
        "    WeightNormalization(Dense(num_labels, activation=\"sigmoid\"))\n",
        "])\n",
        "\n",
        "bce = tf.keras.losses.BinaryCrossentropy() #val_binary_crossentropy\n",
        "model.compile(optimizer='adam', loss= BinaryCrossentropy(label_smoothing= label_smoothing_alpha)\n",
        "              , metrics= [bce])"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-7364aab6cd22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m model = Sequential([\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# BatchNormalization(),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mWeightNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gelu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregularizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# BatchNormalization(),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_initializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_initializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/activations.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(identifier)\u001b[0m\n\u001b[1;32m    527\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0midentifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/activations.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(name, custom_objects)\u001b[0m\n\u001b[1;32m    490\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m       \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m       printable_module_name='activation function')\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    376\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         raise ValueError(\n\u001b[0;32m--> 378\u001b[0;31m             'Unknown ' + printable_module_name + ': ' + object_name)\n\u001b[0m\u001b[1;32m    379\u001b[0m     \u001b[0;31m# Classes passed by name are instantiated with no args, functions are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# returned as-is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unknown activation function: gelu"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZAqvmmP1XFV"
      },
      "source": [
        "# n_components = 256\n",
        "\n",
        "# input_u = Input(shape= ( X_train.shape[1], ))\n",
        "# layer_u = BatchNormalization() (input_u)\n",
        "# layer_u = WeightNormalization(Dense(1024, activation=\"relu\")) (layer_u)\n",
        "# layer_u = BatchNormalization() (layer_u)\n",
        "# layer_u = Dropout(0.25) (layer_u)\n",
        "# layer_u = WeightNormalization(Dense(1024, activation=\"relu\")) (layer_u)\n",
        "# layer_u = BatchNormalization() (layer_u)\n",
        "# layer_u = Dropout(0.25) (layer_u)\n",
        "\n",
        "# layer_u = WeightNormalization(Dense(n_components, activation=\"relu\")) (layer_u)\n",
        "# layer_u = BatchNormalization() (layer_u)\n",
        "\n",
        "# #2.1. Addition information for item_info\n",
        "# chemical_category = tf.transpose(\n",
        "#         tf.constant(\n",
        "#             [[1 if '_inhibitor' in i else 0 for i in cols_target],\n",
        "#                [1 if '_agonist' in i else 0 for i in cols_target],\n",
        "#                [1 if '_agent' in i else 0 for i in cols_target],\n",
        "#                [1 if '_antagonist' in i else 0 for i in cols_target],\n",
        "#                [1 if '_blocker' in i else 0 for i in cols_target],\n",
        "#                [1 if '_activator' in i else 0 for i in cols_target] \n",
        "#              ]))\n",
        "\n",
        "# #2.2 Full item fts: addition + onehot\n",
        "# item_ft = tf.concat(\n",
        "#     [chemical_category ,\n",
        "#      tf.eye(num_labels, dtype = tf.int32) # Create tensor 0-1 coresponse with chemical labels\n",
        "#     ], axis = 1\n",
        "# )\n",
        "# layer_i = Dense(n_components, activation = 'relu', kernel_initializer='he_normal', name ='layer_u1') (item_ft)\n",
        "# layer_i = BatchNormalization() (layer_i)\n",
        "# #3. Dot product user - item\n",
        "# def dot_2layer(x):\n",
        "#     return K.dot( x[0], K.transpose(x[1]))\n",
        "# dot_ui = Lambda( dot_2layer, name = 'lambda_dot' ) ([layer_u,layer_i])\n",
        "# dot_ui = WeightNormalization(Dense(num_labels, activation = 'sigmoid', kernel_initializer='he_normal', name = 'labels'))(dot_ui)\n",
        "\n",
        "# model = Model(inputs=[ input_u, ], outputs= [dot_ui])\n",
        "\n",
        "# bce = tf.keras.losses.BinaryCrossentropy() #val_binary_crossentropy\n",
        "# model.compile(optimizer='adam', loss= BinaryCrossentropy(label_smoothing= label_smoothing_alpha)\n",
        "#               , metrics= [bce])"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0XZWdvS1XFY",
        "outputId": "7eb8951e-0b05-404a-e15b-4dfc3137a2a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "reduce_lr = ReduceLROnPlateau(monitor='val_binary_crossentropy', factor=0.3, patience=5, mode='min', min_lr=1E-5, verbose=1)\n",
        "early_stopping = EarlyStopping(monitor='val_binary_crossentropy', min_delta=1E-5, patience=15, mode='min',restore_best_weights=True, verbose=1)\n",
        "    \n",
        "model.fit(\n",
        "        X_train, y_train, validation_split = 0.3, \n",
        "        callbacks=[reduce_lr, early_stopping], epochs=150, verbose =1,\n",
        "        batch_size=128)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "121/121 [==============================] - 1s 11ms/step - loss: 0.0134 - binary_crossentropy: 0.0134 - val_loss: 0.0181 - val_binary_crossentropy: 0.0178\n",
            "Epoch 2/150\n",
            "121/121 [==============================] - 1s 10ms/step - loss: 0.0133 - binary_crossentropy: 0.0133 - val_loss: 0.0181 - val_binary_crossentropy: 0.0178\n",
            "Epoch 3/150\n",
            "121/121 [==============================] - 1s 10ms/step - loss: 0.0133 - binary_crossentropy: 0.0132 - val_loss: 0.0181 - val_binary_crossentropy: 0.0178\n",
            "Epoch 4/150\n",
            "121/121 [==============================] - 1s 10ms/step - loss: 0.0133 - binary_crossentropy: 0.0132 - val_loss: 0.0181 - val_binary_crossentropy: 0.0179\n",
            "Epoch 5/150\n",
            "121/121 [==============================] - 1s 10ms/step - loss: 0.0133 - binary_crossentropy: 0.0131 - val_loss: 0.0181 - val_binary_crossentropy: 0.0179\n",
            "Epoch 6/150\n",
            "117/121 [============================>.] - ETA: 0s - loss: 0.0133 - binary_crossentropy: 0.0130\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "121/121 [==============================] - 1s 10ms/step - loss: 0.0133 - binary_crossentropy: 0.0132 - val_loss: 0.0181 - val_binary_crossentropy: 0.0179\n",
            "Epoch 7/150\n",
            "121/121 [==============================] - 1s 10ms/step - loss: 0.0132 - binary_crossentropy: 0.0131 - val_loss: 0.0181 - val_binary_crossentropy: 0.0179\n",
            "Epoch 8/150\n",
            "121/121 [==============================] - 1s 10ms/step - loss: 0.0132 - binary_crossentropy: 0.0130 - val_loss: 0.0181 - val_binary_crossentropy: 0.0179\n",
            "Epoch 9/150\n",
            "121/121 [==============================] - 1s 10ms/step - loss: 0.0132 - binary_crossentropy: 0.0132 - val_loss: 0.0181 - val_binary_crossentropy: 0.0179\n",
            "Epoch 10/150\n",
            "121/121 [==============================] - 1s 10ms/step - loss: 0.0132 - binary_crossentropy: 0.0130 - val_loss: 0.0181 - val_binary_crossentropy: 0.0179\n",
            "Epoch 11/150\n",
            "121/121 [==============================] - 1s 10ms/step - loss: 0.0132 - binary_crossentropy: 0.0131 - val_loss: 0.0181 - val_binary_crossentropy: 0.0179\n",
            "Epoch 12/150\n",
            "121/121 [==============================] - 1s 10ms/step - loss: 0.0132 - binary_crossentropy: 0.0131 - val_loss: 0.0181 - val_binary_crossentropy: 0.0179\n",
            "Epoch 13/150\n",
            "121/121 [==============================] - 1s 10ms/step - loss: 0.0132 - binary_crossentropy: 0.0131 - val_loss: 0.0181 - val_binary_crossentropy: 0.0179\n",
            "Epoch 14/150\n",
            "121/121 [==============================] - 1s 10ms/step - loss: 0.0132 - binary_crossentropy: 0.0130 - val_loss: 0.0181 - val_binary_crossentropy: 0.0179\n",
            "Epoch 15/150\n",
            "121/121 [==============================] - 1s 10ms/step - loss: 0.0132 - binary_crossentropy: 0.0131 - val_loss: 0.0181 - val_binary_crossentropy: 0.0179\n",
            "Epoch 16/150\n",
            "119/121 [============================>.] - ETA: 0s - loss: 0.0131 - binary_crossentropy: 0.0129Restoring model weights from the end of the best epoch.\n",
            "121/121 [==============================] - 1s 10ms/step - loss: 0.0132 - binary_crossentropy: 0.0130 - val_loss: 0.0182 - val_binary_crossentropy: 0.0179\n",
            "Epoch 00016: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff68f6ab080>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxXFH-Hg1XFa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd2db284-6100-4ebd-fda0-63b83491d593"
      },
      "source": [
        "# model_2list = kfolds_training(NFOLDS, model, SEEDS, X_train, y_train, bagging_alpha = 0.8, bagging_samples = 15)\n",
        "model_2list = kfolds_boosting_training(NFOLDS, model, SEEDS, X_train, y_train, n_interations = 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training at fold:  0 ####################################################################################################\n",
            "Epoch 1/150\n",
            "151/151 [==============================] - 2s 15ms/step - loss: 0.0483 - mse: 0.0483 - val_loss: 0.0418 - val_mse: 0.0418\n",
            "Epoch 2/150\n",
            "151/151 [==============================] - 1s 10ms/step - loss: 0.0231 - mse: 0.0231 - val_loss: 0.0421 - val_mse: 0.0421\n",
            "Epoch 3/150\n",
            "151/151 [==============================] - 1s 10ms/step - loss: 0.0167 - mse: 0.0167 - val_loss: 0.0506 - val_mse: 0.0506\n",
            "Epoch 4/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0135 - mse: 0.0135 - val_loss: 0.0364 - val_mse: 0.0364\n",
            "Epoch 5/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0346 - val_mse: 0.0346\n",
            "Epoch 6/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0211 - val_mse: 0.0211\n",
            "Epoch 7/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0146 - val_mse: 0.0146\n",
            "Epoch 8/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0087 - val_mse: 0.0087\n",
            "Epoch 9/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0082 - mse: 0.0082 - val_loss: 0.0062 - val_mse: 0.0062\n",
            "Epoch 10/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0067 - val_mse: 0.0067\n",
            "Epoch 11/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.0050 - val_mse: 0.0050\n",
            "Epoch 12/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0055 - mse: 0.0055 - val_loss: 0.0041 - val_mse: 0.0041\n",
            "Epoch 13/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0040 - val_mse: 0.0040\n",
            "Epoch 14/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0037 - val_mse: 0.0037\n",
            "Epoch 15/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0035 - val_mse: 0.0035\n",
            "Epoch 16/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0037 - val_mse: 0.0037\n",
            "Epoch 17/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 18/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 19/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 20/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 21/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 22/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 23/150\n",
            "151/151 [==============================] - 1s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0035 - val_mse: 0.0035\n",
            "Epoch 24/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0035 - val_mse: 0.0035\n",
            "Epoch 25/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 26/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 27/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 28/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 29/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 30/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 31/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 32/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 33/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 34/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 35/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 36/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 37/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 38/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 39/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 40/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 41/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Loss_at boosting round  0 :  0.021026582\n",
            "Epoch 1/150\n",
            "151/151 [==============================] - 2s 15ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 2/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 3/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 4/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 5/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0035 - val_mse: 0.0035\n",
            "Epoch 6/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0035 - val_mse: 0.0035\n",
            "Epoch 7/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0036 - val_mse: 0.0036\n",
            "Epoch 8/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0036 - val_mse: 0.0036\n",
            "Epoch 9/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0036 - val_mse: 0.0036\n",
            "Epoch 10/150\n",
            "151/151 [==============================] - 1s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0037 - val_mse: 0.0037\n",
            "Epoch 11/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0037 - val_mse: 0.0037\n",
            "Epoch 12/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0037 - val_mse: 0.0037\n",
            "Epoch 13/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0037 - val_mse: 0.0037\n",
            "Epoch 14/150\n",
            "151/151 [==============================] - 1s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0037 - val_mse: 0.0037\n",
            "Epoch 15/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0037 - val_mse: 0.0037\n",
            "Epoch 16/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0037 - val_mse: 0.0037\n",
            "Loss_at boosting round  1 :  0.021182016\n",
            "Epoch 1/150\n",
            "151/151 [==============================] - 2s 15ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 2/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0037 - val_mse: 0.0037\n",
            "Epoch 3/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0038 - val_mse: 0.0038\n",
            "Epoch 4/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0039 - val_mse: 0.0039\n",
            "Epoch 5/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0040 - val_mse: 0.0040\n",
            "Epoch 6/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0041 - val_mse: 0.0041\n",
            "Epoch 7/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0042 - val_mse: 0.0042\n",
            "Epoch 8/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0042 - val_mse: 0.0042\n",
            "Epoch 9/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0043 - val_mse: 0.0043\n",
            "Epoch 10/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0043 - val_mse: 0.0043\n",
            "Epoch 11/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0043 - val_mse: 0.0043\n",
            "Epoch 12/150\n",
            "151/151 [==============================] - 1s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0043 - val_mse: 0.0043\n",
            "Epoch 13/150\n",
            "151/151 [==============================] - 1s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0044 - val_mse: 0.0044\n",
            "Epoch 14/150\n",
            "151/151 [==============================] - 1s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0044 - val_mse: 0.0044\n",
            "Epoch 15/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0044 - val_mse: 0.0044\n",
            "Epoch 16/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0044 - val_mse: 0.0044\n",
            "Loss_at boosting round  2 :  0.022715405\n",
            "Epoch 1/150\n",
            "151/151 [==============================] - 2s 15ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0038 - val_mse: 0.0038\n",
            "Epoch 2/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0039 - val_mse: 0.0039\n",
            "Epoch 3/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0041 - val_mse: 0.0041\n",
            "Epoch 4/150\n",
            "151/151 [==============================] - 1s 10ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0042 - val_mse: 0.0042\n",
            "Epoch 5/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0044 - val_mse: 0.0044\n",
            "Epoch 6/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0044 - val_mse: 0.0044\n",
            "Epoch 7/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0045 - val_mse: 0.0045\n",
            "Epoch 8/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0046 - val_mse: 0.0046\n",
            "Epoch 9/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0046 - val_mse: 0.0046\n",
            "Epoch 10/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0047 - val_mse: 0.0047\n",
            "Epoch 11/150\n",
            "151/151 [==============================] - 1s 10ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0047 - val_mse: 0.0047\n",
            "Epoch 12/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0047 - val_mse: 0.0047\n",
            "Epoch 13/150\n",
            "151/151 [==============================] - 1s 10ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0047 - val_mse: 0.0047\n",
            "Epoch 14/150\n",
            "151/151 [==============================] - 1s 10ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0048 - val_mse: 0.0048\n",
            "Epoch 15/150\n",
            "151/151 [==============================] - 1s 10ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0048 - val_mse: 0.0048\n",
            "Epoch 16/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0048 - val_mse: 0.0048\n",
            "Loss_at boosting round  3 :  0.03212597\n",
            "Epoch 1/150\n",
            "151/151 [==============================] - 2s 15ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0039 - val_mse: 0.0039\n",
            "Epoch 2/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0047 - mse: 0.0047 - val_loss: 0.0040 - val_mse: 0.0040\n",
            "Epoch 3/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0041 - val_mse: 0.0041\n",
            "Epoch 4/150\n",
            "151/151 [==============================] - 1s 10ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0042 - val_mse: 0.0042\n",
            "Epoch 5/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0043 - val_mse: 0.0043\n",
            "Epoch 6/150\n",
            "151/151 [==============================] - 1s 10ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0043 - val_mse: 0.0043\n",
            "Epoch 7/150\n",
            "151/151 [==============================] - 1s 10ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0044 - val_mse: 0.0044\n",
            "Epoch 8/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0044 - val_mse: 0.0044\n",
            "Epoch 9/150\n",
            "151/151 [==============================] - 1s 10ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0045 - val_mse: 0.0045\n",
            "Epoch 10/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0045 - val_mse: 0.0045\n",
            "Epoch 11/150\n",
            "151/151 [==============================] - 1s 10ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0046 - val_mse: 0.0046\n",
            "Epoch 12/150\n",
            "151/151 [==============================] - 1s 10ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0046 - val_mse: 0.0046\n",
            "Epoch 13/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0046 - val_mse: 0.0046\n",
            "Epoch 14/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0046 - val_mse: 0.0046\n",
            "Epoch 15/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0047 - val_mse: 0.0047\n",
            "Epoch 16/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0047 - val_mse: 0.0047\n",
            "Loss_at boosting round  4 :  0.040049426\n",
            "Logloss:  0.04025174\n",
            "Training at fold:  1 ####################################################################################################\n",
            "Epoch 1/150\n",
            "151/151 [==============================] - 2s 15ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0038 - val_mse: 0.0038\n",
            "Epoch 2/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0037 - val_mse: 0.0037\n",
            "Epoch 3/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0036 - val_mse: 0.0036\n",
            "Epoch 4/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0035 - val_mse: 0.0035\n",
            "Epoch 5/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0035 - val_mse: 0.0035\n",
            "Epoch 6/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0035 - val_mse: 0.0035\n",
            "Epoch 7/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0035 - val_mse: 0.0035\n",
            "Epoch 8/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0035 - val_mse: 0.0035\n",
            "Epoch 9/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0035 - val_mse: 0.0035\n",
            "Epoch 10/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0035 - val_mse: 0.0035\n",
            "Epoch 11/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0035 - val_mse: 0.0035\n",
            "Epoch 12/150\n",
            "151/151 [==============================] - 2s 11ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0035 - val_mse: 0.0035\n",
            "Epoch 13/150\n",
            "151/151 [==============================] - 2s 11ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0035 - val_mse: 0.0035\n",
            "Epoch 14/150\n",
            "151/151 [==============================] - 2s 11ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0035 - val_mse: 0.0035\n",
            "Epoch 15/150\n",
            "151/151 [==============================] - 2s 11ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0035 - val_mse: 0.0035\n",
            "Epoch 16/150\n",
            "151/151 [==============================] - 2s 11ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0035 - val_mse: 0.0035\n",
            "Epoch 17/150\n",
            "151/151 [==============================] - 2s 11ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0035 - val_mse: 0.0035\n",
            "Epoch 18/150\n",
            "151/151 [==============================] - 2s 11ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0035 - val_mse: 0.0035\n",
            "Epoch 19/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0035 - val_mse: 0.0035\n",
            "Epoch 20/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0035 - val_mse: 0.0035\n",
            "Epoch 21/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0035 - val_mse: 0.0035\n",
            "Epoch 22/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0035 - val_mse: 0.0035\n",
            "Epoch 23/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0035 - val_mse: 0.0035\n",
            "Epoch 24/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0035 - val_mse: 0.0035\n",
            "Epoch 25/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0035 - val_mse: 0.0035\n",
            "Loss_at boosting round  0 :  0.021626173\n",
            "Epoch 1/150\n",
            "151/151 [==============================] - 2s 15ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0035 - val_mse: 0.0035\n",
            "Epoch 2/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0036 - val_mse: 0.0036\n",
            "Epoch 3/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0036 - val_mse: 0.0036\n",
            "Epoch 4/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0036 - val_mse: 0.0036\n",
            "Epoch 5/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0036 - val_mse: 0.0036\n",
            "Epoch 6/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0036 - val_mse: 0.0036\n",
            "Epoch 7/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0036 - val_mse: 0.0036\n",
            "Epoch 8/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0037 - val_mse: 0.0037\n",
            "Epoch 9/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0037 - val_mse: 0.0037\n",
            "Epoch 10/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0037 - val_mse: 0.0037\n",
            "Epoch 11/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0037 - val_mse: 0.0037\n",
            "Epoch 12/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0037 - val_mse: 0.0037\n",
            "Epoch 13/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0037 - val_mse: 0.0037\n",
            "Epoch 14/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0037 - val_mse: 0.0037\n",
            "Epoch 15/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0037 - val_mse: 0.0037\n",
            "Epoch 16/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0037 - val_mse: 0.0037\n",
            "Loss_at boosting round  1 :  0.022205371\n",
            "Epoch 1/150\n",
            "151/151 [==============================] - 2s 15ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0037 - val_mse: 0.0037\n",
            "Epoch 2/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0038 - val_mse: 0.0038\n",
            "Epoch 3/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0039 - val_mse: 0.0039\n",
            "Epoch 4/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0040 - val_mse: 0.0040\n",
            "Epoch 5/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0042 - val_mse: 0.0042\n",
            "Epoch 6/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0042 - val_mse: 0.0042\n",
            "Epoch 7/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0043 - val_mse: 0.0043\n",
            "Epoch 8/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0043 - val_mse: 0.0043\n",
            "Epoch 9/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0043 - val_mse: 0.0043\n",
            "Epoch 10/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0043 - val_mse: 0.0043\n",
            "Epoch 11/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0043 - val_mse: 0.0043\n",
            "Epoch 12/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0043 - val_mse: 0.0043\n",
            "Epoch 13/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0043 - val_mse: 0.0043\n",
            "Epoch 14/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0043 - val_mse: 0.0043\n",
            "Epoch 15/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0043 - val_mse: 0.0043\n",
            "Epoch 16/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0043 - val_mse: 0.0043\n",
            "Loss_at boosting round  2 :  0.024614735\n",
            "Epoch 1/150\n",
            "151/151 [==============================] - 2s 15ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0038 - val_mse: 0.0038\n",
            "Epoch 2/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0039 - val_mse: 0.0039\n",
            "Epoch 3/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0042 - val_mse: 0.0042\n",
            "Epoch 4/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0043 - val_mse: 0.0043\n",
            "Epoch 5/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0043 - val_mse: 0.0043\n",
            "Epoch 6/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0044 - val_mse: 0.0044\n",
            "Epoch 7/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0044 - val_mse: 0.0044\n",
            "Epoch 8/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0045 - val_mse: 0.0045\n",
            "Epoch 9/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0045 - val_mse: 0.0045\n",
            "Epoch 10/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0045 - val_mse: 0.0045\n",
            "Epoch 11/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0045 - val_mse: 0.0045\n",
            "Epoch 12/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0045 - val_mse: 0.0045\n",
            "Epoch 13/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0045 - val_mse: 0.0045\n",
            "Epoch 14/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0046 - val_mse: 0.0046\n",
            "Epoch 15/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0046 - val_mse: 0.0046\n",
            "Epoch 16/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0046 - val_mse: 0.0046\n",
            "Loss_at boosting round  3 :  0.032498855\n",
            "Epoch 1/150\n",
            "151/151 [==============================] - 2s 15ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0039 - val_mse: 0.0039\n",
            "Epoch 2/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0041 - val_mse: 0.0041\n",
            "Epoch 3/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0042 - val_mse: 0.0042\n",
            "Epoch 4/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0043 - val_mse: 0.0043\n",
            "Epoch 5/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0044 - val_mse: 0.0044\n",
            "Epoch 6/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0044 - val_mse: 0.0044\n",
            "Epoch 7/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0044 - val_mse: 0.0044\n",
            "Epoch 8/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0044 - val_mse: 0.0044\n",
            "Epoch 9/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0045 - val_mse: 0.0045\n",
            "Epoch 10/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0045 - val_mse: 0.0045\n",
            "Epoch 11/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0046 - val_mse: 0.0046\n",
            "Epoch 12/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0046 - val_mse: 0.0046\n",
            "Epoch 13/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0046 - val_mse: 0.0046\n",
            "Epoch 14/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0046 - val_mse: 0.0046\n",
            "Epoch 15/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0046 - val_mse: 0.0046\n",
            "Epoch 16/150\n",
            "151/151 [==============================] - 2s 11ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0047 - val_mse: 0.0047\n",
            "Loss_at boosting round  4 :  0.040200725\n",
            "Logloss:  0.03891016\n",
            "Training at fold:  2 ####################################################################################################\n",
            "Epoch 1/150\n",
            "151/151 [==============================] - 2s 15ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0039 - val_mse: 0.0039\n",
            "Epoch 2/150\n",
            "151/151 [==============================] - 2s 11ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0037 - val_mse: 0.0037\n",
            "Epoch 3/150\n",
            "151/151 [==============================] - 2s 11ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0036 - val_mse: 0.0036\n",
            "Epoch 4/150\n",
            "151/151 [==============================] - 2s 11ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0035 - val_mse: 0.0035\n",
            "Epoch 5/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0035 - val_mse: 0.0035\n",
            "Epoch 6/150\n",
            "151/151 [==============================] - 2s 11ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0035 - val_mse: 0.0035\n",
            "Epoch 7/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0035 - val_mse: 0.0035\n",
            "Epoch 8/150\n",
            "151/151 [==============================] - 2s 11ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 9/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 10/150\n",
            "151/151 [==============================] - 2s 11ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 11/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 12/150\n",
            "151/151 [==============================] - 2s 11ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 13/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 14/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 15/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 16/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 17/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 18/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 19/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 20/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 21/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 22/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 23/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 24/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 25/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 26/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Epoch 27/150\n",
            "151/151 [==============================] - 2s 11ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0034 - val_mse: 0.0034\n",
            "Loss_at boosting round  0 :  0.021573618\n",
            "Epoch 1/150\n",
            "151/151 [==============================] - 2s 15ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0035 - val_mse: 0.0035\n",
            "Epoch 2/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0035 - val_mse: 0.0035\n",
            "Epoch 3/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0036 - val_mse: 0.0036\n",
            "Epoch 4/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0036 - val_mse: 0.0036\n",
            "Epoch 5/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0036 - val_mse: 0.0036\n",
            "Epoch 6/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0036 - val_mse: 0.0036\n",
            "Epoch 7/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0036 - val_mse: 0.0036\n",
            "Epoch 8/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0036 - val_mse: 0.0036\n",
            "Epoch 9/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0037 - val_mse: 0.0037\n",
            "Epoch 10/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0037 - val_mse: 0.0037\n",
            "Epoch 11/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0037 - val_mse: 0.0037\n",
            "Epoch 12/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0037 - val_mse: 0.0037\n",
            "Epoch 13/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0037 - val_mse: 0.0037\n",
            "Epoch 14/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0037 - val_mse: 0.0037\n",
            "Epoch 15/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0037 - val_mse: 0.0037\n",
            "Epoch 16/150\n",
            "151/151 [==============================] - 2s 11ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0037 - val_mse: 0.0037\n",
            "Loss_at boosting round  1 :  0.022340236\n",
            "Epoch 1/150\n",
            "151/151 [==============================] - 3s 18ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0037 - val_mse: 0.0037\n",
            "Epoch 2/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0038 - val_mse: 0.0038\n",
            "Epoch 3/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0039 - val_mse: 0.0039\n",
            "Epoch 4/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0041 - val_mse: 0.0041\n",
            "Epoch 5/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0041 - val_mse: 0.0041\n",
            "Epoch 6/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0042 - val_mse: 0.0042\n",
            "Epoch 7/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0042 - val_mse: 0.0042\n",
            "Epoch 8/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0042 - val_mse: 0.0042\n",
            "Epoch 9/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0042 - val_mse: 0.0042\n",
            "Epoch 10/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0043 - val_mse: 0.0043\n",
            "Epoch 11/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0043 - val_mse: 0.0043\n",
            "Epoch 12/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0043 - val_mse: 0.0043\n",
            "Epoch 13/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0043 - val_mse: 0.0043\n",
            "Epoch 14/150\n",
            "151/151 [==============================] - 2s 10ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0043 - val_mse: 0.0043\n",
            "Epoch 15/150\n",
            " 88/151 [================>.............] - ETA: 0s - loss: 0.0035 - mse: 0.0035"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fRhcYjV1XFd"
      },
      "source": [
        "# cols_target_low_score = to_train_targets[cols_target].sum().sort_values(ascending = True).head(2).index\n",
        "# cols_target_low_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SP5rAhV51XFf"
      },
      "source": [
        "# nn_pred_train1 = voting_predict( sum(model_2list, []), X_train, 206)\n",
        "# np.savetxt(\"NN_fts_pred_1.csv\", nn_pred_train1, delimiter=\",\")\n",
        "# single_model = model_2list[0][0]\n",
        "# represent = tf.keras.models.Sequential(single_model.layers[:-1])\n",
        "# nn_rept = represent.predict(X_train)\n",
        "# np.savetxt(\"NN_fts_representation.csv\", nn_rept, delimiter=\",\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foKXKUpV1XFi"
      },
      "source": [
        "prediction = voting_predict( sum(model_2list, []), X_pred, 206)\n",
        "\n",
        "df_preds_non_ctl =  pd.DataFrame(prediction, columns= cols_target, index = to_pred.index)\n",
        "\n",
        "# concat with all to pred values\n",
        "df_preds = pd.concat([ full_pred[cols_id], df_preds_non_ctl], axis = 1).fillna(0)\n",
        "# to csv\n",
        "df_preds.to_csv(\"submission.csv\", index = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4X03396OhGV"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"submission.csv\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW7JVOiQ1XFk"
      },
      "source": [
        "# import glob, os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvwM6P70NG-0"
      },
      "source": [
        "# glob.glob('/content/drive/My Drive/Data/colabs_data/MOA_kaggle/Model_MOA/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0y_v7pSNh6Y"
      },
      "source": [
        "# names = [os.path.basename(x) for x in glob.glob('/content/drive/My Drive/Data/colabs_data/MOA_kaggle/Model_MOA/')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrkeS7OoNsjl"
      },
      "source": [
        "# names"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}