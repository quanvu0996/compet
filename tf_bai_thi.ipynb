{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf_bai_thi.ipynb",
      "provenance": [],
      "mount_file_id": "17Upkt9V9tSgfrsxaTq0iTUxvvFQlSLJU",
      "authorship_tag": "ABX9TyOgwRGVnj4xPVbUX3P7WfQG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quanvu0996/compet/blob/master/tf_bai_thi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQ-5sHOSaPdt"
      },
      "source": [
        "!pip install tensorflow==2.3.0\r\n",
        "!pip install tensorflow_datasets==3.2.1\r\n",
        "!pip install Pillow==7.2.0"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCBkq61LcMA0"
      },
      "source": [
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jdg0Lce8gJ6r"
      },
      "source": [
        "Q1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqwNgzmQaqUO"
      },
      "source": [
        "xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\r\n",
        "ys = np.array([0.0, 1.0, 2.0, 3.0, 4.0, 5.0], dtype=float)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPJkdzShauMh",
        "outputId": "6921f8b2-2290-4322-b514-739f7e0ba63c"
      },
      "source": [
        "model = tf.keras.Sequential(\r\n",
        "        tf.keras.layers.Dense(1)\r\n",
        "    )\r\n",
        "model.compile(loss=tf.losses.MeanSquaredError(),\r\n",
        "                optimizer=tf.optimizers.Adam(),\r\n",
        "                metrics=[tf.metrics.MeanAbsoluteError()])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.4106 - mean_absolute_error: 2.0172\n",
            "Epoch 2/2\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 5.3965 - mean_absolute_error: 2.0147\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f93005377f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4MTolyucvq1",
        "outputId": "0ab44b58-4bde-46b3-938c-e640d2f1712f"
      },
      "source": [
        "model.fit(xs, ys, epochs = 10)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 5.3546 - mean_absolute_error: 2.0072\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 5.3406 - mean_absolute_error: 2.0047\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.3267 - mean_absolute_error: 2.0022\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 5.3128 - mean_absolute_error: 1.9997\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.2989 - mean_absolute_error: 1.9972\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 5.2850 - mean_absolute_error: 1.9947\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.2711 - mean_absolute_error: 1.9922\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 5.2573 - mean_absolute_error: 1.9897\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 5.2435 - mean_absolute_error: 1.9872\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 5.2297 - mean_absolute_error: 1.9848\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f93003437f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCkXuuLpgNFx"
      },
      "source": [
        "Q2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "v2nH1r_4czSY",
        "outputId": "6ccd4d76-e43e-4fb4-e677-7f2174b6762d"
      },
      "source": [
        "# ======================================================================\r\n",
        "# There are 5 questions in this exam with increasing difficulty from 1-5.\r\n",
        "# Please note that the weight of the grade for the question is relative\r\n",
        "# to its difficulty. So your Category 1 question will score significantly\r\n",
        "# less than your Category 5 question.\r\n",
        "#\r\n",
        "# Don't use lambda layers in your model.\r\n",
        "# You do not need them to solve the question.\r\n",
        "# Lambda layers are not supported by the grading infrastructure.\r\n",
        "#\r\n",
        "# You must use the Submit and Test button to submit your model\r\n",
        "# at least once in this category before you finally submit your exam,\r\n",
        "# otherwise you will score zero for this category.\r\n",
        "# ======================================================================\r\n",
        "#\r\n",
        "# Basic Datasets Question\r\n",
        "#\r\n",
        "# Create a classifier for the Fashion MNIST dataset\r\n",
        "# Note that the test will expect it to classify 10 classes and that the\r\n",
        "# input shape should be the native size of the Fashion MNIST dataset which is\r\n",
        "# 28x28 monochrome. Do not resize the data. Your input layer should accept\r\n",
        "# (28,28) as the input shape only. If you amend this, the tests will fail.\r\n",
        "#\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "\r\n",
        "def solution_model():\r\n",
        "    fashion_mnist = tf.keras.datasets.fashion_mnist\r\n",
        "\r\n",
        "    # YOUR CODE HERE\r\n",
        "    # loading data:\r\n",
        "    (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\r\n",
        "    class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\r\n",
        "                   'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\r\n",
        "    # Bá»• sung channel cho train features\r\n",
        "    train_images= np.expand_dims(train_images, -1)\r\n",
        "    num_classes = len(class_names)\r\n",
        "    model = tf.keras.Sequential([\r\n",
        "      tf.keras.layers.experimental.preprocessing.Rescaling(1./255, input_shape=(28, 28, 1)),\r\n",
        "      tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu'),\r\n",
        "      tf.keras.layers.MaxPooling2D(),\r\n",
        "      tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),\r\n",
        "      tf.keras.layers.MaxPooling2D(),\r\n",
        "      tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\r\n",
        "      tf.keras.layers.MaxPooling2D(),\r\n",
        "      tf.keras.layers.Flatten(),\r\n",
        "      tf.keras.layers.Dense(128, activation='relu'),\r\n",
        "      tf.keras.layers.Dense(num_classes, activation = 'softmax')\r\n",
        "    ])\r\n",
        "\r\n",
        "    model.compile(optimizer='adam',\r\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n",
        "              metrics=['accuracy'])\r\n",
        "\r\n",
        "    epochs = 20\r\n",
        "    # early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1E-5, patience=15, mode='min',restore_best_weights=True, verbose=1)\r\n",
        "    history = model.fit(train_images, train_labels, validation_split=0.2, epochs=epochs)\r\n",
        "\r\n",
        "    return model\r\n",
        "\r\n",
        "\r\n",
        "# Note that you'll need to save your model as a .h5 like this.\r\n",
        "# When you press the Submit and Test button, your saved .h5 model will\r\n",
        "# be sent to the testing infrastructure for scoring\r\n",
        "# and the score will be returned to you.\r\n",
        "if __name__ == '__main__':\r\n",
        "    model = solution_model()\r\n",
        "    model.save(\"mymodel.h5\")\r\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 1.7620 - accuracy: 0.7017 - val_loss: 1.7354 - val_accuracy: 0.7268\n",
            "Epoch 2/20\n",
            " 905/1500 [=================>............] - ETA: 1s - loss: 1.6913 - accuracy: 0.7704"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-33186d07234b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m# and the score will be returned to you.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolution_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mymodel.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-33186d07234b>\u001b[0m in \u001b[0;36msolution_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1E-5, patience=15, mode='min',restore_best_weights=True, verbose=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1101\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1103\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \"\"\"\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    287\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    307\u001b[0m       \u001b[0mbatch_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    340\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_supports_tf_logs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1014\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1016\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1017\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 635\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 635\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    531\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \"\"\"\n\u001b[1;32m   1062\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1027\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1030\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fN93wELshisx"
      },
      "source": [
        "Q3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7IpjiklrURs"
      },
      "source": [
        "_TRAIN_URL = \"https://storage.googleapis.com/download.tensorflow.org/data/horse-or-human.zip\"\r\n",
        "_TEST_URL = \"https://storage.googleapis.com/download.tensorflow.org/data/validation-horse-or-human.zip\"\r\n",
        "urllib.request.urlretrieve(_TRAIN_URL, 'horse-or-human.zip')\r\n",
        "local_zip = 'horse-or-human.zip'\r\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\r\n",
        "zip_ref.extractall('tmp/horse-or-human/')\r\n",
        "zip_ref.close()\r\n",
        "urllib.request.urlretrieve(_TEST_URL, 'testdata.zip')\r\n",
        "local_zip = 'testdata.zip'\r\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\r\n",
        "zip_ref.extractall('tmp/testdata/')\r\n",
        "zip_ref.close()"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9upfJupsUlw",
        "outputId": "0415fe26-a23b-4c05-a9f1-6ddca78590ff"
      },
      "source": [
        "dir_train = 'tmp/horse-or-human/'\r\n",
        "dir_test = 'tmp/testdata/'\r\n",
        "\r\n",
        "datagen_kwargs = dict(rescale=1./255#, validation_split=.20\r\n",
        "    )\r\n",
        "IMAGE_SIZE = (300, 300)\r\n",
        "BATCH_SIZE = 128\r\n",
        "dataflow_kwargs = dict(target_size=IMAGE_SIZE, batch_size=BATCH_SIZE, class_mode='binary')\r\n",
        "\r\n",
        "# All images will be rescaled by 1./255\r\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\r\n",
        " \r\n",
        "# Flow training images in batches of 128 using train_datagen generator\r\n",
        "train_generator = train_datagen.flow_from_directory(\r\n",
        "        dir_train, shuffle=True, **dataflow_kwargs)\r\n",
        "\r\n",
        "validation_datagen = ImageDataGenerator(#Your Code here\r\n",
        "    **datagen_kwargs)\r\n",
        "\r\n",
        "validation_generator = validation_datagen.flow_from_directory(\r\n",
        "    #Your Code Here\r\n",
        "    dir_test, shuffle=False, **dataflow_kwargs)\r\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1027 images belonging to 2 classes.\n",
            "Found 256 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0pv_rTUhhi3",
        "outputId": "9535f33e-63eb-4661-d1d7-2c817a2759e9"
      },
      "source": [
        "model = tf.keras.models.Sequential([\r\n",
        "    # Note the input shape is the desired size of the image 300x300 with 3 bytes color\r\n",
        "    # This is the first convolution\r\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\r\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\r\n",
        "    # The second convolution\r\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\r\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\r\n",
        "    # The third convolution\r\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\r\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\r\n",
        "    # The fourth convolution\r\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\r\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\r\n",
        "    # The fifth convolution\r\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\r\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\r\n",
        "    # Flatten the results to feed into a DNN\r\n",
        "    tf.keras.layers.Flatten(),\r\n",
        "    # 512 neuron hidden layer\r\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\r\n",
        "    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') and 1 for the other ('humans')\r\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\r\n",
        "])\r\n",
        "\r\n",
        " \r\n",
        "model.compile(loss='binary_crossentropy',\r\n",
        "              optimizer=tf.keras.optimizers.RMSprop(lr=0.001),\r\n",
        "              metrics=['acc'])\r\n",
        "\r\n",
        "model.fit(#Your Code Here#\r\n",
        "    train_generator, epochs = 5, validation_data = validation_generator)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "9/9 [==============================] - 7s 785ms/step - loss: 0.7362 - acc: 0.5034 - val_loss: 0.7246 - val_acc: 0.5000\n",
            "Epoch 2/5\n",
            "9/9 [==============================] - 8s 898ms/step - loss: 1.2286 - acc: 0.7342 - val_loss: 0.5053 - val_acc: 0.7891\n",
            "Epoch 3/5\n",
            "9/9 [==============================] - 7s 807ms/step - loss: 0.3350 - acc: 0.8909 - val_loss: 0.7380 - val_acc: 0.8516\n",
            "Epoch 4/5\n",
            "9/9 [==============================] - 7s 813ms/step - loss: 0.4797 - acc: 0.8403 - val_loss: 1.0012 - val_acc: 0.8242\n",
            "Epoch 5/5\n",
            "9/9 [==============================] - 7s 807ms/step - loss: 0.1681 - acc: 0.9318 - val_loss: 0.6841 - val_acc: 0.8750\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f92b1322048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tb-rZJqc3F0"
      },
      "source": [
        "model.save(\"q3_model.h5\")"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtSbdj4Lr9d5"
      },
      "source": [
        "train_generator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1NH610uxR6h"
      },
      "source": [
        "Q4_NLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFViDStVxRbO"
      },
      "source": [
        "url = 'https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json'\r\n",
        "urllib.request.urlretrieve(url, 'sarcasm.json')\r\n",
        "\r\n",
        "# DO NOT CHANGE THIS CODE OR THE TESTS MAY NOT WORK\r\n",
        "vocab_size = 1000\r\n",
        "embedding_dim = 16\r\n",
        "max_length = 120\r\n",
        "trunc_type='post'\r\n",
        "padding_type='post'\r\n",
        "oov_tok = \"<OOV>\"\r\n",
        "training_size = 20000\r\n",
        "\r\n",
        "sentences = []\r\n",
        "labels = []\r\n",
        "\r\n",
        "# YOUR CODE HERE\r\n",
        "with open('sarcasm.json', 'r') as f:\r\n",
        "    data = json.load(f)\r\n",
        "\r\n",
        "for d in data:\r\n",
        "    sentences.append(d['headline'])\r\n",
        "    labels.append(d['is_sarcastic'])\r\n",
        "\r\n",
        "train_ratio = 0.8\r\n",
        "train_size = int(len(data) * train_ratio)\r\n",
        "\r\n",
        "# train \r\n",
        "train_sentences = sentences[:train_size]\r\n",
        "valid_sentences = sentences[train_size:]\r\n",
        "# label \r\n",
        "train_labels = labels[:train_size]\r\n",
        "valid_labels = labels[train_size:]\r\n",
        "\r\n",
        "token = Tokenizer(num_words=vocab_size, oov_token= oov_tok)\r\n",
        "token.fit_on_texts(sentences)\r\n",
        "word_index = token.word_index\r\n",
        "\r\n",
        "train_sequences = token.texts_to_sequences(train_sentences)\r\n",
        "valid_sequences = token.texts_to_sequences(valid_sentences)\r\n",
        "\r\n",
        "train_padded = pad_sequences(train_sequences, truncating=trunc_type, padding=padding_type, maxlen=max_length)\r\n",
        "valid_padded = pad_sequences(valid_sequences, truncating=trunc_type, padding=padding_type, maxlen=max_length)\r\n",
        "\r\n",
        "train_labels = np.asarray(train_labels)\r\n",
        "valid_labels = np.asarray(valid_labels)\r\n",
        "\r\n",
        "#################################\r\n",
        "\r\n",
        "\r\n",
        "model = tf.keras.Sequential([\r\n",
        "# YOUR CODE HERE. KEEP THIS OUTPUT LAYER INTACT OR TESTS MAY FAIL\r\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\r\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\r\n",
        "    tf.keras.layers.Dense(24, activation='relu'),\r\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\r\n",
        "])"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7vkE-hiy2Gt"
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xrxt_Fo2y5B6",
        "outputId": "d9484981-a66f-4d71-9871-7c84659a0bb0"
      },
      "source": [
        "history = model.fit(train_padded, train_labels, \r\n",
        "                    validation_data=(valid_padded, valid_labels),\r\n",
        "                    epochs=20)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "668/668 [==============================] - 10s 14ms/step - loss: 0.4460 - accuracy: 0.7730 - val_loss: 0.3983 - val_accuracy: 0.8182\n",
            "Epoch 2/20\n",
            "668/668 [==============================] - 9s 14ms/step - loss: 0.3553 - accuracy: 0.8355 - val_loss: 0.3655 - val_accuracy: 0.8347\n",
            "Epoch 3/20\n",
            "668/668 [==============================] - 9s 13ms/step - loss: 0.3284 - accuracy: 0.8516 - val_loss: 0.3636 - val_accuracy: 0.8308\n",
            "Epoch 4/20\n",
            "668/668 [==============================] - 9s 13ms/step - loss: 0.3096 - accuracy: 0.8610 - val_loss: 0.3696 - val_accuracy: 0.8317\n",
            "Epoch 5/20\n",
            "668/668 [==============================] - 9s 13ms/step - loss: 0.2982 - accuracy: 0.8674 - val_loss: 0.3852 - val_accuracy: 0.8302\n",
            "Epoch 6/20\n",
            "668/668 [==============================] - 9s 13ms/step - loss: 0.2900 - accuracy: 0.8710 - val_loss: 0.3731 - val_accuracy: 0.8383\n",
            "Epoch 7/20\n",
            "668/668 [==============================] - 9s 14ms/step - loss: 0.2811 - accuracy: 0.8751 - val_loss: 0.3819 - val_accuracy: 0.8313\n",
            "Epoch 8/20\n",
            "668/668 [==============================] - 9s 14ms/step - loss: 0.2775 - accuracy: 0.8785 - val_loss: 0.3857 - val_accuracy: 0.8325\n",
            "Epoch 9/20\n",
            "668/668 [==============================] - 10s 16ms/step - loss: 0.2700 - accuracy: 0.8823 - val_loss: 0.4032 - val_accuracy: 0.8323\n",
            "Epoch 10/20\n",
            "668/668 [==============================] - 10s 15ms/step - loss: 0.2670 - accuracy: 0.8838 - val_loss: 0.3898 - val_accuracy: 0.8317\n",
            "Epoch 11/20\n",
            "668/668 [==============================] - 10s 15ms/step - loss: 0.2635 - accuracy: 0.8858 - val_loss: 0.4009 - val_accuracy: 0.8311\n",
            "Epoch 12/20\n",
            "668/668 [==============================] - 10s 15ms/step - loss: 0.2569 - accuracy: 0.8899 - val_loss: 0.3981 - val_accuracy: 0.8315\n",
            "Epoch 13/20\n",
            "668/668 [==============================] - 10s 15ms/step - loss: 0.2507 - accuracy: 0.8918 - val_loss: 0.4059 - val_accuracy: 0.8330\n",
            "Epoch 14/20\n",
            "668/668 [==============================] - 10s 14ms/step - loss: 0.2452 - accuracy: 0.8950 - val_loss: 0.4055 - val_accuracy: 0.8287\n",
            "Epoch 15/20\n",
            "668/668 [==============================] - 9s 14ms/step - loss: 0.2384 - accuracy: 0.8975 - val_loss: 0.4356 - val_accuracy: 0.8311\n",
            "Epoch 16/20\n",
            "668/668 [==============================] - 9s 14ms/step - loss: 0.2298 - accuracy: 0.9024 - val_loss: 0.4158 - val_accuracy: 0.8261\n",
            "Epoch 17/20\n",
            "668/668 [==============================] - 9s 14ms/step - loss: 0.2214 - accuracy: 0.9062 - val_loss: 0.4483 - val_accuracy: 0.8270\n",
            "Epoch 18/20\n",
            "668/668 [==============================] - 10s 14ms/step - loss: 0.2130 - accuracy: 0.9115 - val_loss: 0.4575 - val_accuracy: 0.8253\n",
            "Epoch 19/20\n",
            "668/668 [==============================] - 9s 14ms/step - loss: 0.2067 - accuracy: 0.9129 - val_loss: 0.4994 - val_accuracy: 0.8209\n",
            "Epoch 20/20\n",
            "668/668 [==============================] - 10s 14ms/step - loss: 0.1977 - accuracy: 0.9181 - val_loss: 0.5196 - val_accuracy: 0.8227\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJSdLJN91AOZ"
      },
      "source": [
        "model.save(\"q4_model.h5\")"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNsNFY0O3pwe"
      },
      "source": [
        "Q5. TIMESERIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eXZHBsH3r6t"
      },
      "source": [
        "# DO NOT CHANGE THIS CODE\r\n",
        "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\r\n",
        "    series = tf.expand_dims(series, axis=-1)\r\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\r\n",
        "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\r\n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\r\n",
        "    ds = ds.shuffle(shuffle_buffer)\r\n",
        "    ds = ds.map(lambda w: (w[:-1], w[1:]))\r\n",
        "    return ds.batch(batch_size).prefetch(1)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ef8suxu41Vu"
      },
      "source": [
        "import csv"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-H5UG_J3uyZ"
      },
      "source": [
        "url = 'https://storage.googleapis.com/download.tensorflow.org/data/Sunspots.csv'\r\n",
        "urllib.request.urlretrieve(url, 'sunspots.csv')\r\n",
        "\r\n",
        "time_step = []\r\n",
        "sunspots = []\r\n",
        "\r\n",
        "idx = []\r\n",
        "\r\n",
        "with open('sunspots.csv') as csvfile:\r\n",
        "  reader = csv.reader(csvfile, delimiter=',')\r\n",
        "  next(reader)\r\n",
        "  for row in reader:\r\n",
        "    sunspots.append(row[2])\r\n",
        "    time_step.append(row[1])\r\n",
        "    idx.append(row[0])"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfexXHro5rX1",
        "outputId": "3c965cb9-d59f-4ead-d440-f5d1cb901526"
      },
      "source": [
        "series"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.24284279, 0.26192868, 0.29306881, ..., 0.03314917, 0.03992968,\n",
              "       0.00401808])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22Z1W2oF4xgI"
      },
      "source": [
        "series = np.array(sunspots, dtype= np.double)\r\n",
        "min = np.min(series)\r\n",
        "max = np.max(series)\r\n",
        "series -= min\r\n",
        "series /= max\r\n",
        "time = np.array(time_step)"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDdnVs3d44ea"
      },
      "source": [
        "split_time = 3000\r\n",
        "time_train = time[:split_time]\r\n",
        "x_train = series[:split_time]\r\n",
        "time_valid = time[split_time:]\r\n",
        "x_valid = series[split_time:]"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0PpZO6p6M4Z"
      },
      "source": [
        "# DO NOT CHANGE THIS CODE\r\n",
        "window_size = 30\r\n",
        "batch_size = 32\r\n",
        "shuffle_buffer_size = 1000"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S4YDK8s6RJD"
      },
      "source": [
        "train_set = windowed_dataset(x_train, window_size=window_size, batch_size=batch_size, shuffle_buffer=shuffle_buffer_size)"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc3ZM83Z70iU"
      },
      "source": [
        "valid_set = windowed_dataset(x_valid, window_size=window_size, batch_size=batch_size, shuffle_buffer=shuffle_buffer_size)"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yq90TcEt7mIf",
        "outputId": "043b4744-8839-4a95-d2bb-a07aefa2aafc"
      },
      "source": [
        "train_set"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((None, None, 1), (None, None, 1)), types: (tf.float64, tf.float64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UB2KWJmv6uGv"
      },
      "source": [
        "model = tf.keras.Sequential([\r\n",
        "    tf.keras.layers.Dense(units=64, activation='relu'),\r\n",
        "    tf.keras.layers.Dense(units=64, activation='relu'),\r\n",
        "    tf.keras.layers.Dense(units=1)\r\n",
        "])"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gk9FzEcL7uDh",
        "outputId": "64b5a28b-0409-42b4-eb95-9644602ecc0c"
      },
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\r\n",
        "                                                    patience=2,\r\n",
        "                                                    mode='min')\r\n",
        "model.compile(loss=tf.losses.MeanSquaredError(),\r\n",
        "            optimizer=tf.optimizers.Adam(),\r\n",
        "            metrics=[tf.metrics.MeanAbsoluteError()])\r\n",
        "\r\n",
        "history = model.fit( train_set, epochs=10,\r\n",
        "                  validation_data= valid_set,\r\n",
        "                  callbacks=[early_stopping])"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:Layer dense_48 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 0.0107 - mean_absolute_error: 0.0713 - val_loss: 0.0029 - val_mean_absolute_error: 0.0393\n",
            "Epoch 2/10\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 0.0046 - mean_absolute_error: 0.0486 - val_loss: 0.0029 - val_mean_absolute_error: 0.0389\n",
            "Epoch 3/10\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 0.0046 - mean_absolute_error: 0.0485 - val_loss: 0.0029 - val_mean_absolute_error: 0.0387\n",
            "Epoch 4/10\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 0.0046 - mean_absolute_error: 0.0485 - val_loss: 0.0029 - val_mean_absolute_error: 0.0389\n",
            "Epoch 5/10\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 0.0046 - mean_absolute_error: 0.0485 - val_loss: 0.0029 - val_mean_absolute_error: 0.0383\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lc581jE7EPE"
      },
      "source": [
        "def model_forecast(model, series, window_size):\r\n",
        "   ds = tf.data.Dataset.from_tensor_slices(series)\r\n",
        "   ds = ds.window(window_size, shift=1, drop_remainder=True)\r\n",
        "   ds = ds.flat_map(lambda w: w.batch(window_size))\r\n",
        "   ds = ds.batch(32).prefetch(1)\r\n",
        "   forecast = model.predict(ds)\r\n",
        "   return forecast\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "rnn_forecast = model_forecast(model, series[..., np.newaxis], window_size)\r\n",
        "rnn_forecast = rnn_forecast[split_time - window_size:-1, -1, 0]\r\n",
        "\r\n",
        "result = tf.keras.metrics.mean_absolute_error(x_valid, rnn_forecast).numpy()"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47LsQuYQ8LhR",
        "outputId": "618a6312-fcc9-4fe7-ed86-46bf59ca4b4e"
      },
      "source": [
        "result"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.03973555"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MT7GRQLT8SM4"
      },
      "source": [
        "model.save('q5_model.h5')"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97i1N8ph8Vn9",
        "outputId": "03eef4ed-fd7f-43dd-8d8d-26d982607c33"
      },
      "source": [
        "# ======================================================================\r\n",
        "# There are 5 questions in this exam with increasing difficulty from 1-5.\r\n",
        "# Please note that the weight of the grade for the question is relative\r\n",
        "# to its difficulty. So your Category 1 question will score significantly\r\n",
        "# less than your Category 5 question.\r\n",
        "#\r\n",
        "# Don't use lambda layers in your model.\r\n",
        "# You do not need them to solve the question.\r\n",
        "# Lambda layers are not supported by the grading infrastructure.\r\n",
        "#\r\n",
        "# You must use the Submit and Test model button to submit your model\r\n",
        "# at least once in this category before you finally submit your exam,\r\n",
        "# otherwise you will score zero for this category.\r\n",
        "# ======================================================================\r\n",
        "#\r\n",
        "# Getting Started Question\r\n",
        "#\r\n",
        "# Given this data, train a neural network to match the xs to the ys\r\n",
        "# So that a predictor for a new value of X will give a float value\r\n",
        "# very close to the desired answer\r\n",
        "# i.e. print(model.predict([10.0])) would give a satisfactory result\r\n",
        "# The test infrastructure expects a trained model that accepts\r\n",
        "# an input shape of [1]\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "def solution_model():\r\n",
        "    xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\r\n",
        "    ys = np.array([0.0, 1.0, 2.0, 3.0, 4.0, 5.0], dtype=float)\r\n",
        "\r\n",
        "    # YOUR CODE HERE\r\n",
        "    model = tf.keras.Sequential([\r\n",
        "        # tf.keras.layers.Dense(1, activation='relu'),\r\n",
        "        # tf.keras.layers.Dense(4, activation='relu'),\r\n",
        "        tf.keras.layers.Dense(16),\r\n",
        "        # tf.keras.layers.Dense(4, activation='relu'),\r\n",
        "        tf.keras.layers.Dense(1)\r\n",
        "    ])\r\n",
        "    model.compile(loss=tf.losses.MeanSquaredError(),\r\n",
        "                  optimizer=tf.optimizers.Adam(),\r\n",
        "                  metrics=[tf.metrics.MeanAbsoluteError()])\r\n",
        "    model.fit(xs, ys, epochs=100, batch_size = 1)\r\n",
        "    return model\r\n",
        "\r\n",
        "\r\n",
        "# Note that you'll need to save your model as a .h5 like this.\r\n",
        "# When you press the Submit and Test button, your saved .h5 model will\r\n",
        "# be sent to the testing infrastructure for scoring\r\n",
        "# and the score will be returned to you.\r\n",
        "if __name__ == '__main__':\r\n",
        "    model = solution_model()\r\n",
        "    model.save(\"q1_model.h5\")\r\n",
        "    print('saved_model')\r\n"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 13.0686 - mean_absolute_error: 2.9844\n",
            "Epoch 2/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 12.5042 - mean_absolute_error: 2.9175\n",
            "Epoch 3/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 11.8163 - mean_absolute_error: 2.8380\n",
            "Epoch 4/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 11.1886 - mean_absolute_error: 2.7676\n",
            "Epoch 5/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 10.7410 - mean_absolute_error: 2.7052\n",
            "Epoch 6/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 10.2259 - mean_absolute_error: 2.6359\n",
            "Epoch 7/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 9.5825 - mean_absolute_error: 2.5601\n",
            "Epoch 8/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 9.2023 - mean_absolute_error: 2.5038\n",
            "Epoch 9/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 8.5777 - mean_absolute_error: 2.4210\n",
            "Epoch 10/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 8.1625 - mean_absolute_error: 2.3600\n",
            "Epoch 11/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 7.7129 - mean_absolute_error: 2.2964\n",
            "Epoch 12/100\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 7.3322 - mean_absolute_error: 2.2379\n",
            "Epoch 13/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 6.9348 - mean_absolute_error: 2.1709\n",
            "Epoch 14/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 6.4739 - mean_absolute_error: 2.0998\n",
            "Epoch 15/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 6.0059 - mean_absolute_error: 2.0244\n",
            "Epoch 16/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 5.6923 - mean_absolute_error: 1.9686\n",
            "Epoch 17/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 5.3195 - mean_absolute_error: 1.9054\n",
            "Epoch 18/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 4.8839 - mean_absolute_error: 1.8300\n",
            "Epoch 19/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 4.6612 - mean_absolute_error: 1.7840\n",
            "Epoch 20/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 4.2713 - mean_absolute_error: 1.7061\n",
            "Epoch 21/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 4.0022 - mean_absolute_error: 1.6520\n",
            "Epoch 22/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 3.6577 - mean_absolute_error: 1.5786\n",
            "Epoch 23/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 3.3782 - mean_absolute_error: 1.5188\n",
            "Epoch 24/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 3.1056 - mean_absolute_error: 1.4551\n",
            "Epoch 25/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 2.8543 - mean_absolute_error: 1.3955\n",
            "Epoch 26/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 2.6119 - mean_absolute_error: 1.3322\n",
            "Epoch 27/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 2.4116 - mean_absolute_error: 1.2767\n",
            "Epoch 28/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 2.1236 - mean_absolute_error: 1.2052\n",
            "Epoch 29/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 1.9494 - mean_absolute_error: 1.1515\n",
            "Epoch 30/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 1.7647 - mean_absolute_error: 1.0954\n",
            "Epoch 31/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 1.5812 - mean_absolute_error: 1.0398\n",
            "Epoch 32/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 1.3890 - mean_absolute_error: 0.9780\n",
            "Epoch 33/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 1.2455 - mean_absolute_error: 0.9257\n",
            "Epoch 34/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 1.1159 - mean_absolute_error: 0.8755\n",
            "Epoch 35/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.9952 - mean_absolute_error: 0.8285\n",
            "Epoch 36/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.8769 - mean_absolute_error: 0.7738\n",
            "Epoch 37/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.7755 - mean_absolute_error: 0.7298\n",
            "Epoch 38/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6773 - mean_absolute_error: 0.6799\n",
            "Epoch 39/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.5931 - mean_absolute_error: 0.6361\n",
            "Epoch 40/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.5029 - mean_absolute_error: 0.5872\n",
            "Epoch 41/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.4452 - mean_absolute_error: 0.5473\n",
            "Epoch 42/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.3720 - mean_absolute_error: 0.5016\n",
            "Epoch 43/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.3176 - mean_absolute_error: 0.4664\n",
            "Epoch 44/100\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.2722 - mean_absolute_error: 0.4294\n",
            "Epoch 45/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.2226 - mean_absolute_error: 0.3909\n",
            "Epoch 46/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.1963 - mean_absolute_error: 0.3661\n",
            "Epoch 47/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.1660 - mean_absolute_error: 0.3351\n",
            "Epoch 48/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.1361 - mean_absolute_error: 0.3029\n",
            "Epoch 49/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.1082 - mean_absolute_error: 0.2732\n",
            "Epoch 50/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.0939 - mean_absolute_error: 0.2527\n",
            "Epoch 51/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.0790 - mean_absolute_error: 0.2306\n",
            "Epoch 52/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.0662 - mean_absolute_error: 0.2106\n",
            "Epoch 53/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.0518 - mean_absolute_error: 0.1869\n",
            "Epoch 54/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.0419 - mean_absolute_error: 0.1678\n",
            "Epoch 55/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.0347 - mean_absolute_error: 0.1526\n",
            "Epoch 56/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.0273 - mean_absolute_error: 0.1357\n",
            "Epoch 57/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.0213 - mean_absolute_error: 0.1212\n",
            "Epoch 58/100\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.0171 - mean_absolute_error: 0.1086\n",
            "Epoch 59/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.0139 - mean_absolute_error: 0.0978\n",
            "Epoch 60/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.0112 - mean_absolute_error: 0.0870\n",
            "Epoch 61/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.0089 - mean_absolute_error: 0.0775\n",
            "Epoch 62/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.0070 - mean_absolute_error: 0.0691\n",
            "Epoch 63/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.0051 - mean_absolute_error: 0.0595\n",
            "Epoch 64/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.0042 - mean_absolute_error: 0.0533\n",
            "Epoch 65/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.0034 - mean_absolute_error: 0.0477\n",
            "Epoch 66/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.0026 - mean_absolute_error: 0.0417\n",
            "Epoch 67/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.0019 - mean_absolute_error: 0.0363\n",
            "Epoch 68/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.0016 - mean_absolute_error: 0.0328\n",
            "Epoch 69/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.0012 - mean_absolute_error: 0.0280\n",
            "Epoch 70/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 8.7800e-04 - mean_absolute_error: 0.0243\n",
            "Epoch 71/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 6.6421e-04 - mean_absolute_error: 0.0210\n",
            "Epoch 72/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 5.3738e-04 - mean_absolute_error: 0.0187\n",
            "Epoch 73/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 3.8789e-04 - mean_absolute_error: 0.0160\n",
            "Epoch 74/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 3.0069e-04 - mean_absolute_error: 0.0143\n",
            "Epoch 75/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 2.3638e-04 - mean_absolute_error: 0.0128\n",
            "Epoch 76/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 1.8302e-04 - mean_absolute_error: 0.0113\n",
            "Epoch 77/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 1.4590e-04 - mean_absolute_error: 0.0102\n",
            "Epoch 78/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 1.0922e-04 - mean_absolute_error: 0.0089\n",
            "Epoch 79/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 8.7127e-05 - mean_absolute_error: 0.0078\n",
            "Epoch 80/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 7.0925e-05 - mean_absolute_error: 0.0072\n",
            "Epoch 81/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 5.3594e-05 - mean_absolute_error: 0.0062\n",
            "Epoch 82/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 4.4107e-05 - mean_absolute_error: 0.0058\n",
            "Epoch 83/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 3.9125e-05 - mean_absolute_error: 0.0054\n",
            "Epoch 84/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 3.1798e-05 - mean_absolute_error: 0.0050\n",
            "Epoch 85/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 2.8533e-05 - mean_absolute_error: 0.0047\n",
            "Epoch 86/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 2.4801e-05 - mean_absolute_error: 0.0044\n",
            "Epoch 87/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 2.2404e-05 - mean_absolute_error: 0.0041\n",
            "Epoch 88/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 2.0401e-05 - mean_absolute_error: 0.0039\n",
            "Epoch 89/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 1.9166e-05 - mean_absolute_error: 0.0037\n",
            "Epoch 90/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 1.7904e-05 - mean_absolute_error: 0.0036\n",
            "Epoch 91/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 1.6761e-05 - mean_absolute_error: 0.0035\n",
            "Epoch 92/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 1.6064e-05 - mean_absolute_error: 0.0034\n",
            "Epoch 93/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 1.5231e-05 - mean_absolute_error: 0.0033\n",
            "Epoch 94/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 1.4422e-05 - mean_absolute_error: 0.0032\n",
            "Epoch 95/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 1.3980e-05 - mean_absolute_error: 0.0032\n",
            "Epoch 96/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 1.3482e-05 - mean_absolute_error: 0.0031\n",
            "Epoch 97/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 1.2977e-05 - mean_absolute_error: 0.0030\n",
            "Epoch 98/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 1.2493e-05 - mean_absolute_error: 0.0030\n",
            "Epoch 99/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 1.1983e-05 - mean_absolute_error: 0.0029\n",
            "Epoch 100/100\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 1.1523e-05 - mean_absolute_error: 0.0029\n",
            "saved_model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyjenEbr-oIF"
      },
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\r\n",
        "\r\n",
        "# YOUR CODE HERE\r\n",
        "# loading data:\r\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\r\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\r\n",
        "                'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\r\n",
        "# Bá»• sung channel cho train features\r\n",
        "num_classes = len(class_names)"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JFF2a-HAfl1",
        "outputId": "b46910c2-13a8-4d53-a314-ae21457f2b22"
      },
      "source": [
        "train_images.shape"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uy5eSyhDAey_",
        "outputId": "86a5fc76-f2ef-429b-9da5-6e4a9d37b59c"
      },
      "source": [
        "model = tf.keras.Sequential([\r\n",
        "  tf.keras.layers.BatchNormalization(input_shape = (28,28)),\r\n",
        "  tf.keras.layers.Reshape(target_shape=(28,28,1), input_shape = (28,28)),\r\n",
        "  tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu'),\r\n",
        "  tf.keras.layers.MaxPooling2D(),\r\n",
        "  tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),\r\n",
        "  tf.keras.layers.BatchNormalization(),\r\n",
        "  tf.keras.layers.MaxPooling2D(),\r\n",
        "  tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\r\n",
        "  tf.keras.layers.MaxPooling2D(),\r\n",
        "  tf.keras.layers.Flatten(),\r\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\r\n",
        "  tf.keras.layers.BatchNormalization(),\r\n",
        "  tf.keras.layers.Dense(num_classes, activation = 'softmax')\r\n",
        "])\r\n",
        "\r\n",
        "model.compile(optimizer='adam',\r\n",
        "          loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n",
        "          metrics=['accuracy'])\r\n",
        "epochs = 20\r\n",
        "# early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1E-5, patience=15, mode='min',restore_best_weights=True, verbose=1)\r\n",
        "history = model.fit(train_images, train_labels, validation_split=0.2, epochs=epochs)"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1500/1500 [==============================] - 5s 4ms/step - loss: 1.6339 - accuracy: 0.8371 - val_loss: 1.5855 - val_accuracy: 0.8788\n",
            "Epoch 2/20\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 1.5851 - accuracy: 0.8789 - val_loss: 1.5815 - val_accuracy: 0.8800\n",
            "Epoch 3/20\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 1.5708 - accuracy: 0.8921 - val_loss: 1.5762 - val_accuracy: 0.8856\n",
            "Epoch 4/20\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 1.5645 - accuracy: 0.8980 - val_loss: 1.5749 - val_accuracy: 0.8870\n",
            "Epoch 5/20\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 1.5582 - accuracy: 0.9044 - val_loss: 1.5676 - val_accuracy: 0.8946\n",
            "Epoch 6/20\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 1.5539 - accuracy: 0.9083 - val_loss: 1.5665 - val_accuracy: 0.8953\n",
            "Epoch 7/20\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 1.5505 - accuracy: 0.9115 - val_loss: 1.5708 - val_accuracy: 0.8908\n",
            "Epoch 8/20\n",
            "1500/1500 [==============================] - 5s 4ms/step - loss: 1.5460 - accuracy: 0.9159 - val_loss: 1.5667 - val_accuracy: 0.8943\n",
            "Epoch 9/20\n",
            "1500/1500 [==============================] - 5s 4ms/step - loss: 1.5439 - accuracy: 0.9177 - val_loss: 1.5561 - val_accuracy: 0.9052\n",
            "Epoch 10/20\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 1.5405 - accuracy: 0.9215 - val_loss: 1.5650 - val_accuracy: 0.8963\n",
            "Epoch 11/20\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 1.5372 - accuracy: 0.9247 - val_loss: 1.5595 - val_accuracy: 0.9024\n",
            "Epoch 12/20\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 1.5358 - accuracy: 0.9256 - val_loss: 1.5610 - val_accuracy: 0.9003\n",
            "Epoch 13/20\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 1.5335 - accuracy: 0.9280 - val_loss: 1.5542 - val_accuracy: 0.9082\n",
            "Epoch 14/20\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 1.5309 - accuracy: 0.9309 - val_loss: 1.5570 - val_accuracy: 0.9050\n",
            "Epoch 15/20\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 1.5295 - accuracy: 0.9319 - val_loss: 1.5504 - val_accuracy: 0.9103\n",
            "Epoch 16/20\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 1.5279 - accuracy: 0.9338 - val_loss: 1.5493 - val_accuracy: 0.9125\n",
            "Epoch 17/20\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 1.5270 - accuracy: 0.9344 - val_loss: 1.5482 - val_accuracy: 0.9127\n",
            "Epoch 18/20\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 1.5249 - accuracy: 0.9369 - val_loss: 1.5578 - val_accuracy: 0.9030\n",
            "Epoch 19/20\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 1.5233 - accuracy: 0.9382 - val_loss: 1.5489 - val_accuracy: 0.9128\n",
            "Epoch 20/20\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 1.5222 - accuracy: 0.9391 - val_loss: 1.5509 - val_accuracy: 0.9103\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErYY7PjLAJzp"
      },
      "source": [
        "model.save(\"q2_model.h5\")"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgXiTJaIClIA",
        "outputId": "cadc6bd5-9cc7-42da-e5bf-05e2b6c1c8d3"
      },
      "source": [
        "# ======================================================================\r\n",
        "# There are 5 questions in this exam with increasing difficulty from 1-5.\r\n",
        "# Please note that the weight of the grade for the question is relative\r\n",
        "# to its difficulty. So your Category 1 question will score significantly\r\n",
        "# less than your Category 5 question.\r\n",
        "#\r\n",
        "# Don't use lambda layers in your model.\r\n",
        "# You do not need them to solve the question.\r\n",
        "# Lambda layers are not supported by the grading infrastructure.\r\n",
        "#\r\n",
        "# You must use the Submit and Test button to submit your model\r\n",
        "# at least once in this category before you finally submit your exam,\r\n",
        "# otherwise you will score zero for this category.\r\n",
        "# ======================================================================\r\n",
        "#\r\n",
        "# Basic Datasets Question\r\n",
        "#\r\n",
        "# Create a classifier for the Fashion MNIST dataset\r\n",
        "# Note that the test will expect it to classify 10 classes and that the\r\n",
        "# input shape should be the native size of the Fashion MNIST dataset which is\r\n",
        "# 28x28 monochrome. Do not resize the data. Your input layer should accept\r\n",
        "# (28,28) as the input shape only. If you amend this, the tests will fail.\r\n",
        "#\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "\r\n",
        "def solution_model():\r\n",
        "    fashion_mnist = tf.keras.datasets.fashion_mnist\r\n",
        "\r\n",
        "    # YOUR CODE HERE\r\n",
        "    # loading data:\r\n",
        "    (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\r\n",
        "    train_images = train_images / 255.0\r\n",
        "    test_images = test_images / 255.0\r\n",
        "\r\n",
        "    # Bá»• sung channel cho train features\r\n",
        "    num_classes = 10\r\n",
        "    model = tf.keras.Sequential([\r\n",
        "        tf.keras.layers.Conv1D(16, 3, padding='same', activation='relu'),\r\n",
        "        tf.keras.layers.Conv1D(8, 3, padding='same', activation='relu'),\r\n",
        "        tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\r\n",
        "        tf.keras.layers.Dense(10, activation='softmax')\r\n",
        "    ])\r\n",
        "    model.compile(optimizer='adam',\r\n",
        "                  loss='sparse_categorical_crossentropy',\r\n",
        "                  metrics=['accuracy'])\r\n",
        "    epochs = 20\r\n",
        "    # early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1E-5, patience=15, mode='min',restore_best_weights=True, verbose=1)\r\n",
        "    history = model.fit(train_images, train_labels, validation_data = (test_images, test_labels), epochs=epochs)\r\n",
        "\r\n",
        "    return model\r\n",
        "\r\n",
        "\r\n",
        "# Note that you'll need to save your model as a .h5 like this.\r\n",
        "# When you press the Submit and Test button, your saved .h5 model will\r\n",
        "# be sent to the testing infrastructure for scoring\r\n",
        "# and the score will be returned to you.\r\n",
        "if __name__ == '__main__':\r\n",
        "    model = solution_model()\r\n",
        "    model.save(\"q2_model.h5\")"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.5260 - accuracy: 0.8123 - val_loss: 0.4569 - val_accuracy: 0.8375\n",
            "Epoch 2/20\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.3781 - accuracy: 0.8633 - val_loss: 0.3863 - val_accuracy: 0.8590\n",
            "Epoch 3/20\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.3425 - accuracy: 0.8747 - val_loss: 0.3823 - val_accuracy: 0.8581\n",
            "Epoch 4/20\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.3201 - accuracy: 0.8820 - val_loss: 0.3433 - val_accuracy: 0.8738\n",
            "Epoch 5/20\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.3033 - accuracy: 0.8874 - val_loss: 0.3421 - val_accuracy: 0.8749\n",
            "Epoch 6/20\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2882 - accuracy: 0.8914 - val_loss: 0.3381 - val_accuracy: 0.8756\n",
            "Epoch 7/20\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2766 - accuracy: 0.8967 - val_loss: 0.3532 - val_accuracy: 0.8745\n",
            "Epoch 8/20\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2666 - accuracy: 0.8998 - val_loss: 0.3372 - val_accuracy: 0.8784\n",
            "Epoch 9/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2558 - accuracy: 0.9029 - val_loss: 0.3395 - val_accuracy: 0.8766\n",
            "Epoch 10/20\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2477 - accuracy: 0.9068 - val_loss: 0.3491 - val_accuracy: 0.8720\n",
            "Epoch 11/20\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2396 - accuracy: 0.9099 - val_loss: 0.3356 - val_accuracy: 0.8782\n",
            "Epoch 12/20\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2313 - accuracy: 0.9123 - val_loss: 0.3552 - val_accuracy: 0.8800\n",
            "Epoch 13/20\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2245 - accuracy: 0.9152 - val_loss: 0.3453 - val_accuracy: 0.8772\n",
            "Epoch 14/20\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2167 - accuracy: 0.9181 - val_loss: 0.3238 - val_accuracy: 0.8880\n",
            "Epoch 15/20\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2092 - accuracy: 0.9205 - val_loss: 0.3348 - val_accuracy: 0.8854\n",
            "Epoch 16/20\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2046 - accuracy: 0.9227 - val_loss: 0.3584 - val_accuracy: 0.8799\n",
            "Epoch 17/20\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.1968 - accuracy: 0.9247 - val_loss: 0.3552 - val_accuracy: 0.8828\n",
            "Epoch 18/20\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.1898 - accuracy: 0.9277 - val_loss: 0.3489 - val_accuracy: 0.8810\n",
            "Epoch 19/20\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.1860 - accuracy: 0.9295 - val_loss: 0.3470 - val_accuracy: 0.8867\n",
            "Epoch 20/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1798 - accuracy: 0.9314 - val_loss: 0.3621 - val_accuracy: 0.8824\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2v0o0VACxd8",
        "outputId": "d02cbc6f-f147-4767-a8ed-4d4689f395a4"
      },
      "source": [
        "model.predict()"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiyCAOk5KuI_"
      },
      "source": [
        "  q4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yty96k2KDpVD",
        "outputId": "83ab9def-f9c8-4892-ac3e-7932a94dabce"
      },
      "source": [
        "# Q4\r\n",
        "# ======================================================================\r\n",
        "# There are 5 questions in this exam with increasing difficulty from 1-5.\r\n",
        "# Please note that the weight of the grade for the question is relative\r\n",
        "# to its difficulty. So your Category 1 question will score significantly\r\n",
        "# less than your Category 5 question.\r\n",
        "#\r\n",
        "# Don't use lambda layers in your model.\r\n",
        "# You do not need them to solve the question.\r\n",
        "# Lambda layers are not supported by the grading infrastructure.\r\n",
        "#\r\n",
        "# You must use the Submit and Test button to submit your model\r\n",
        "# at least once in this category before you finally submit your exam,\r\n",
        "# otherwise you will score zero for this category.\r\n",
        "# ======================================================================\r\n",
        "#\r\n",
        "# NLP QUESTION\r\n",
        "#\r\n",
        "# Build and train a classifier for the sarcasm dataset.\r\n",
        "# The classifier should have a final layer with 1 neuron activated by sigmoid as shown.\r\n",
        "# It will be tested against a number of sentences that the network hasn't previously seen\r\n",
        "# and you will be scored on whether sarcasm was correctly detected in those sentences.\r\n",
        "\r\n",
        "import json\r\n",
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "import urllib\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "\r\n",
        "\r\n",
        "def solution_model():\r\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json'\r\n",
        "    urllib.request.urlretrieve(url, 'sarcasm.json')\r\n",
        "\r\n",
        "    # DO NOT CHANGE THIS CODE OR THE TESTS MAY NOT WORK\r\n",
        "    vocab_size = 1000\r\n",
        "    embedding_dim = 16\r\n",
        "    max_length = 120\r\n",
        "    trunc_type='post'\r\n",
        "    padding_type='post'\r\n",
        "    oov_tok = \"<OOV>\"\r\n",
        "    training_size = 20000\r\n",
        "\r\n",
        "    sentences = []\r\n",
        "    labels = []\r\n",
        "    # YOUR CODE HERE\r\n",
        "    with open('sarcasm.json', 'r') as f:\r\n",
        "        data = json.load(f)\r\n",
        "\r\n",
        "    for d in data:\r\n",
        "        sentences.append(d['headline'])\r\n",
        "        labels.append(d['is_sarcastic'])\r\n",
        "\r\n",
        "    token = Tokenizer(num_words=vocab_size, oov_token= oov_tok)\r\n",
        "    token.fit_on_texts(sentences)\r\n",
        "\r\n",
        "    sequences = token.texts_to_sequences(sentences)\r\n",
        "    padded = pad_sequences(sequences, truncating=trunc_type, padding=padding_type, maxlen=max_length)\r\n",
        "\r\n",
        "    arr_labels = np.asarray(labels)\r\n",
        "\r\n",
        "    model = tf.keras.Sequential([\r\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\r\n",
        "        tf.keras.layers.GlobalAveragePooling1D(),\r\n",
        "        tf.keras.layers.Dense(24, activation='relu'),\r\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\r\n",
        "    ])\r\n",
        "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\r\n",
        "\r\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\r\n",
        "                                                        patience=2,\r\n",
        "                                                        mode='min')\r\n",
        "    history = model.fit(padded, arr_labels, \r\n",
        "                        validation_split = 0.2,\r\n",
        "                        epochs=20, batch_size = 1, callbacks=[early_stopping])\r\n",
        "    return model\r\n",
        "\r\n",
        "\r\n",
        "# Note that you'll need to save your model as a .h5 like this.\r\n",
        "# When you press the Submit and Test button, your saved .h5 model will\r\n",
        "# be sent to the testing infrastructure for scoring\r\n",
        "# and the score will be returned to you.\r\n",
        "if __name__ == '__main__':\r\n",
        "    model = solution_model()\r\n",
        "    model.save(\"mymodel.h5\")\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "21367/21367 [==============================] - 60s 3ms/step - loss: 0.4940 - accuracy: 0.7448 - val_loss: 0.4661 - val_accuracy: 0.7679\n",
            "Epoch 2/20\n",
            "21367/21367 [==============================] - 58s 3ms/step - loss: 0.3954 - accuracy: 0.8143 - val_loss: 0.4013 - val_accuracy: 0.8117\n",
            "Epoch 3/20\n",
            "21367/21367 [==============================] - 57s 3ms/step - loss: 0.3816 - accuracy: 0.8222 - val_loss: 0.3975 - val_accuracy: 0.8186\n",
            "Epoch 4/20\n",
            "21367/21367 [==============================] - 56s 3ms/step - loss: 0.3750 - accuracy: 0.8274 - val_loss: 0.4005 - val_accuracy: 0.8119\n",
            "Epoch 5/20\n",
            " 1473/21367 [=>............................] - ETA: 45s - loss: 0.3716 - accuracy: 0.8357"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mtfn5aNWKzrB",
        "outputId": "6e2eab02-07db-4cf4-86f0-acc95c24f12f"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_51\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 120, 16)           16000     \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 64)                12544     \n",
            "_________________________________________________________________\n",
            "dense_101 (Dense)            (None, 24)                1560      \n",
            "_________________________________________________________________\n",
            "dropout_41 (Dropout)         (None, 24)                0         \n",
            "_________________________________________________________________\n",
            "dense_102 (Dense)            (None, 1)                 25        \n",
            "=================================================================\n",
            "Total params: 30,129\n",
            "Trainable params: 30,129\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uatLbz_HLuMy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}