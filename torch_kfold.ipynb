{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/iterativestratification')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport copy\nimport os\n\nimport tensorflow as tf\nfrom tensorflow import feature_column as fc\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.layers import (Dense, DenseFeatures, Dropout, \n                                     BatchNormalization, Embedding, Input, Concatenate, Average,\n                                     InputLayer, Lambda)\nfrom tensorflow.keras.metrics import AUC, Precision, Recall\nfrom tensorflow.keras import backend as K, Sequential, Model\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.optimizers import Adam, RMSprop\n\nfrom tensorflow_addons.layers import WeightNormalization\nfrom keras.wrappers.scikit_learn import KerasRegressor\nimport keras\n\nfrom sklearn.preprocessing import QuantileTransformer, StandardScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.nn.modules.loss import _WeightedLoss\nfrom torch.utils.data import DataLoader,  TensorDataset\n\nimport random\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport matplotlib.pyplot as plt\nfrom math import log2\n\nprint(pd.__version__)\nprint(tf.__version__)","execution_count":2,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:68: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.3.0 (nightly versions are not supported). \n The versions of TensorFlow you are currently using is 2.3.0 and is not supported. \nSome things might work, some things might not.\nIf you were to encounter a bug, do not file an issue.\nIf you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \nYou can find the compatibility matrix in TensorFlow Addon's readme:\nhttps://github.com/tensorflow/addons\n  UserWarning,\n","name":"stderr"},{"output_type":"stream","text":"1.1.1\n2.3.0\n","name":"stdout"}]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Loading data and encoding\n\nfolder_path = '../input/lish-moa/'\nraw_test = pd.read_csv(folder_path + 'test_features.csv')\nraw_train = pd.read_csv(folder_path + 'train_features.csv')\nraw_targets = pd.read_csv(folder_path + 'train_targets_scored.csv')\n\n# Phân loại dữ liệu\ncols_id = ['sig_id']\ncols_to_remove = ['cp_type']\ncols_fts = [i for i in raw_train.columns if i not in cols_id +cols_to_remove]\ncols_gene = [col for col in raw_train.columns if col.startswith(\"g-\")]\ncols_cell = [col for col in raw_train.columns if col.startswith(\"c-\")]\ncols_experiment = [col for col in cols_fts if col not in cols_gene+cols_cell]\ncols_target = [i for i in raw_targets.columns if i not in cols_id]\nnum_fts, num_labels = len(cols_fts), len(cols_target)\n\n# xử lý categorical\ndef transform_data(input_data):\n    '''Clean data and encoding\n        * input_data: table '''\n    out = input_data.copy()\n    out['cp_dose'] = out['cp_dose'].map({'D1':0, 'D2':1})\n    out['cp_time'] = out['cp_time']/72\n    \n    return out\n\nto_train = transform_data(raw_train[raw_train['cp_type'] != 'ctl_vehicle'])\nto_train_targets = raw_targets.iloc[to_train.index]\nto_pred  = transform_data(raw_test)\nto_pred_non_ctl = to_pred[to_pred['cp_type'] != 'ctl_vehicle']","execution_count":3,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# preprocessing pipeline\ndef pipe_line_builder(quantiles_num, pca_dims):\n    '''Dựng pipe line cho từng nhóm columns\n    :quantiles_num: int: số quantile khi normalise\n    :pca_dims: int: số chiều pca'''\n    norm = QuantileTransformer(n_quantiles=quantiles_num,random_state=0, output_distribution=\"normal\")\n    pca = PCA(n_components = pca_dims)\n    # kmean = KMeans(kmean_centroids)\n    p_norm_pca = Pipeline([ \n        ('norm', norm),\n        ('pca', pca)\n    ])\n    return FeatureUnion([\n        ('norm', norm), \n        ('norm_pca', p_norm_pca) \n        ])\n\npipe = Pipeline([\n    ('norm_pca', ColumnTransformer([\n                     ('gene', pipe_line_builder(quantiles_num = 200, pca_dims = 600), cols_gene),\n                     ('cell', pipe_line_builder(quantiles_num = 200, pca_dims = 50), cols_cell),\n                    ]) \n    ), \n    ('var', VarianceThreshold(0.02)) \n])\n\npipe = ColumnTransformer([\n    ('gene_cell', pipe, cols_gene+ cols_cell),\n    ('experiment', 'passthrough', cols_experiment)\n])","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Final data\npipe.fit(to_train[cols_fts].append(to_pred[cols_fts]))\nX_train = pipe.transform(to_train[cols_fts])\nX_pred = pipe.transform(to_pred_non_ctl[cols_fts])\ny_train_df = to_train_targets[cols_target]\nX_train","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"array([[ 1.12865013,  0.89757181, -0.43257076, ...,  0.7825549 ,\n         0.33333333,  0.        ],\n       [ 0.11963182,  0.66804082,  0.2620229 , ...,  0.44708212,\n         1.        ,  0.        ],\n       [ 0.77965924,  0.93525417,  1.41545837, ..., -1.00764586,\n         0.66666667,  0.        ],\n       ...,\n       [-1.94725399,  0.5723286 , -0.60322335, ..., -0.8214109 ,\n         0.33333333,  1.        ],\n       [ 0.81585077,  0.40123688,  0.42092442, ...,  0.61777918,\n         0.33333333,  0.        ],\n       [-1.27014803,  1.56905205, -0.28473651, ..., -0.24679456,\n         1.        ,  0.        ]])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, n_features, n_targets, hidden_size=512, dropratio=0.2):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(n_features)\n        self.dropout1 = nn.Dropout(dropratio)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(n_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(dropratio)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm2a = nn.BatchNorm1d(hidden_size)\n        self.dropout2a = nn.Dropout(dropratio)\n        self.dense2a = nn.utils.weight_norm(nn.Linear(hidden_size, int(hidden_size/2)))\n        \n        self.batch_norm3 = nn.BatchNorm1d(int(hidden_size/2))\n        self.dropout3 = nn.Dropout(dropratio)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(int(hidden_size/2), n_targets))\n        \n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        x = self.relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = self.relu(self.dense2(x))\n        \n        x = self.batch_norm2a(x)\n        x = self.dropout2a(x)\n        x = self.relu(self.dense2a(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n#         x = self.sigmoid(x)\n        \n        return x\n\n# label smmothing\nclass SmoothCrossEntropyLoss(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets, n_classes, smoothing=0.0):\n        assert 0 <= smoothing <= 1\n        with torch.no_grad():\n#             targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n            targets = targets * (1 - smoothing) + torch.ones_like(targets).to(device) * smoothing / n_classes\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothCrossEntropyLoss()._smooth(targets, inputs.shape[1], self.smoothing)\n\n        if self.weight is not None:\n            inputs = inputs * self.weight.unsqueeze(0)\n\n        loss = F.binary_cross_entropy_with_logits(inputs, targets)\n\n        return loss\n\ndef running_train(X_train, Y_train, X_val, Y_val, dataloader, i_fold=None, seed=None):\n    # prepare for train\n    model = Model(n_features=X_train.shape[1], n_targets=Y_train.shape[1], hidden_size=hsize, dropratio=dropratio).to(device)\n    criterion = SmoothCrossEntropyLoss(smoothing=smoothing)\n    metric = lambda inputs, targets : F.binary_cross_entropy((torch.clamp(torch.sigmoid(inputs), p_min, p_max)), targets)\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=lr, weight_decay=wd)\n    if lr_scheduler == 'OneCycleLR' or lr_scheduler == 'both':\n        scheduler1 = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=pct_start, div_factor=div_factor, \n                                                    max_lr=1e-2, epochs=nepoch, steps_per_epoch=len(dataloader))\n    if lr_scheduler == 'ReduceLROnPlateau' or lr_scheduler == 'both':\n        scheduler2 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=0.5, patience=3)\n    # train\n    min_valmetric = np.inf\n    step = 0\n    for epoch in range(nepoch):\n        train_loss = 0\n        train_metric = 0\n        for i, (X, Y) in enumerate(dataloader):\n            model.train()\n            predictions = model(X.to(device=device))\n            loss = criterion(predictions, Y.to(device=device))\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            if lr_scheduler == 'OneCycleLR' or lr_scheduler == 'both':\n                scheduler1.step()\n\n            train_loss += loss.item()\n            train_metric += metric(predictions, Y.to(device=device))\n\n        train_loss /= len(dataloader)\n        train_metric /= len(dataloader)\n        model.eval()\n        predictions = model(X_val.to(device=device))\n\n        val_loss = criterion(predictions, Y_val.to(device=device))\n        val_metric = metric(predictions, Y_val.to(device=device))\n        if lr_scheduler == 'ReduceLROnPlateau' or lr_scheduler == 'both':\n            scheduler2.step(val_metric)\n        print('Epoch {}/{}, Train Loss={:5f}, Train Metric={:.5f}, Val Loss={:.5f}, Val Metric={:.5f}'.format(\n            epoch + 1, nepoch, train_loss, train_metric, val_loss, val_metric))\n        if val_metric.item() < min_valmetric:\n            min_valmetric = val_metric.item()\n            model_name = 'model_{}_{}.pth'.format(i_fold + 1, seed) if eval_strategy == 'kfold' else 'model_single.pth'\n            torch.save(model.state_dict(), model_name)\n        elif earlystop:\n            step += 1\n            if step > earlystop_step:\n                break","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hypter-parameters\ndevice = ('cuda' if torch.cuda.is_available() else 'cpu')\n\n# model\nhsize = 1024\ndropratio = 0.2\n\n# train\nbatchsize = 128\nlr = 0.001\nwd = 1e-5\nsmoothing = 0.001\np_min = smoothing\np_max = 1 - smoothing\nnepoch = 20\nearlystop = True\nearlystop_step = 10\n\n# lr_scheduler, options: ['OneCycleLR', 'ReduceLROnPlateau', 'both']\nlr_scheduler = 'OneCycleLR'\n# OneCycleLR\npct_start = 0.1\ndiv_factor = 1e3\n# ReduceLROnPlateau\nfactor=0.5\npatience=3\n\n# kfold\nnseed = 5\nNFOLDS = 10\neval_strategy = 'kfold'","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits= NFOLDS, shuffle = True)\n\nss= np.zeros([to_pred_non_ctl.shape[0], num_labels])\nseed = 0\n\nfor i_fold, (train_index, val_index) in enumerate(kf.split(X_train)):\n    print('Training at fold: ', i_fold)\n    tf.keras.backend.clear_session()\n    \n    # Get data\n    fold_X_train, fold_y_train = X_train[train_index], to_train_targets[cols_target].values[train_index]\n    \n    fold_X_val, fold_y_val = X_train[val_index], to_train_targets[cols_target].values[val_index]\n    \n    # compile model\n    \n    \n    X_train_tensor = torch.from_numpy(fold_X_train).to(dtype=torch.float32)\n    y_train_tensor = torch.from_numpy(fold_y_train).to(dtype=torch.float32)\n    X_val_tensor = torch.from_numpy(fold_X_val).to(dtype=torch.float32)\n    y_val_tensor = torch.from_numpy(fold_y_val).to(dtype=torch.float32)\n    \n    \n    dataset = TensorDataset(X_train_tensor, y_train_tensor)\n    dataloader = DataLoader(dataset, batch_size= 64, shuffle=True)\n\n    # train\n    running_train(X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor, dataloader, i_fold=i_fold, seed=seed)\n    \n    # Predict\n    model = Model(n_features=X_train_tensor.shape[1], \n                  n_targets=y_train_tensor.shape[1], \n                  hidden_size=hsize, \n                  dropratio=dropratio).to(device)\n    model.load_state_dict(torch.load('model_{}_{}.pth'.format(i_fold + 1, seed)))\n    model.eval()\n\n    # predict on test\n    X_pred_tensor = torch.from_numpy(X_pred).to(dtype=torch.float32)\n    print('predict on test...')\n    prediction_test = torch.clamp(torch.sigmoid(model(X_pred_tensor.to(device=device)).detach().cpu()), p_min, p_max)\n    \n    print('  done.\\n')\n    ss += prediction_test.numpy()\n\nss = ss/NFOLDS","execution_count":12,"outputs":[{"output_type":"stream","text":"Training at fold:  0\nEpoch 1/20, Train Loss=0.807834, Train Metric=0.80783, Val Loss=0.69615, Val Metric=0.69615\nEpoch 2/20, Train Loss=0.693865, Train Metric=0.69386, Val Loss=0.69318, Val Metric=0.69318\nEpoch 3/20, Train Loss=0.693211, Train Metric=0.69321, Val Loss=0.69321, Val Metric=0.69321\nEpoch 4/20, Train Loss=0.693224, Train Metric=0.69322, Val Loss=0.69321, Val Metric=0.69321\nEpoch 5/20, Train Loss=0.693237, Train Metric=0.69324, Val Loss=0.69319, Val Metric=0.69319\nEpoch 6/20, Train Loss=0.693248, Train Metric=0.69325, Val Loss=0.69328, Val Metric=0.69328\nEpoch 7/20, Train Loss=0.693292, Train Metric=0.69329, Val Loss=0.69329, Val Metric=0.69329\n","name":"stdout"},{"output_type":"error","ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-ba3f1a0e8c75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mrunning_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_fold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# Predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-9313326a1d8c>\u001b[0m in \u001b[0;36mrunning_train\u001b[0;34m(X_train, Y_train, X_val, Y_val, dataloader, i_fold, seed)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"]}]},{"metadata":{},"cell_type":"markdown","source":"MODELING"},{"metadata":{"trusted":true},"cell_type":"code","source":"# HyperParameters\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 25\nBATCH_SIZE = 128\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nNFOLDS = 7           \nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = False\n\nnum_features= X_train.shape[1]\nnum_targets= num_labels\nhidden_size=1500\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.shape","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}